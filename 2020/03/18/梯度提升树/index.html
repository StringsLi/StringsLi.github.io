<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        梯度提升树 - undefined
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 人总要有些乐趣，比如编程和阅读 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/lixin.jpg" />
        </div>
        <div class="name">
            <i>Lixin</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-提升树模型"><span class="toc-text">1.1 提升树模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-提升树算法"><span class="toc-text">1.2 提升树算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-梯度提升"><span class="toc-text">1.3 梯度提升</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-提升树主要损失函数"><span class="toc-text">1.4 提升树主要损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-总结及优缺点"><span class="toc-text">1.5 总结及优缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-6-参考文献"><span class="toc-text">1.6 参考文献</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 人总要有些乐趣，比如编程和阅读 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        梯度提升树
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2020-03-18 20:25:34</span></span>
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <p>提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p>
<h5 id="1-1-提升树模型"><a href="#1-1-提升树模型" class="headerlink" title="1.1 提升树模型"></a>1.1 提升树模型</h5><p>提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p>
<p>提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示成决策树的加法模型。</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)</script><p>​    其中，$T(x;\Theta_m)$表示决策树；$\Theta_m$表示决策树的参数；$M$为树的个数.</p>
<h5 id="1-2-提升树算法"><a href="#1-2-提升树算法" class="headerlink" title="1.2 提升树算法"></a>1.2 提升树算法</h5><p>提升树算法采取前向分步算法。首先确定初始提升树$f_0(x) = 0$,第$m$步的模型是</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x)+T(x;\Theta_m)</script><p>其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$,</p>
<script type="math/tex; mode=display">
\hat{\Theta}_m = \arg \min_{\Theta_m}\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>由于树的线性组合可以很好的拟合训练数据，即数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p>
<p>下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的一般决策问题。</p>
<p>对于二分类分类问题，提升树算法只需要将$Adaboost$算中基本分类器限制为二类分类树即可，可以说这时的提升树算法是$Adaboost$算法的特殊情况，这里不再详细叙述。下面重点叙述回归问题的提升树。</p>
<p>已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间。如果将输入空间$\chi$划分成$J$个互不相交的区域$R_1,R_2,…,R_J$，并且在每个区域上确定输出的常量$c_j$,那么树可以表示为</p>
<script type="math/tex; mode=display">
T(x;\Theta) = \sum_{j=1}^{J}c_jI(x \in R_j)</script><p>其中参数$\Theta={(R_1,c_1),(R_2,c_2),…,(R_J,c_J)}$表示树的区域划分和各区域上常数，$J$是回归树的复杂度即叶子节点的个数。</p>
<p>回归问题提升树使用以下前向分布算法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_0(x) &= 0\\
f_m(x) &= f_{m-1}(x) + T(x;\Theta_m),m = 1,2,...,M \\
f_M(x) &= \sum_{m=1}^{M}T(x;\Theta_m) \\
\end{aligned}</script><p>在前向分布算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解</p>
<script type="math/tex; mode=display">
\hat{\Theta}_m = \arg \min_{\Theta_m}\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>得到$\hat{\Theta}_m$,即第$m$棵树的参数。</p>
<p>当采用平方损失函数时，</p>
<script type="math/tex; mode=display">
L(y,f(x)) = (y - f(x))^{2}</script><p>其损失变为</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(y,f_{m-1}(x) - T(x;\Theta_m)) &=(y - f_{m-1}(x) - T(x;\Theta_m))^{m}\\
 &= (r - T(x;\Theta_m))^2
\end{aligned}</script><p>这里,</p>
<script type="math/tex; mode=display">
r = y - f_{m-1}(x)</script><p>是当前模型拟合数据的<strong>残差(residual)</strong>,所以，对回归问题的提升树来说，只需要简单地拟合当前模型的残差。这样，算法是相当简单。 </p>
<p><strong>算法1 回归问题的提升树算法</strong></p>
<p>输入：训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间</p>
<p>输出：提升树$f_M(x)$</p>
<p>(1) 初始化$f_0(x) = 0$</p>
<p>(2) 对$m=1,2,3,…,M$</p>
<p>​     (a) 按照式$r = y - f_{m-1}(x)$计算残差</p>
<script type="math/tex; mode=display">
r_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N</script><p>​     (b) 拟合残差$r_{mi}$学习一棵回归树，得到$T(x;\Theta_m)$</p>
<p>​     (c) 更新$f_m(x) = f_{m-1}(x) + T(x;\Theta_m)$</p>
<p>(3) 得到回归问题提升树</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)</script><h5 id="1-3-梯度提升"><a href="#1-3-梯度提升" class="headerlink" title="1.3 梯度提升"></a>1.3 梯度提升</h5><p>梯度提升(Gradient Boosting）是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失时，每一步优化是很简单的。对于一般的损失函数而言，往往每一步优化并不是那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，<strong>其关键是利用损失函数的负梯度在当前模型的值</strong>：</p>
<script type="math/tex; mode=display">
-[\frac{\partial{L(y,f(x_i))}}{\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}</script><p><strong>作为回归问题提升树算法中残差的近似值</strong>，拟合一个回归树。</p>
<p><strong>算法2 梯度提升树算法</strong></p>
<p>输入：训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间</p>
<p>输出: 回归树$\hat{f}(x)$</p>
<p>(1) 初始化$f_0(x) = \arg \min_{c}\sum_{i=1}^{N}L(y_i,c)$</p>
<p>(2) 对$m=1,2,3,…,M$</p>
<p>​     (a) 对$i = 1,2, …,N$计算残差</p>
<script type="math/tex; mode=display">
r_{mi} = -[\frac{\partial{L(y_i,f(x_i))}}{\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}</script><p>​     (b) 拟合$r_{mi}$学习一棵回归树，得到第$m$棵树的叶节点区域$R_{mj},j  = 1,2,…,J$</p>
<p>​     (c) 对于$j  = 1,2,…,J$，计算</p>
<script type="math/tex; mode=display">
c_{mj} = \arg \min_{c}\sum_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i) +c)</script><p>​    (d）更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^Jc_{mi}I(x \in R_{mj})$</p>
<p>(3) 得到回归问题提升树</p>
<script type="math/tex; mode=display">
\hat{f}(x) = f_M(x) = \sum_{m=1}^{M}\sum_{j=1}^{J}c_{mj}I(x\in R_{mj})</script><p>算法的第一步初始化，估计使损失函数极小化的常数值，它只有一个根节点的树，第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数来说，它的负梯度其实就是常说的残差，对于一般的损失函数，它就是残差的近似值。第2（b)步，估计回归树叶子节点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶子终点区域的值，使损失函数极小化。第2（d）步更新回归树。第3步，得到输出的最终模型。</p>
<h5 id="1-4-提升树主要损失函数"><a href="#1-4-提升树主要损失函数" class="headerlink" title="1.4 提升树主要损失函数"></a>1.4 提升树主要损失函数</h5><p>下面我们对提升树所用的损失函数做一个总结：</p>
<p>1) 对于分类算法来说：其损失函数一般有对数损失函数和指数损失函数：</p>
<p>a）<strong>指数损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = exp(-y_if(x_i))</script><p>其负梯度误差为：</p>
<script type="math/tex; mode=display">
-y_i.exp(-f(x_i))</script><p>b）<strong>对数损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))</script><p>其负梯度为：</p>
<script type="math/tex; mode=display">
\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}</script><p>化简为：</p>
<script type="math/tex; mode=display">
\frac{y_i}{(1+exp(y_if(x_i)))}</script><p>2) 回归算法：常见的有以下四种</p>
<ol>
<li><p><strong>均方差损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = (y_i - f(x_i))^2</script><p>其负梯度为：</p>
<script type="math/tex; mode=display">
y_i - f(x_i)</script><p>p.s. 损失函数为$L(y,f(x))=(y-f(x))^2$,我们需要最小化$J= \sum_iL(y_i,f(x_i))$通过调整$f(x_1),f(x_2),…,f(x_n)$.我们把$f(x_i)$当成参数并求导</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial f(x_i)} = \frac{\partial \sum_iL(y_i,f(x_i))}{\partial f(x_i)} = \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} = f(x_i) - y_i</script><p>所以，<strong>我们在均方差损失函数下，可以把残差理解成负梯度。</strong></p>
<script type="math/tex; mode=display">
y_i-f(x_i) = - \frac{\partial}{\partial f(x_i)}</script></li>
<li><p>绝对损失函数：</p>
<script type="math/tex; mode=display">
L(y_i,f(x_i) = |y_i - f(x_i)|</script><p>其对应的负梯度为：</p>
<script type="math/tex; mode=display">
sign (y_i - f(x_i))</script></li>
<li><p>Huber损失函数：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(y_i,f(x_i)) = 
\begin{cases} 
\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\leq \delta\\
\delta(|y_i -f(x_i)| - \frac{\delta}{2}), |y_i - f(x_i)|\geq \delta
\end{cases}
\end{aligned}</script><p>其对应的负梯度为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r(y_i,f(x_i)) = 
\begin{cases} 
y_i-f(x_i), |y_i - f(x_i)|\leq \delta\\
\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\geq \delta
\end{cases}
\end{aligned}</script></li>
<li><p>分位数损失。它对应的是分位数回归的损失函数，表达式为</p>
<script type="math/tex; mode=display">
L(x_i,f(x_i)) = \sum_{y_i \geq f(x_i)}\theta|y_i - f(x_i)| + \sum_{y_i < f(x_i)}(1-\theta)|y_i - f(x_i)|</script><p>其中，$\theta$为分位数，需要在回归钱设置，其对应的负梯度为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r(y_i,f(x_i)) = 
\begin{cases} 
\theta,  y_i \geq f(x_i)\\
\theta - 1, y_i < f(x_i)
\end{cases}
\end{aligned}</script><p><img src="/2020/03/18/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/梯度提升树\损失函数.png" alt></p>
</li>
</ol>
<h5 id="1-5-总结及优缺点"><a href="#1-5-总结及优缺点" class="headerlink" title="1.5 总结及优缺点"></a>1.5 总结及优缺点</h5><p>本文介绍了boosting族的提升树算法和梯度提升树（GBDT)算法，提升树算法的每轮弱学习器是拟合上一轮的残差生成的，GBDT算法的每轮弱学习器是拟合上一轮损失函数的负梯度生成的。提升树算法和GBDT算法都是用CART回归树作为弱学习器，只要确定模型的损失函数，提升树和GBDT就可以通过前向分布算法进行构建。</p>
<p>梯度提升树主要的优点有：</p>
<p>1） 可以灵活处理各种类型的数据，包括连续值和离散值。</p>
<p>2）在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p>
<p>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>梯度提升树的主要缺点有：</p>
<p>1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<h5 id="1-6-参考文献"><a href="#1-6-参考文献" class="headerlink" title="1.6 参考文献"></a>1.6 参考文献</h5><p>李航 《统计学习》</p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud" target="_blank" rel="noopener">AirCloud</a></p>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
