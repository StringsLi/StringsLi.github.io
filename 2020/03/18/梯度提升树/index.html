<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>梯度提升树 | Lixin</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="机器学习、数据挖掘" />
  

  <meta name="description" content="提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。 1.1 提升树模型提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 提升方法">
<meta property="og:type" content="article">
<meta property="og:title" content="梯度提升树">
<meta property="og:url" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;03&#x2F;18&#x2F;%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91&#x2F;index.html">
<meta property="og:site_name" content="Lixin">
<meta property="og:description" content="提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。 1.1 提升树模型提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。 提升方法">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;03&#x2F;18&#x2F;%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91&#x2F;梯度提升树&#x2F;损失函数.png">
<meta property="og:image" content="http:&#x2F;&#x2F;wx3.sinaimg.cn&#x2F;mw690&#x2F;00630Defgy1g4tdwhqzsdj30rp0afdho.jpg">
<meta property="og:updated_time" content="2021-04-15T09:21:41.117Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;03&#x2F;18&#x2F;%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91&#x2F;梯度提升树&#x2F;损失函数.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-提升树模型"><span class="toc-text">1.1 提升树模型</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-提升树算法"><span class="toc-text">1.2 提升树算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-梯度提升"><span class="toc-text">1.3 梯度提升</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-4-提升树主要损失函数"><span class="toc-text">1.4 提升树主要损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-5-总结及优缺点"><span class="toc-text">1.5 总结及优缺点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-6-梯度提升和梯度下降的区别和联系是什么？"><span class="toc-text">1.6. 梯度提升和梯度下降的区别和联系是什么？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-7-代码实现"><span class="toc-text">1.7 代码实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-8-参考文献"><span class="toc-text">1.8 参考文献</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#偏差和方差"><span class="toc-text">偏差和方差</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT-和LR"><span class="toc-text">GBDT 和LR</span></a>
  </div>



<div class="content content-post CENTER">
   <article id="post-梯度提升树" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">梯度提升树</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2020.03.18</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Lixin</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <p>提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p>
<h5 id="1-1-提升树模型"><a href="#1-1-提升树模型" class="headerlink" title="1.1 提升树模型"></a>1.1 提升树模型</h5><p>提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。</p>
<p>提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示成决策树的加法模型。</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)</script><p>​    其中，$T(x;\Theta_m)$表示决策树；$\Theta_m$表示决策树的参数；$M$为树的个数.</p>
<h5 id="1-2-提升树算法"><a href="#1-2-提升树算法" class="headerlink" title="1.2 提升树算法"></a>1.2 提升树算法</h5><p>提升树算法采取前向分步算法。首先确定初始提升树$f_0(x) = 0$,第$m$步的模型是</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x)+T(x;\Theta_m)</script><p>其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$,</p>
<script type="math/tex; mode=display">
\hat{\Theta}_m = \arg \min_{\Theta_m}\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>由于树的线性组合可以很好的拟合训练数据，即数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p>
<p>下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的一般决策问题。</p>
<p>对于二分类分类问题，提升树算法只需要将$Adaboost$算中基本分类器限制为二类分类树即可，可以说这时的提升树算法是$Adaboost$算法的特殊情况，这里不再详细叙述。下面重点叙述回归问题的提升树。</p>
<p>已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间。如果将输入空间$\chi$划分成$J$个互不相交的区域$R_1,R_2,…,R_J$，并且在每个区域上确定输出的常量$c_j$,那么树可以表示为</p>
<script type="math/tex; mode=display">
T(x;\Theta) = \sum_{j=1}^{J}c_jI(x \in R_j)</script><p>其中参数$\Theta={(R_1,c_1),(R_2,c_2),…,(R_J,c_J)}$表示树的区域划分和各区域上常数，$J$是回归树的复杂度即叶子节点的个数。</p>
<p>回归问题提升树使用以下前向分布算法：</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_0(x) &= 0\\
f_m(x) &= f_{m-1}(x) + T(x;\Theta_m),m = 1,2,...,M \\
f_M(x) &= \sum_{m=1}^{M}T(x;\Theta_m) \\
\end{aligned}</script><p>在前向分布算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解</p>
<script type="math/tex; mode=display">
\hat{\Theta}_m = \arg \min_{\Theta_m}\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>得到$\hat{\Theta}_m$,即第$m$棵树的参数。</p>
<p>当采用平方损失函数时，</p>
<script type="math/tex; mode=display">
L(y,f(x)) = (y - f(x))^{2}</script><p>其损失变为</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(y,f_{m-1}(x) - T(x;\Theta_m)) &=(y - f_{m-1}(x) - T(x;\Theta_m))^{m}\\
 &= (r - T(x;\Theta_m))^2
\end{aligned}</script><p>这里,</p>
<script type="math/tex; mode=display">
r = y - f_{m-1}(x)</script><p>是当前模型拟合数据的<strong>残差(residual)</strong>,所以，对回归问题的提升树来说，只需要简单地拟合当前模型的残差。这样，算法是相当简单。 </p>
<p><strong>算法1 回归问题的提升树算法</strong></p>
<p>输入：训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间</p>
<p>输出：提升树$f_M(x)$</p>
<p>(1) 初始化$f_0(x) = 0$</p>
<p>(2) 对$m=1,2,3,…,M$</p>
<p>​     (a) 按照式$r = y - f_{m-1}(x)$计算残差</p>
<script type="math/tex; mode=display">
r_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N</script><p>​     (b) 拟合残差$r_{mi}$学习一棵回归树，得到$T(x;\Theta_m)$</p>
<p>​     (c) 更新$f_m(x) = f_{m-1}(x) + T(x;\Theta_m)$</p>
<p>(3) 得到回归问题提升树</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^{M}T(x;\Theta_m)</script><h5 id="1-3-梯度提升"><a href="#1-3-梯度提升" class="headerlink" title="1.3 梯度提升"></a>1.3 梯度提升</h5><p>梯度提升(Gradient Boosting）是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失时，每一步优化是很简单的。对于一般的损失函数而言，往往每一步优化并不是那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，<strong>其关键是利用损失函数的负梯度在当前模型的值</strong>：</p>
<script type="math/tex; mode=display">
-[\frac{\partial{L(y,f(x_i))}}{\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}</script><p><strong>作为回归问题提升树算法中残差的近似值</strong>，拟合一个回归树。</p>
<p>当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是$y_i$，即“当前预测模型的值”，也就是对它求负梯度。</p>
<p><strong>算法2 梯度提升树算法</strong></p>
<p>输入：训练数据$T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)},x_{i} \in \chi \subseteq\mathbb{R}^n$,$\chi$为输入空间，$y_i \in \nu \subseteq \mathbb{R}  $ ,$\nu$为输出空间</p>
<p>输出: 回归树$\hat{f}(x)$</p>
<p>(1) 初始化$f_0(x) = \arg \min_{c}\sum_{i=1}^{N}L(y_i,c)$</p>
<p>(2) 对$m=1,2,3,…,M$</p>
<p>​     (a) 对$i = 1,2, …,N$计算残差</p>
<script type="math/tex; mode=display">
r_{mi} = -[\frac{\partial{L(y_i,f(x_i))}}{\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}</script><p>​     (b) 拟合$r_{mi}$学习一棵回归树，得到第$m$棵树的叶节点区域$R_{mj},j  = 1,2,…,J$</p>
<p>​     (c) 对于$j  = 1,2,…,J$，计算</p>
<script type="math/tex; mode=display">
c_{mj} = \arg \min_{c}\sum_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i) +c)</script><p>​    (d）更新$f_m(x) = f_{m-1}(x) + \sum_{j=1}^Jc_{mi}I(x \in R_{mj})$</p>
<p>(3) 得到回归问题提升树</p>
<script type="math/tex; mode=display">
\hat{f}(x) = f_M(x) = \sum_{m=1}^{M}\sum_{j=1}^{J}c_{mj}I(x\in R_{mj})</script><p>算法的第一步初始化，估计使损失函数极小化的常数值，它只有一个根节点的树，第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数来说，它的负梯度其实就是常说的残差，对于一般的损失函数，它就是残差的近似值。第2（b)步，估计回归树叶子节点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶子终点区域的值，使损失函数极小化。第2（d）步更新回归树。第3步，得到输出的最终模型。</p>
<h5 id="1-4-提升树主要损失函数"><a href="#1-4-提升树主要损失函数" class="headerlink" title="1.4 提升树主要损失函数"></a>1.4 提升树主要损失函数</h5><p>下面我们对提升树所用的损失函数做一个总结：</p>
<p>1) 对于分类算法来说：其损失函数一般有对数损失函数和指数损失函数：</p>
<p>a）<strong>指数损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = exp(-y_if(x_i))</script><p>其负梯度误差为：</p>
<script type="math/tex; mode=display">
-y_i.exp(-f(x_i))</script><p>b）<strong>对数损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))</script><p>其负梯度为：</p>
<script type="math/tex; mode=display">
\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}</script><p>化简为：</p>
<script type="math/tex; mode=display">
\frac{y_i}{(1+exp(y_if(x_i)))}</script><p>2) 回归算法：常见的有以下四种</p>
<ol>
<li><p><strong>均方差损失函数</strong></p>
<script type="math/tex; mode=display">
L(y_i,f(x_i)) = (y_i - f(x_i))^2</script><p>其负梯度为：</p>
<script type="math/tex; mode=display">
y_i - f(x_i)</script><p>p.s. 损失函数为$L(y,f(x))=(y-f(x))^2$,我们需要最小化$J= \sum_iL(y_i,f(x_i))$通过调整$f(x_1),f(x_2),…,f(x_n)$.我们把$f(x_i)$当成参数并求导</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial f(x_i)} = \frac{\partial \sum_iL(y_i,f(x_i))}{\partial f(x_i)} = \frac{\partial L(y_i,f(x_i))}{\partial f(x_i)} = f(x_i) - y_i</script><p>所以，<strong>我们在均方差损失函数下，可以把残差理解成负梯度。</strong></p>
<script type="math/tex; mode=display">
y_i-f(x_i) = - \frac{\partial}{\partial f(x_i)}</script></li>
<li><p>绝对损失函数：</p>
<script type="math/tex; mode=display">
L(y_i,f(x_i) = |y_i - f(x_i)|</script><p>其对应的负梯度为：</p>
<script type="math/tex; mode=display">
sign (y_i - f(x_i))</script></li>
<li><p>Huber损失函数：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(y_i,f(x_i)) = 
\begin{cases} 
\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\leq \delta\\
\delta(|y_i -f(x_i)| - \frac{\delta}{2}), |y_i - f(x_i)|\geq \delta
\end{cases}
\end{aligned}</script><p>其对应的负梯度为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r(y_i,f(x_i)) = 
\begin{cases} 
y_i-f(x_i), |y_i - f(x_i)|\leq \delta\\
\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\geq \delta
\end{cases}
\end{aligned}</script></li>
<li><p>分位数损失。它对应的是分位数回归的损失函数，表达式为</p>
<script type="math/tex; mode=display">
L(x_i,f(x_i)) = \sum_{y_i \geq f(x_i)}\theta|y_i - f(x_i)| + \sum_{y_i < f(x_i)}(1-\theta)|y_i - f(x_i)|</script><p>其中，$\theta$为分位数，需要在回归钱设置，其对应的负梯度为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
r(y_i,f(x_i)) = 
\begin{cases} 
\theta,  y_i \geq f(x_i)\\
\theta - 1, y_i < f(x_i)
\end{cases}
\end{aligned}</script><p><img src="/2020/03/18/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/梯度提升树\损失函数.png" alt></p>
</li>
</ol>
<h5 id="1-5-总结及优缺点"><a href="#1-5-总结及优缺点" class="headerlink" title="1.5 总结及优缺点"></a>1.5 总结及优缺点</h5><p>本文介绍了boosting族的提升树算法和梯度提升树（GBDT)算法，提升树算法的每轮弱学习器是拟合上一轮的残差生成的，GBDT算法的每轮弱学习器是拟合上一轮损失函数的负梯度生成的。提升树算法和GBDT算法都是用CART回归树作为弱学习器，只要确定模型的损失函数，提升树和GBDT就可以通过前向分布算法进行构建。</p>
<p>梯度提升树主要的优点有：</p>
<p>1） 可以灵活处理各种类型的数据，包括连续值和离散值。</p>
<p>2）在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。</p>
<p>3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</p>
<p>梯度提升树的主要缺点有：</p>
<p>1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。</p>
<h5 id="1-6-梯度提升和梯度下降的区别和联系是什么？"><a href="#1-6-梯度提升和梯度下降的区别和联系是什么？" class="headerlink" title="1.6. 梯度提升和梯度下降的区别和联系是什么？"></a>1.6. 梯度提升和梯度下降的区别和联系是什么？</h5><p>下表是梯度提升算法和梯度下降算法的对比情况。可以发现，两者都是在每 一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更 新，只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参 数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函 数空间中，从而大大扩展了可以使用的模型种类。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/00630Defgy1g4tdwhqzsdj30rp0afdho.jpg" alt></p>
<h5 id="1-7-代码实现"><a href="#1-7-代码实现" class="headerlink" title="1.7 代码实现"></a>1.7 代码实现</h5><p>scala代码地址:<a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/GBDT.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/GBDT.scala</a></p>
<h5 id="1-8-参考文献"><a href="#1-8-参考文献" class="headerlink" title="1.8 参考文献"></a>1.8 参考文献</h5><p>李航 《统计学习》</p>
<h4 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h4><ul>
<li>偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差。偏差通常是由于我们对学习算法做了错误的假设所导致的，比如真实模型是某个二次函数，但我们假设模型是一次函数。由偏差带来的误差通常在训练误差上就能体现出来。</li>
<li>方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。方差通常是由于模型的复杂度相对于训练样本数m过高导致的，比如一共有100个训练样本，而我们假设模型是阶数不大于200的多项式函数。由方差带来的误差通常体现在测试误差相对于训练误差的增量上。</li>
</ul>
<h4 id="GBDT-和LR"><a href="#GBDT-和LR" class="headerlink" title="GBDT 和LR"></a>GBDT 和LR</h4><p>图中共有两棵树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1的。举例来说：上图有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第一个节点，编码[1,0,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到LR中进行分类。</p>
<ul>
<li><p>为什么建树采用ensemble决策树？</p>
<p>一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少颗树。按paper以及Kaggle竞赛中的GBDT+LR融合方式，多棵树正好满足LR每条训练样本可以通过GBDT映射成多个特征的需求。</p>
</li>
<li><p>为什么建树采用GBDT而非RF？</p>
<p>RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</p>
</li>
</ul>
<ul>
<li><p>如何使用GBDT 映射得到的特征？</p>
<p>通过GBDT生成的特征，可直接作为LR的特征使用，省去人工处理分析特征的环节，LR的输入特征完全依赖于通过GBDT得到的特征。此思路已尝试，通过实验发现GBDT+LR在曝光充分的广告上确实有效果，但整体效果需要权衡优化各类树的使用。 </p>
</li>
</ul>

    
  </div>

</article>


   

   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2020/03/18/xgboost/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
