<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>线性回归 | Lixin</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="机器学习、数据挖掘" />
  

  <meta name="description" content="线性回归的代码实现1. 原理多元线性回归的损失函数为：  J=\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2其中：$\hat{y}^{(i)} = \theta_{0} + \theta_{1}X_{1}^{(i)} + \theta_{2}X_{2}^{(i)} + … + \theta_{n}X_{n}^{(i)}$ 。 对 $J$ 求导为：  \nabla">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;01&#x2F;09&#x2F;%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&#x2F;index.html">
<meta property="og:site_name" content="Lixin">
<meta property="og:description" content="线性回归的代码实现1. 原理多元线性回归的损失函数为：  J=\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2其中：$\hat{y}^{(i)} = \theta_{0} + \theta_{1}X_{1}^{(i)} + \theta_{2}X_{2}^{(i)} + … + \theta_{n}X_{n}^{(i)}$ 。 对 $J$ 求导为：  \nabla">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2020-02-12T12:25:22.680Z">
<meta name="twitter:card" content="summary">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归的代码实现"><span class="toc-text">线性回归的代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-原理"><span class="toc-text">1. 原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-实现代码"><span class="toc-text">2.实现代码</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-python版本实现"><span class="toc-text">2.1 python版本实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-Scala版本实现"><span class="toc-text">2.2 Scala版本实现</span></a></li></ol></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-线性回归" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">线性回归</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2020.01.09</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Lixin</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h3 id="线性回归的代码实现"><a href="#线性回归的代码实现" class="headerlink" title="线性回归的代码实现"></a>线性回归的代码实现</h3><h4 id="1-原理"><a href="#1-原理" class="headerlink" title="1. 原理"></a>1. 原理</h4><p>多元线性回归的损失函数为：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2</script><p>其中：$\hat{y}^{(i)} = \theta_{0} + \theta_{1}X_{1}^{(i)} + \theta_{2}X_{2}^{(i)} + … + \theta_{n}X_{n}^{(i)}$ 。</p>
<p>对 $J$ 求导为：</p>
<script type="math/tex; mode=display">
\nabla J=(\frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta_1},...,\frac{\partial J}{\partial \theta_n})</script><p>其中：$\frac{\partial J}{\partial \theta_i}$ 为偏导数，与导数的求法一样。</p>
<p>对 $\nabla J$ 进一步计算：</p>
<script type="math/tex; mode=display">
\nabla J(\theta) =  \begin{pmatrix} \frac{\partial J}{\partial \theta_0} \\\ \frac{\partial J}{\partial \theta_1} \\\ \frac{\partial J}{\partial \theta_2} \\\ \cdots \\\ \frac{\partial J}{\partial \theta_n} \end{pmatrix} =   \begin{pmatrix} \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-1) \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_1^{(i)}) \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_2^{(i)}) \\\ \cdots \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_n^{(i)}) \end{pmatrix} = 2·\begin{pmatrix} \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)}) \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_1^{(i)} \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_2^{(i)} \\\ \cdots \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_n^{(i)} \end{pmatrix}</script><p>其中：$X_b = \begin{pmatrix}<br>1 &amp; X_1^{(1)} &amp; X_2^{(1)} &amp; \cdots &amp; X_n^{(1)} \\<br>1 &amp; X_1^{(2)} &amp; X_2^{(2)} &amp; \cdots &amp; X_n^{(2)} \\<br>\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\<br>1 &amp; X_1^{(m)} &amp; X_2^{(m)} &amp; \cdots &amp; X_n^{(m)}<br>\end{pmatrix}$</p>
<p>​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：</p>
<script type="math/tex; mode=display">
\nabla J(\theta)  = \frac{2}{m}\begin{pmatrix} \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)}) \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_1^{(i)} \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_2^{(i)} \\\ \cdots \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_n^{(i)} \end{pmatrix}</script><p>​        此时，目标函数就成了使 $\frac{1}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：</p>
<script type="math/tex; mode=display">
J(\theta) = MSE(y, \hat{y})</script><p>​        </p>
<p>注1. 有时候目标函数也去 $\frac{1}{\boldsymbol{2}m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2$ ,其中的<strong>2</strong>是为了抵消梯度中的2，实际的效果有限。</p>
<p>注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。</p>
<h4 id="2-实现代码"><a href="#2-实现代码" class="headerlink" title="2.实现代码"></a>2.实现代码</h4><h5 id="2-1-python版本实现"><a href="#2-1-python版本实现" class="headerlink" title="2.1 python版本实现"></a>2.1 python版本实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Jan  8 17:25:39 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: lixin</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,lr=<span class="number">.01</span>, num_iters=<span class="number">10000</span>,tolerance=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.num_iters = num_iters</span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.toterance = tolerance</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,x_train, y_train)</span>:</span></span><br><span class="line">        n_samples = len(x_train)</span><br><span class="line">        x_train = np.column_stack((np.ones(n_samples), x_train)) <span class="comment">#也可以写成np.c_</span></span><br><span class="line">        </span><br><span class="line">        self.w = <span class="number">.01</span> * np.ones(x_train.shape[<span class="number">1</span>])</span><br><span class="line">        self.loss_ = [<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># w_&#123;i&#125; = w_&#123;i&#125; - lr * (h(w_&#123;i&#125;) - y)*x_&#123;i&#125; 迭代公式</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> range(self.num_iters):</span><br><span class="line">            self.count += <span class="number">1</span></span><br><span class="line">            raw_output = np.matmul(x_train, self.w)</span><br><span class="line">            errors =  raw_output - y_train</span><br><span class="line">            loss = <span class="number">1</span>/(<span class="number">2</span> * n_samples) * errors.dot(errors)</span><br><span class="line">            delta_loss = loss - self.loss_[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">            self.loss_.append(loss)</span><br><span class="line">            <span class="keyword">if</span> np.abs(delta_loss) &lt; self.toterance:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                grad = (<span class="number">1.0</span> /n_samples) *np.matmul(x_train.T, np.array(errors))</span><br><span class="line">                self.w -= self.lr * grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x_test)</span>:</span></span><br><span class="line">        x_test = np.column_stack((np.ones(len(x_test)), x_test))</span><br><span class="line">        </span><br><span class="line">        output = np.matmul(x_test, self.w)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    num_inputs = <span class="number">2</span></span><br><span class="line">    num_examples = <span class="number">10000</span></span><br><span class="line">    true_w = [<span class="number">6.4</span>, <span class="number">-3.2</span>]</span><br><span class="line">    true_b = <span class="number">2.3</span></span><br><span class="line">    features = np.random.random((num_examples, num_inputs))</span><br><span class="line">    labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">    labels += np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>,size = len(labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> sklearn.model_selection</span><br><span class="line">    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = <span class="number">.20</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    lr = LinearRegression()</span><br><span class="line">    lr.fit(x_train,y_train)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"回归系数:"</span>,lr.w)</span><br><span class="line">    print(<span class="string">"迭代次数:"</span>,lr.count)</span><br><span class="line">    </span><br><span class="line">    y_pred = lr.predict(x_test)</span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">    mse = metrics.mean_squared_error(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"MSE: %.4f"</span> % mse)</span><br><span class="line"></span><br><span class="line">    mae = metrics.mean_absolute_error(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"MAE: %.4f"</span> % mae)</span><br><span class="line"></span><br><span class="line">    R2 = metrics.r2_score(y_test,y_pred)</span><br><span class="line">    print(<span class="string">"R2: %.4f"</span> % R2)</span><br></pre></td></tr></table></figure>
<h5 id="2-2-Scala版本实现"><a href="#2-2-Scala版本实现" class="headerlink" title="2.2 Scala版本实现"></a>2.2 Scala版本实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Scala 版本的实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LinearRegression</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> num_inputs = <span class="number">2</span></span><br><span class="line">    <span class="keyword">val</span> num_examples = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">val</span> x_train: <span class="type">BDM</span>[<span class="type">Double</span>] = <span class="type">BDM</span>.rand(num_examples, num_inputs)</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](num_examples, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_cat = <span class="type">BDM</span>.horzcat(ones, x_train)</span><br><span class="line">    <span class="keyword">val</span> y_train = x_cat * <span class="type">BDV</span>(<span class="number">2.3</span>, <span class="number">6.4</span>, <span class="number">-3.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">LinearRegression</span>(num_iters = <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">val</span> weights = model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">val</span> predictions = model.predict(weights, x_train)</span><br><span class="line">    println(<span class="string">"梯度下降求解的权重为："</span> + weights)</span><br><span class="line">    println(predictions)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>(<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y_train: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line">    <span class="keyword">val</span> n_features = x_train.cols</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDV</span>.ones[<span class="type">Double</span>](n_features) :* <span class="number">.01</span> <span class="comment">// 注意是:*</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loss_lst: <span class="type">ArrayBuffer</span>[<span class="type">Double</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Double</span>]()</span><br><span class="line">    loss_lst.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to num_iters <span class="keyword">if</span> flag) &#123;</span><br><span class="line">      <span class="keyword">val</span> raw_output = x_train * weights</span><br><span class="line">      <span class="keyword">val</span> error = raw_output - y_train</span><br><span class="line">      <span class="keyword">val</span> loss: <span class="type">Double</span> = error.t * error</span><br><span class="line">      <span class="keyword">val</span> delta_loss = loss - loss_lst.apply(loss_lst.size - <span class="number">1</span>)</span><br><span class="line">      loss_lst.append(loss)</span><br><span class="line">      <span class="keyword">if</span> (scala.math.abs(delta_loss) &lt; tolerance) &#123;</span><br><span class="line">        flag = <span class="literal">false</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> gradient = (error.t * x_train) :/ n_samples.toDouble</span><br><span class="line">        weights = weights - (gradient :* lr).t</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDV</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(<span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>), x)</span><br><span class="line">    <span class="keyword">val</span> output = x_test * weights</span><br><span class="line">    output</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参考文献：1. <a href="https://segmentfault.com/a/1190000017048213" target="_blank" rel="noopener">机器学习之梯度下降法与线性回归</a></p>

    
  </div>

</article>


   

   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/11/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2020/01/16/%E9%99%8D%E7%BB%B4_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
