<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>线性回归 | Lixin</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="机器学习、数据挖掘" />
  

  <meta name="description" content="线性回归的代码实现1. 线性回归推导​    对于给定了$m$个样本点($x^{(1)}$,$y^{(1)}$)，($x^{(2)}$,$y^{(2)}$)，$\dots$，($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in\R^{n}$。定义假设函数为$h_\theta(x)$，即$h_\theta(x)$为最终的拟合函数，$\theta$为待拟合参数也称作权重。  \beg">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归">
<meta property="og:url" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;01&#x2F;09&#x2F;%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92&#x2F;index.html">
<meta property="og:site_name" content="Lixin">
<meta property="og:description" content="线性回归的代码实现1. 线性回归推导​    对于给定了$m$个样本点($x^{(1)}$,$y^{(1)}$)，($x^{(2)}$,$y^{(2)}$)，$\dots$，($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in\R^{n}$。定义假设函数为$h_\theta(x)$，即$h_\theta(x)$为最终的拟合函数，$\theta$为待拟合参数也称作权重。  \beg">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2021-03-02T08:20:06.483Z">
<meta name="twitter:card" content="summary">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性回归的代码实现"><span class="toc-text">线性回归的代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-线性回归推导"><span class="toc-text">1. 线性回归推导</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-从样本数据出发推导损失函数"><span class="toc-text">1.1 从样本数据出发推导损失函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-从统计学理论出发推导损失函数"><span class="toc-text">1.2 从统计学理论出发推导损失函数</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-梯度下降算法求解凸优化问题"><span class="toc-text">2. 梯度下降算法求解凸优化问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-正规方程"><span class="toc-text">3. 正规方程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-实现代码"><span class="toc-text">2.实现代码</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-python版本实现"><span class="toc-text">2.1 python版本实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-Scala版本实现"><span class="toc-text">2.2 Scala版本实现</span></a></li></ol></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-线性回归" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">线性回归</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2020.01.09</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Lixin</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h3 id="线性回归的代码实现"><a href="#线性回归的代码实现" class="headerlink" title="线性回归的代码实现"></a>线性回归的代码实现</h3><h4 id="1-线性回归推导"><a href="#1-线性回归推导" class="headerlink" title="1. 线性回归推导"></a>1. 线性回归推导</h4><p>​    对于给定了$m$个样本点($x^{(1)}$,$y^{(1)}$)，($x^{(2)}$,$y^{(2)}$)，$\dots$，($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\in\R^{n}$。定义假设函数为$h_\theta(x)$，即$h_\theta(x)$为最终的拟合函数，$\theta$为待拟合参数也称作权重。</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_\theta(x)&=\theta_0+\theta_1x_1+\cdots+\theta_nx_n \\
&=\begin{bmatrix}
\theta_0, & \theta_1,& \cdots ,& \theta_n 
\end{bmatrix}
\begin{bmatrix} 1\\x_1 \\ \vdots \\ x_n\end{bmatrix}\\
&=\theta^Tx

\end{aligned}</script><h5 id="1-1-从样本数据出发推导损失函数"><a href="#1-1-从样本数据出发推导损失函数" class="headerlink" title="1.1 从样本数据出发推导损失函数"></a>1.1 从样本数据出发推导损失函数</h5><p>​    在样本数据中$y^{(i)}$是实际存在值而$h_\theta(x^{(i)})$对应的是模型预测值，显然如果想要模型预测的效果好，那么对应的误差就要小，假设函数在任意样本点的误差为$|h_\theta(x^{(i)})-y^{(i)}|$则$m$个样本点的误差和为$ \sum\limits_{i=1}^m|h_\theta(x^{(i)})-y^{(i)}|$，因此问题就转化为求解$  \begin{aligned}\mathop{\arg\min}_{\theta} \sum\limits_{i=1}^m|h_\theta(x^{(i)}-y^{(i)}|\end{aligned}$为了后续求解最优值(绝对值函数不好求导)，所以损失函数采用了误差平方和的形式$ \begin{aligned}\mathop{\arg\min}_{\theta} \sum\limits_{i=1}^m(h_\theta(x^{(i)}-y^{(i)})^2\end{aligned}$。</p>
<h5 id="1-2-从统计学理论出发推导损失函数"><a href="#1-2-从统计学理论出发推导损失函数" class="headerlink" title="1.2 从统计学理论出发推导损失函数"></a>1.2 从统计学理论出发推导损失函数</h5><p>​    为什么线性回归问题的损失函数会是误差平方和的形式？这里从统计理论上进行解释，对于给定的$y^{(i)}$总能找到$\varepsilon^{(i)}$使得这个等式成立$y^{(i)}=h_\theta(x^{(i)})+\varepsilon^{(i))}$，$ \varepsilon^{(i)}$代表真实值和预测值之间的误差且$\varepsilon^{(i)} \sim \mathcal{N}(0,\sigma^2)$,简单解释下为什么误差$\varepsilon^{(i)}$会服从均值为零的正态分布，误差的产生有很多种因素的影响，误差可以看作是这些因素(随机变量)之和共同作用而产生的，由中心极限定理可知随机变量和的分布近似的服从正态分布；更通俗易懂的解释是，当你在选择$h_\theta(x)$时主观的会认定这个$h_\theta(x)$是比较符合样本数据的，比如对一些样本数据可视化后，发现样本数据明显是趋近于一条直线，而你在对$h_\theta(x)$的选择上肯定会选择直线方程作为$h_\theta(x)$而不会选择多项式函数作为$h_\theta(x)$。而这种$h_\theta(x)$一旦选定，可以认为大部分数据都在$h_\theta(x)$的附近，因此误差大部分集中在零值附近所以$\mathcal{N}(0,\sigma^2)$作为$\varepsilon^{(i)}$的先验分布是比较合理的。</p>
<p>​    随机变量$\varepsilon^{(i)}$的概率密度函数为：</p>
<script type="math/tex; mode=display">
p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\varepsilon^{(i)})^2}{2\sigma^2}}</script><p>​    代入$h_\theta(x^{(i)}),  y^{(i)}$则</p>
<script type="math/tex; mode=display">
p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\theta^Tx^{(i)}-y^{(i)})^2}{2\sigma^2}}</script><p>这里的$p(y^{(i)}|x^{(i)};\theta)$并不代表条件概率，只是一个记号它表示给定$x^{(i)}, y^{(i)} $和一组$\theta$后的概率密度函数。由最大似然估计可知：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\theta)&=\prod\limits_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\
&=\prod\limits_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\theta^Tx^{(i)}-y^{(i)})^2}{2\sigma^2}}
\end{aligned}</script><p>对$L(\theta)$取对数从而得到对数化最大似然估计函数</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal L(\theta) &= \mathcal log(L(\theta)) \\
&=\mathcal \log \prod\limits_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(\theta^Tx^{(i)}-y^{(i)})^2}{2\sigma^2}}\\
&=\sum\limits_{i=1}^m \mathcal \log\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(\theta^Tx^{(i)}-y^{(i)})^2}{2\sigma^2}}\\
&=m \mathcal \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2}\sum\limits_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
\end{aligned}</script><p>模型的损失函数$ J(\theta)=\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$，其实只是在先前的最优化模型前面乘了一个常系数$\dfrac{1}{2}$对最优化的结果并不会产生影响。</p>
<h4 id="2-梯度下降算法求解凸优化问题"><a href="#2-梯度下降算法求解凸优化问题" class="headerlink" title="2. 梯度下降算法求解凸优化问题"></a>2. 梯度下降算法求解凸优化问题</h4><p>​    在机器学习算法中，求解最小化损失函数时，可以通过梯度下降法来一步步进行迭代求解，从而得到最小化损失函数的模型参数值，梯度下降算法不一定能够找到全局的最优解，有可能是一个局部最优解。然而，如果损失函数是凸函数，那么梯度下降法得到的解就一定是全局最优解。  在这里不加证明的给出梯度下降法的迭代格式:</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha\nabla_{\theta_j} {J(\theta)}</script><p>其中$\theta_j$为假设函数$h_\theta(x)$的参数值，$\alpha \in (0,1]$且为常数代表模型学习速率，$\nabla _{\theta_j} J(\theta)$为损失函数$J(\theta)$对参数$\theta_j$的梯度。</p>
<p>​    下面使用梯度下降算法求解上面推导出的线性回归模型的损失函数$ J(\theta)=\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2$，为了实现该算法首先要求出损失函数$J(\theta)$对参数$\theta_j$的梯度：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta_j} J(\theta) &=\nabla_{\theta_j} \dfrac{1}{2} \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\\
&=2\cdot\dfrac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \nabla_{\theta_j}(h_\theta(x^{(i)})-y^{(i)})\\
&=\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)}) \nabla_{\theta_j}\theta^Tx^{(i)}\\
&=\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
\end{aligned}</script><p>因此在线性回归模型中利用所有的样本数据，训练梯度下降算法的完整迭代格式为</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \ \ \ ( j\ \  for \ \ 0 \ \sim \ n)</script><p>上述迭代过程每次迭代都会使用所有的样本数据，数学上已经证明线性回归模型的损失函数通过梯度下降算法求解一定会全局收敛，所以如果要编程实现该算法只需要控制迭代次数即可，不过对于线性回归模型的求解一般不用梯度下降算法，还有更容易实现且更快捷的形式—正规方程。</p>
<h4 id="3-正规方程"><a href="#3-正规方程" class="headerlink" title="3. 正规方程"></a>3. 正规方程</h4><p>多元线性回归的损失函数为：</p>
<script type="math/tex; mode=display">
J=\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2</script><p>其中：$\hat{y}^{(i)} = \theta_{0} + \theta_{1}X_{1}^{(i)} + \theta_{2}X_{2}^{(i)} + … + \theta_{n}X_{n}^{(i)}$ 。</p>
<p>对 $J$ 求导为：</p>
<script type="math/tex; mode=display">
\nabla J=(\frac{\partial J}{\partial \theta_0},\frac{\partial J}{\partial \theta_1},...,\frac{\partial J}{\partial \theta_n})</script><p>其中：$\frac{\partial J}{\partial \theta_i}$ 为偏导数，与导数的求法一样。</p>
<p>对 $\nabla J$ 进一步计算：</p>
<script type="math/tex; mode=display">
\nabla J(\theta) =  \begin{pmatrix} \frac{\partial J}{\partial \theta_0} \\\ \frac{\partial J}{\partial \theta_1} \\\ \frac{\partial J}{\partial \theta_2} \\\ \cdots \\\ \frac{\partial J}{\partial \theta_n} \end{pmatrix} =   \begin{pmatrix} \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-1) \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_1^{(i)}) \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_2^{(i)}) \\\ \cdots \\\ \sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\theta)·(-X_n^{(i)}) \end{pmatrix} = 2·\begin{pmatrix} \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)}) \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_1^{(i)} \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_2^{(i)} \\\ \cdots \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_n^{(i)} \end{pmatrix}</script><p>其中：$X_b = \begin{pmatrix}<br>1 &amp; X_1^{(1)} &amp; X_2^{(1)} &amp; \cdots &amp; X_n^{(1)} \\<br>1 &amp; X_1^{(2)} &amp; X_2^{(2)} &amp; \cdots &amp; X_n^{(2)} \\<br>\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\<br>1 &amp; X_1^{(m)} &amp; X_2^{(m)} &amp; \cdots &amp; X_n^{(m)}<br>\end{pmatrix}$</p>
<p>​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：</p>
<script type="math/tex; mode=display">
\nabla J(\theta)  = \frac{2}{m}\begin{pmatrix} \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)}) \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_1^{(i)} \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_2^{(i)} \\\ \cdots \\\ \sum_{i=1}^{m}(X_b^{(i)}\theta - y^{(i)})·X_n^{(i)} \end{pmatrix}</script><p>​        此时，目标函数就成了使 $\frac{1}{m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：</p>
<script type="math/tex; mode=display">
J(\theta) = MSE(y, \hat{y})</script><p>​        </p>
<p>注1. 有时候目标函数也去 $\frac{1}{\boldsymbol{2}m}\sum_{i=1}^{m}(y^{(i)} - \hat{y}^{(i)})^2$ ,其中的<strong>2</strong>是为了抵消梯度中的2，实际的效果有限。</p>
<p>注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。</p>
<h4 id="2-实现代码"><a href="#2-实现代码" class="headerlink" title="2.实现代码"></a>2.实现代码</h4><h5 id="2-1-python版本实现"><a href="#2-1-python版本实现" class="headerlink" title="2.1 python版本实现"></a>2.1 python版本实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Jan  8 17:25:39 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: lixin</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,lr=<span class="number">.01</span>, num_iters=<span class="number">10000</span>,tolerance=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.num_iters = num_iters</span><br><span class="line">        self.w = <span class="literal">None</span></span><br><span class="line">        self.toterance = tolerance</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self,x_train, y_train)</span>:</span></span><br><span class="line">        n_samples = len(x_train)</span><br><span class="line">        x_train = np.column_stack((np.ones(n_samples), x_train)) <span class="comment">#也可以写成np.c_</span></span><br><span class="line">        </span><br><span class="line">        self.w = <span class="number">.01</span> * np.ones(x_train.shape[<span class="number">1</span>])</span><br><span class="line">        self.loss_ = [<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># w_&#123;i&#125; = w_&#123;i&#125; - lr * (h(w_&#123;i&#125;) - y)*x_&#123;i&#125; 迭代公式</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> range(self.num_iters):</span><br><span class="line">            self.count += <span class="number">1</span></span><br><span class="line">            raw_output = np.matmul(x_train, self.w)</span><br><span class="line">            errors =  raw_output - y_train</span><br><span class="line">            loss = <span class="number">1</span>/(<span class="number">2</span> * n_samples) * errors.dot(errors)</span><br><span class="line">            delta_loss = loss - self.loss_[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">            self.loss_.append(loss)</span><br><span class="line">            <span class="keyword">if</span> np.abs(delta_loss) &lt; self.toterance:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                grad = (<span class="number">1.0</span> /n_samples) *np.matmul(x_train.T, np.array(errors))</span><br><span class="line">                self.w -= self.lr * grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self,x_test)</span>:</span></span><br><span class="line">        x_test = np.column_stack((np.ones(len(x_test)), x_test))</span><br><span class="line">        </span><br><span class="line">        output = np.matmul(x_test, self.w)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    </span><br><span class="line">    num_inputs = <span class="number">2</span></span><br><span class="line">    num_examples = <span class="number">10000</span></span><br><span class="line">    true_w = [<span class="number">6.4</span>, <span class="number">-3.2</span>]</span><br><span class="line">    true_b = <span class="number">2.3</span></span><br><span class="line">    features = np.random.random((num_examples, num_inputs))</span><br><span class="line">    labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">    labels += np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>,size = len(labels))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> sklearn.model_selection</span><br><span class="line">    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = <span class="number">.20</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">    lr = LinearRegression()</span><br><span class="line">    lr.fit(x_train,y_train)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"回归系数:"</span>,lr.w)</span><br><span class="line">    print(<span class="string">"迭代次数:"</span>,lr.count)</span><br><span class="line">    </span><br><span class="line">    y_pred = lr.predict(x_test)</span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">    mse = metrics.mean_squared_error(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"MSE: %.4f"</span> % mse)</span><br><span class="line"></span><br><span class="line">    mae = metrics.mean_absolute_error(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"MAE: %.4f"</span> % mae)</span><br><span class="line"></span><br><span class="line">    R2 = metrics.r2_score(y_test,y_pred)</span><br><span class="line">    print(<span class="string">"R2: %.4f"</span> % R2)</span><br></pre></td></tr></table></figure>
<h5 id="2-2-Scala版本实现"><a href="#2-2-Scala版本实现" class="headerlink" title="2.2 Scala版本实现"></a>2.2 Scala版本实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>&#125;</span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Scala 版本的实现</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LinearRegression</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> num_inputs = <span class="number">2</span></span><br><span class="line">    <span class="keyword">val</span> num_examples = <span class="number">1000</span></span><br><span class="line">    <span class="keyword">val</span> x_train: <span class="type">BDM</span>[<span class="type">Double</span>] = <span class="type">BDM</span>.rand(num_examples, num_inputs)</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](num_examples, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_cat = <span class="type">BDM</span>.horzcat(ones, x_train)</span><br><span class="line">    <span class="keyword">val</span> y_train = x_cat * <span class="type">BDV</span>(<span class="number">2.3</span>, <span class="number">6.4</span>, <span class="number">-3.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">LinearRegression</span>(num_iters = <span class="number">10000</span>)</span><br><span class="line">    <span class="keyword">val</span> weights = model.fit(x_train, y_train)</span><br><span class="line">    <span class="keyword">val</span> predictions = model.predict(weights, x_train)</span><br><span class="line">    println(<span class="string">"梯度下降求解的权重为："</span> + weights)</span><br><span class="line">    println(predictions)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>(<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y_train: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line">    <span class="keyword">val</span> n_features = x_train.cols</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDV</span>.ones[<span class="type">Double</span>](n_features) :* <span class="number">.01</span> <span class="comment">// 注意是:*</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loss_lst: <span class="type">ArrayBuffer</span>[<span class="type">Double</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Double</span>]()</span><br><span class="line">    loss_lst.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to num_iters <span class="keyword">if</span> flag) &#123;</span><br><span class="line">      <span class="keyword">val</span> raw_output = x_train * weights</span><br><span class="line">      <span class="keyword">val</span> error = raw_output - y_train</span><br><span class="line">      <span class="keyword">val</span> loss: <span class="type">Double</span> = error.t * error</span><br><span class="line">      <span class="keyword">val</span> delta_loss = loss - loss_lst.apply(loss_lst.size - <span class="number">1</span>)</span><br><span class="line">      loss_lst.append(loss)</span><br><span class="line">      <span class="keyword">if</span> (scala.math.abs(delta_loss) &lt; tolerance) &#123;</span><br><span class="line">        flag = <span class="literal">false</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> gradient = (error.t * x_train) :/ n_samples.toDouble</span><br><span class="line">        weights = weights - (gradient :* lr).t</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDV</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(<span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>), x)</span><br><span class="line">    <span class="keyword">val</span> output = x_test * weights</span><br><span class="line">    output</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参考文献：1. <a href="https://segmentfault.com/a/1190000017048213" target="_blank" rel="noopener">机器学习之梯度下降法与线性回归</a></p>

    
  </div>

</article>


   

   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2019/11/27/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2020/01/16/%E9%99%8D%E7%BB%B4_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
