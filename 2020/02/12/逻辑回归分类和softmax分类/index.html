<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>逻辑回归分类和softmax分类 | Lixin</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="机器学习、数据挖掘" />
  

  <meta name="description" content="逻辑回归分类和softmax分类1.逻辑回归1.1 算法原理一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。 1.2 假设函数​    回顾线性回归中的假设函数$h_\theta(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n=\theta^Tx$这表示的是一个超平">
<meta property="og:type" content="article">
<meta property="og:title" content="逻辑回归分类和softmax分类">
<meta property="og:url" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;index.html">
<meta property="og:site_name" content="Lixin">
<meta property="og:description" content="逻辑回归分类和softmax分类1.逻辑回归1.1 算法原理一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。 1.2 假设函数​    回顾线性回归中的假设函数$h_\theta(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n=\theta^Tx$这表示的是一个超平">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;逻辑回归分类和softmax分类&#x2F;sigmoid.jpg">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;logx.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;logistic.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;softmax.png">
<meta property="og:updated_time" content="2021-03-02T08:24:43.109Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;02&#x2F;12&#x2F;%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB&#x2F;逻辑回归分类和softmax分类&#x2F;sigmoid.jpg">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑回归分类和softmax分类"><span class="toc-text">逻辑回归分类和softmax分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-逻辑回归"><span class="toc-text">1.逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-算法原理"><span class="toc-text">1.1 算法原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-假设函数"><span class="toc-text">1.2 假设函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-构造逻辑回归算法的损失函数"><span class="toc-text">1.3 构造逻辑回归算法的损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-梯度下降过程的向量化"><span class="toc-text">1.5 梯度下降过程的向量化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-总结"><span class="toc-text">1.6 总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-scala-代码实现"><span class="toc-text">1.7 scala 代码实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-7-python-代码实现"><span class="toc-text">1.7 python 代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-softmax回归"><span class="toc-text">2.softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-算法原理和步骤"><span class="toc-text">2.1 算法原理和步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-scala-代码"><span class="toc-text">2.2 scala 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-python-代码"><span class="toc-text">2.3 python 代码</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-逻辑回归分类和softmax分类" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">逻辑回归分类和softmax分类</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2020.02.12</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Lixin</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h3 id="逻辑回归分类和softmax分类"><a href="#逻辑回归分类和softmax分类" class="headerlink" title="逻辑回归分类和softmax分类"></a>逻辑回归分类和softmax分类</h3><h3 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1.逻辑回归"></a>1.逻辑回归</h3><h4 id="1-1-算法原理"><a href="#1-1-算法原理" class="headerlink" title="1.1 算法原理"></a>1.1 算法原理</h4><p>一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。</p>
<h4 id="1-2-假设函数"><a href="#1-2-假设函数" class="headerlink" title="1.2 假设函数"></a>1.2 假设函数</h4><p>​    回顾线性回归中的假设函数$h_\theta(x)=\theta_0+\theta_1x_1+\cdots+\theta_nx_n=\theta^Tx$这表示的是一个超平面，逻辑回归中的超平面与线性回归中的超平面并无什么本质上的差异，但是线性回归是回归问题逻辑回归是分类问题两者本质上不同，线性回归中样本集中在超平面附近且没有类别差异而逻辑回归中样本点被所超平面分割且有明确的类别。因此逻辑回归的假设函数需要判断样本点在超平面上还是超平面下，所以给出一种映射关系：</p>
<script type="math/tex; mode=display">
g(z) = \dfrac{1}{1+e^{-z}}</script><p>函数$ g(z) $称为$sigmoid$函数，其函数图像如下图所示：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/逻辑回归分类和softmax分类\sigmoid.jpg" alt></p>
<p>使用$sigmoid$函数对超平面进行映射是因为它有一些很好的性质：</p>
<ul>
<li><p>$sigmoid$函数把所有样本点都映射到(0,1)区间内</p>
</li>
<li><p>$sigmoid$函数连续可导，求导后的形式很$nice$</p>
<script type="math/tex; mode=display">
\begin{aligned}
g'(z) &=\dfrac{\mathrm{d}}{\mathrm{d}z} \dfrac{1}{1+e^{-z}}\\
&=\dfrac{1}{(1+e^{-z})^2}(e^{-z})\\
&=\dfrac{1}{1+e^{-z}} \cdot \left(1-  \dfrac{1}{1+e^{-z}} \right)\\
&=g(z)\cdot(1-g(z))
\end{aligned}</script></li>
</ul>
<p>综上可以给出逻辑回归算法的假设函数$ h_\theta(x) $</p>
<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^Tx)=\dfrac{1}{1+e^{-\theta^Tx}}</script><h4 id="1-3-构造逻辑回归算法的损失函数"><a href="#1-3-构造逻辑回归算法的损失函数" class="headerlink" title="1.3 构造逻辑回归算法的损失函数"></a>1.3 构造逻辑回归算法的损失函数</h4><p>​    不妨假设含有$m$个样本数据($x^{(1)}$,$y^{(1)}$)、($x^{(2)}$,$y^{(2)}$)、$\cdots$、($x^{(m)}$,$y^{(m)}$)，$y^{(i)} \in \{0,1\}$。由于$h_\theta(x) \in (0,1)$，且对于某个样本数据来说它只能属于两种类别中的某一类，所以有如下等式成立</p>
<script type="math/tex; mode=display">
\begin{aligned}
P(y=1|x;\theta)&=h_\theta(x)\\
P(y=0|x;\theta)&=1-h_\theta(x)
\end{aligned}</script><p>训练梯度下降算法的完整迭代更新格式为</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \ \ \ ( j\ \  for \ \ 0 \ \sim \ n)</script><p><strong>可以发现逻辑回归模型的迭代更新格式和线性回归模型的迭代更新格式完全一样，但是它们的假设函数$ h_\theta(x) $的函数表达式是不一样的</strong>。</p>
<h4 id="1-5-梯度下降过程的向量化"><a href="#1-5-梯度下降过程的向量化" class="headerlink" title="1.5 梯度下降过程的向量化"></a>1.5 梯度下降过程的向量化</h4><p>​    向量化是使用矩阵计算来代替$for$循环 ，以简化计算过程提高效率 ，上述迭代更新的过程中有一个连加符号，如果使用$for$循环则需要执行$m$次。在此之前需要先定义一些矩阵，不妨令：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Y&=
\begin{bmatrix}
(y^{(1)}) \\ (y^{(2)}) \\ \vdots \\ (y^{(m)})
\end{bmatrix}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\theta=
\begin{bmatrix}
\theta_0 \\ \theta_1 \\ \vdots \\ \theta_n
\end{bmatrix}\\
~\\
X&=
\begin{bmatrix}
(x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\ (x^{(m)})^T
\end{bmatrix}
=
\begin{bmatrix}
1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
\vdots & \vdots & \vdots & \cdots & \vdots \\
1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
\end{bmatrix}\\
\end{aligned}</script><p>则</p>
<script type="math/tex; mode=display">
\begin{aligned} \phi&=g(X\theta)= \begin{bmatrix} h_\theta(x^{(1)}) \\ h_\theta(x^{(2)}) \\ \vdots \\ h_\theta(x^{(m)}) \end{bmatrix} =g\left(\begin{bmatrix} \theta_0+\theta_1x_1^{(1)}+\cdots + \theta_nx_n^{(1)} \\ \theta_0+\theta_1x_1^{(2)}+\cdots + \theta_nx_n^{(2)}\\ \vdots \\ \theta_0+\theta_1x_1^{(m)}+\cdots + \theta_nx_n^{(m)} \end{bmatrix}\right)\\ ~\\ E&=h_\theta(x)-y=\phi-Y= \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_m \end{bmatrix}\\ \end{aligned}\\</script><p>所以</p>
<script type="math/tex; mode=display">
\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} = X\Epsilon</script><p>最后</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\alpha \sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \ \ \ ( j\ \  for \ \ 0 \ \sim \ n)</script><p>向量化后可改为</p>
<script type="math/tex; mode=display">
\theta:=\theta-\alpha X\Epsilon \ \ \ \ \ \\</script><h4 id="1-6-总结"><a href="#1-6-总结" class="headerlink" title="1.6 总结"></a>1.6 总结</h4><p>​    逻辑回归算法是机器学习算法中比较好理解的分类模型，训练速度也很快。因为只需要存储各个维度的特征值的原因，对资源尤其是内存的占用会比较小 ，在引入了$softmax$以后可以处理多分类的问题。 任何机器学习模型都是有自己的假设，在这个假设成立的情况下模型才是适用的。逻辑回归的第一个基本假设是<strong>假设样本数据的先验分布为伯努利分布。</strong> </p>
<p>总结一下：逻辑回归模型概率估算:</p>
<script type="math/tex; mode=display">
\hat{p}=h_\theta(x)=\sigma(\theta^T\cdot x)</script><p>逻辑函数：</p>
<script type="math/tex; mode=display">
\sigma(t)=\frac{1}{1+exp(-t)}</script><p>单个训练实例的损失函数:</p>
<script type="math/tex; mode=display">
c(\theta)=
\begin{cases}
-log(h_{\theta (x)}) & (y=1)\\
-log(1-\theta (x)) & (y=0)
\end{cases}</script><p>$-log p$ 的图像所下：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/logx.png" alt></p>
<p>我们可以看到，当$p$接近于$0$的时候，$-\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。</p>
<h4 id="1-7-scala-代码实现"><a href="#1-7-scala-代码实现" class="headerlink" title="1.7 scala 代码实现"></a>1.7 scala 代码实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ml.scrath.classification</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogitRegression</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> dataS = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D:/data/iris.csv"</span>).getLines().toSeq.tail</span><br><span class="line">      .map&#123;_.split(<span class="string">","</span>).filter(_.length() &gt; <span class="number">0</span>).map(_.toDouble)&#125;</span><br><span class="line">      .toArray</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">BDM</span>(dataS:_*)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> features = data(<span class="number">0</span> to <span class="number">98</span>, <span class="number">0</span> to <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> labels = data(<span class="number">0</span> to <span class="number">98</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">LogitRegression</span></span><br><span class="line">    <span class="keyword">val</span> w = model.fit(features,labels)</span><br><span class="line">    <span class="keyword">val</span> predictions = model.predict(w, features)</span><br><span class="line">    <span class="keyword">val</span> predictionsNlabels = predictions.toArray.zip(labels.toArray)</span><br><span class="line">    <span class="keyword">val</span> rate = predictionsNlabels.filter(f =&gt; f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble</span><br><span class="line">    println(<span class="string">"正确率为："</span> + rate)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogitRegression</span> (<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y_train: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line">    <span class="keyword">val</span> n_features = x_train.cols</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDV</span>.ones[<span class="type">Double</span>](n_features) :* <span class="number">.01</span> <span class="comment">// 注意是:*</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loss_lst: <span class="type">ArrayBuffer</span>[<span class="type">Double</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Double</span>]()</span><br><span class="line">    loss_lst.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to num_iters <span class="keyword">if</span> flag) &#123;</span><br><span class="line">      <span class="keyword">val</span> raw_output = (x_train * weights).map(sigmoid(_))</span><br><span class="line">      <span class="keyword">val</span> error = raw_output - y_train</span><br><span class="line">      <span class="keyword">val</span> loss: <span class="type">Double</span> = error.t * error</span><br><span class="line">      <span class="keyword">val</span> delta_loss = loss - loss_lst.apply(loss_lst.size - <span class="number">1</span>)</span><br><span class="line">      loss_lst.append(loss)</span><br><span class="line">      <span class="keyword">if</span> (scala.math.abs(delta_loss) &lt; tolerance) &#123;</span><br><span class="line">        flag = <span class="literal">false</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> gradient = (error.t * x_train) :/ n_samples.toDouble</span><br><span class="line">        weights = weights - (gradient :* lr).t</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span></span>(inX: <span class="type">Double</span>) = &#123;</span><br><span class="line">    <span class="number">1.0</span> / (<span class="number">1</span> + scala.math.exp(-inX))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDV</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(<span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>), x)</span><br><span class="line">    <span class="keyword">val</span> output = (x_test * weights).map(sigmoid(_)).map(x =&gt; <span class="keyword">if</span>(x &gt; <span class="number">0.5</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>)</span><br><span class="line">    output</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-7-python-代码实现"><a href="#1-7-python-代码实现" class="headerlink" title="1.7 python 代码实现"></a>1.7 python 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> train_test_split, accuracy_score</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Plot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, learning_rate=<span class="number">.1</span>, n_iterations=<span class="number">4000</span>)</span>:</span></span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.n_iterations = n_iterations</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self, n_features)</span>:</span></span><br><span class="line">        limit = np.sqrt(<span class="number">1</span> / n_features)</span><br><span class="line">        w = np.random.uniform(-limit, limit, (n_features, <span class="number">1</span>))</span><br><span class="line">        b = <span class="number">0</span></span><br><span class="line">        self.w = np.insert(w, <span class="number">0</span>, b, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        m_samples, n_features = X.shape</span><br><span class="line">        self.initialize_weights(n_features)</span><br><span class="line">        <span class="comment"># 为X增加一列特征x1，x1 = 0</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        y = np.reshape(y, (m_samples, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度训练n_iterations轮</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iterations):</span><br><span class="line">            h_x = X.dot(self.w)</span><br><span class="line">            y_pred = sigmoid(h_x)</span><br><span class="line">            w_grad = X.T.dot(y_pred - y)</span><br><span class="line">            self.w = self.w - self.learning_rate * w_grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        h_x = X.dot(self.w)</span><br><span class="line">        y_pred = np.round(sigmoid(h_x))</span><br><span class="line">        <span class="keyword">return</span> y_pred.astype(int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X = data.data[data.target != <span class="number">0</span>]</span><br><span class="line">    y = data.target[data.target != <span class="number">0</span>]</span><br><span class="line">    y[y == <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y[y == <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, seed=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    clf = LogisticRegression()</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    y_pred = np.reshape(y_pred, y_test.shape)</span><br><span class="line"></span><br><span class="line">    accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy)</span><br><span class="line"></span><br><span class="line">    Plot().plot_in_2d(X_test, y_pred, title=<span class="string">"Logistic Regression"</span>, accuracy=accuracy)</span><br></pre></td></tr></table></figure>
<p>Python结果展示如下【基于PCA将高维数据投影而得】：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/logistic.png" alt></p>
<h3 id="2-softmax回归"><a href="#2-softmax回归" class="headerlink" title="2.softmax回归"></a>2.softmax回归</h3><h4 id="2-1-算法原理和步骤"><a href="#2-1-算法原理和步骤" class="headerlink" title="2.1 算法原理和步骤"></a>2.1 算法原理和步骤</h4><p>对逻辑回归模型做推广，可以支持多个类别了。</p>
<p>原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。</p>
<ol>
<li><p>用零（或小的随机值）初始化权重矩阵和偏置值.</p>
</li>
<li><p>对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:</p>
<script type="math/tex; mode=display">
s_k(x)= x\cdot w_k</script><p>向量化表示上式的话，可以写为</p>
</li>
</ol>
<script type="math/tex; mode=display">
   socres = X \cdot W</script><p>   $X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.</p>
<ol>
<li><p>应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:</p>
<script type="math/tex; mode=display">
\hat{p}_k=\sigma(s(x))_k=\frac{exp(s_k(x))}{\sum_{j=1}^{K}exp(s_j(x))}</script></li>
<li><p>计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:</p>
<script type="math/tex; mode=display">
J(W)=-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(\hat{p}_k^{(i)})</script></li>
<li><p>对于类别k的交叉熵梯度向量:</p>
<script type="math/tex; mode=display">
\Delta_{w_k}J(W)=\frac{1}{m}\sum_{i=1}^{m}(\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}</script></li>
<li><p>更新每个类的权重$W$</p>
<script type="math/tex; mode=display">
w_k = w_k - \eta \Delta_{w_k}J</script></li>
</ol>
<p>​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：</p>
<script type="math/tex; mode=display">
H(p,q)=-\sum_xp(x)logq(x)</script><h4 id="2-2-scala-代码"><a href="#2-2-scala-代码" class="headerlink" title="2.2 scala 代码"></a>2.2 scala 代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ml.scrath.classification</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> breeze.numerics._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">softMax</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> dataS = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D:/data/iris.csv"</span>).getLines().toSeq.tail</span><br><span class="line">      .map &#123;</span><br><span class="line">        _.split(<span class="string">","</span>).filter(_.length() &gt; <span class="number">0</span>).map(_.toDouble)</span><br><span class="line">      &#125;</span><br><span class="line">      .toArray</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">BDM</span>(dataS: _*)</span><br><span class="line">    <span class="keyword">val</span> features = data(::, <span class="number">0</span> to <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> labels = data(::, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> soft = <span class="keyword">new</span> <span class="type">SoftMaxRegression</span>()</span><br><span class="line">    <span class="keyword">val</span> w = soft.fit(features, labels)</span><br><span class="line">    println(w)</span><br><span class="line">    <span class="keyword">val</span> predictions = soft.predict(w, features)</span><br><span class="line">    <span class="keyword">val</span> predictionsNlabels = predictions.toArray.zip(labels.toArray)</span><br><span class="line">    <span class="keyword">val</span> rate = predictionsNlabels.filter(f =&gt; f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble</span><br><span class="line">    println(<span class="string">"正确率为："</span> + rate) <span class="comment">// 正确率为0.9664</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftMaxRegression</span>(<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ncol = x_train.cols</span><br><span class="line">    <span class="keyword">val</span> nclasses = y.toArray.distinct.length</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDM</span>.ones[<span class="type">Double</span>](ncol, nclasses) :* <span class="number">1.0</span> / nclasses</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (iterations &lt;- <span class="number">0</span> to num_iters) &#123;</span><br><span class="line">      <span class="keyword">val</span> logits = x_train * weights</span><br><span class="line">      <span class="keyword">val</span> probs = softmax(logits)</span><br><span class="line">      <span class="keyword">val</span> y_one_hot = one_hot(y)</span><br><span class="line"><span class="comment">//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble</span></span><br><span class="line">      <span class="keyword">val</span> error: <span class="type">BDM</span>[<span class="type">Double</span>] = probs - y_one_hot</span><br><span class="line">      <span class="keyword">val</span> gradients = (x_train.t * error) :/ n_samples.toDouble</span><br><span class="line"></span><br><span class="line">      weights -= gradients :* lr</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">softmax</span></span>(logits: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> scores = exp(logits)</span><br><span class="line">    <span class="keyword">val</span> divisor = sum(scores(*, ::))</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to scores.cols - <span class="number">1</span>) &#123;</span><br><span class="line">      scores(::, i) := scores(::, i) :/ divisor</span><br><span class="line">    &#125;</span><br><span class="line">    scores</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">one_hot</span></span>(y: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = y.length</span><br><span class="line">    <span class="keyword">val</span> n_classes = y.toArray.toSet.size</span><br><span class="line">    <span class="keyword">val</span> one_hot = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](n_samples, n_classes)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to n_samples - <span class="number">1</span>) &#123;</span><br><span class="line">      one_hot(i)(y(i).toInt) = <span class="number">1.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">BDM</span>(one_hot: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDM</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> predictions = argmax(x_test * weights, <span class="type">Axis</span>._1)</span><br><span class="line">    predictions</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="2-3-python-代码"><a href="#2-3-python-代码" class="headerlink" title="2.3 python 代码"></a>2.3 python 代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Feb 12 11:58:06 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: lixin</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> train_test_split, accuracy_score</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Plot</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxRegressorII</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,learning_rate = <span class="number">0.1</span>,n_iters = <span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.n_iters = n_iters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y_true, n_classes)</span>:</span></span><br><span class="line"></span><br><span class="line">        x_train = np.column_stack((np.ones(len(X)),X))</span><br><span class="line">        </span><br><span class="line">        self.n_samples, n_features = x_train.shape</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        </span><br><span class="line">        self.weights = np.random.rand(n_features,self.n_classes)</span><br><span class="line">        all_losses = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iters):</span><br><span class="line">            logits = np.dot(x_train, self.weights)</span><br><span class="line">            probs = self.softmax(logits)</span><br><span class="line">            y_one_hot = self.one_hot(y_true)</span><br><span class="line">            loss = self.cross_entropy(y_one_hot, probs)</span><br><span class="line">            all_losses.append(loss)</span><br><span class="line"></span><br><span class="line">            gradients = (<span class="number">1</span> / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))</span><br><span class="line"></span><br><span class="line">            self.weights = self.weights - self.learning_rate * gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#            if i % 100 == 0:</span></span><br><span class="line"><span class="comment">#                print(f'Iteration number: &#123;i&#125;, loss: &#123;np.round(loss, 4)&#125;')</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.weights, all_losses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line"></span><br><span class="line">        x_test = np.column_stack((np.ones(len(X)), X))</span><br><span class="line">        scores = np.dot(x_test, self.weights)</span><br><span class="line">        probs = self.softmax(scores)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(probs, axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, logits)</span>:</span></span><br><span class="line">        exp = np.exp(logits)</span><br><span class="line">        sum_exp = np.sum(np.exp(logits), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> exp / sum_exp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(self, y_true, scores)</span>:</span></span><br><span class="line">        loss = - (<span class="number">1</span> / self.n_samples) * np.sum(y_true * np.log(scores))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        one_hot = np.zeros((self.n_samples, self.n_classes))</span><br><span class="line">        one_hot[np.arange(self.n_samples), y.T] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X= data.data</span><br><span class="line">    y = data.target</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, seed=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    clf = SoftmaxRegressorII()</span><br><span class="line">    ll = clf.train(X_train, y_train,<span class="number">3</span>)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    y_pred = np.reshape(y_pred, y_test.shape)</span><br><span class="line"></span><br><span class="line">    accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reduce dimension to two using PCA and plot the results</span></span><br><span class="line">    Plot().plot_in_2d(X_test, y_pred, title=<span class="string">"SoftMax Regression"</span>, accuracy=accuracy)</span><br></pre></td></tr></table></figure>
<p>结果如图所示：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/softmax.png" alt></p>

    
  </div>

</article>


   

   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2020/01/16/%E9%99%8D%E7%BB%B4_%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2020/03/18/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
