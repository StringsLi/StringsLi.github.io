<!DOCTYPE html>
<html lang="en">
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        逻辑回归分类和softmax分类 - undefined
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 人总要有些乐趣，比如编程和阅读 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/img/lixin.jpg" />
        </div>
        <div class="name">
            <i>Lixin</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑回归分类和softmax分类"><span class="toc-text">逻辑回归分类和softmax分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-逻辑回归"><span class="toc-text">1.逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-算法原理"><span class="toc-text">1.1 算法原理</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-scala-代码实现"><span class="toc-text">1.2 scala 代码实现</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-3-python-代码实现"><span class="toc-text">1.3 python 代码实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-softmax回归"><span class="toc-text">2.softmax回归</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-算法原理和步骤"><span class="toc-text">2.1 算法原理和步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-scala-代码"><span class="toc-text">2.2 scala 代码</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-python-代码"><span class="toc-text">2.3 python 代码</span></a></li></ol></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 人总要有些乐趣，比如编程和阅读 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        逻辑回归分类和softmax分类
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2020-02-12 17:50:40</span></span>
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h3 id="逻辑回归分类和softmax分类"><a href="#逻辑回归分类和softmax分类" class="headerlink" title="逻辑回归分类和softmax分类"></a>逻辑回归分类和softmax分类</h3><h4 id="1-逻辑回归"><a href="#1-逻辑回归" class="headerlink" title="1.逻辑回归"></a>1.逻辑回归</h4><h5 id="1-1-算法原理"><a href="#1-1-算法原理" class="headerlink" title="1.1 算法原理"></a>1.1 算法原理</h5><p>一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。</p>
<p>逻辑回归模型概率估算:</p>
<script type="math/tex; mode=display">\hat{p}=h_\theta(x)=\sigma(\theta^T\cdot x)</script><p>逻辑函数：</p>
<script type="math/tex; mode=display">\sigma(t)=\frac{1}{1+exp(-t)}</script><p>预测模型：</p>
<script type="math/tex; mode=display">\hat{y}=
\begin{cases}
0 & (\hat{p}<0.5)\\
1 & (\hat{p}\geq0.5)
\end{cases}</script><p>单个训练实例的损失函数:</p>
<script type="math/tex; mode=display">c(\theta)=
\begin{cases}
-log(\hat{p}) & (y=1)\\
-log(1-\hat{p}) & (y=0)
\end{cases}</script><p>我们可以看到，当$p$接近于$0$的时候，$-\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。</p>
<p>逻辑回归成本函数:</p>
<script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(\hat{p}^{(i)})+(1-y^{(i)})log(1-\hat{p}^{(i)})]</script><p>坏消息是，这个函数没有已知的闭式方程(也就是不尊在一个标准方差的等价方程)。好消息，这是个凸函数，通过梯度下降算法保证能够找出全局最小值。</p>
<p>Logistic损失函数的偏导数:</p>
<script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}(\sigma(\theta^T \cdot x^{(i)})-y^{(i)})x_j^{(i)}</script><script type="math/tex; mode=display">\frac{\partial}{\partial\theta_j}MSE(\theta)=\frac{2}{m}\sum_{i=1}^{m}(\theta^T\cdot x^{i}-y^{i})x_j^{i}</script><h5 id="1-2-scala-代码实现"><a href="#1-2-scala-代码实现" class="headerlink" title="1.2 scala 代码实现"></a>1.2 scala 代码实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ml.scrath.classification</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogitRegression</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> dataS = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D:/data/iris.csv"</span>).getLines().toSeq.tail</span><br><span class="line">      .map&#123;_.split(<span class="string">","</span>).filter(_.length() &gt; <span class="number">0</span>).map(_.toDouble)&#125;</span><br><span class="line">      .toArray</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">BDM</span>(dataS:_*)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> features = data(<span class="number">0</span> to <span class="number">98</span>, <span class="number">0</span> to <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> labels = data(<span class="number">0</span> to <span class="number">98</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> model = <span class="keyword">new</span> <span class="type">LogitRegression</span></span><br><span class="line">    <span class="keyword">val</span> w = model.fit(features,labels)</span><br><span class="line">    <span class="keyword">val</span> predictions = model.predict(w, features)</span><br><span class="line">    <span class="keyword">val</span> predictionsNlabels = predictions.toArray.zip(labels.toArray)</span><br><span class="line">    <span class="keyword">val</span> rate = predictionsNlabels.filter(f =&gt; f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble</span><br><span class="line">    println(<span class="string">"正确率为："</span> + rate)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogitRegression</span> (<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y_train: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line">    <span class="keyword">val</span> n_features = x_train.cols</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDV</span>.ones[<span class="type">Double</span>](n_features) :* <span class="number">.01</span> <span class="comment">// 注意是:*</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> loss_lst: <span class="type">ArrayBuffer</span>[<span class="type">Double</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Double</span>]()</span><br><span class="line">    loss_lst.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to num_iters <span class="keyword">if</span> flag) &#123;</span><br><span class="line">      <span class="keyword">val</span> raw_output = (x_train * weights).map(sigmoid(_))</span><br><span class="line">      <span class="keyword">val</span> error = raw_output - y_train</span><br><span class="line">      <span class="keyword">val</span> loss: <span class="type">Double</span> = error.t * error</span><br><span class="line">      <span class="keyword">val</span> delta_loss = loss - loss_lst.apply(loss_lst.size - <span class="number">1</span>)</span><br><span class="line">      loss_lst.append(loss)</span><br><span class="line">      <span class="keyword">if</span> (scala.math.abs(delta_loss) &lt; tolerance) &#123;</span><br><span class="line">        flag = <span class="literal">false</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> gradient = (error.t * x_train) :/ n_samples.toDouble</span><br><span class="line">        weights = weights - (gradient :* lr).t</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span></span>(inX: <span class="type">Double</span>) = &#123;</span><br><span class="line">    <span class="number">1.0</span> / (<span class="number">1</span> + scala.math.exp(-inX))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDV</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(<span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>), x)</span><br><span class="line">    <span class="keyword">val</span> output = (x_test * weights).map(sigmoid(_)).map(x =&gt; <span class="keyword">if</span>(x &gt; <span class="number">0.5</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>)</span><br><span class="line">    output</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="1-3-python-代码实现"><a href="#1-3-python-代码实现" class="headerlink" title="1.3 python 代码实现"></a>1.3 python 代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> train_test_split, accuracy_score</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Plot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, learning_rate=<span class="number">.1</span>, n_iterations=<span class="number">4000</span>)</span>:</span></span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.n_iterations = n_iterations</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_weights</span><span class="params">(self, n_features)</span>:</span></span><br><span class="line">        limit = np.sqrt(<span class="number">1</span> / n_features)</span><br><span class="line">        w = np.random.uniform(-limit, limit, (n_features, <span class="number">1</span>))</span><br><span class="line">        b = <span class="number">0</span></span><br><span class="line">        self.w = np.insert(w, <span class="number">0</span>, b, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        m_samples, n_features = X.shape</span><br><span class="line">        self.initialize_weights(n_features)</span><br><span class="line">        <span class="comment"># 为X增加一列特征x1，x1 = 0</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        y = np.reshape(y, (m_samples, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度训练n_iterations轮</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iterations):</span><br><span class="line">            h_x = X.dot(self.w)</span><br><span class="line">            y_pred = sigmoid(h_x)</span><br><span class="line">            w_grad = X.T.dot(y_pred - y)</span><br><span class="line">            self.w = self.w - self.learning_rate * w_grad</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = np.insert(X, <span class="number">0</span>, <span class="number">1</span>, axis=<span class="number">1</span>)</span><br><span class="line">        h_x = X.dot(self.w)</span><br><span class="line">        y_pred = np.round(sigmoid(h_x))</span><br><span class="line">        <span class="keyword">return</span> y_pred.astype(int)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X = data.data[data.target != <span class="number">0</span>]</span><br><span class="line">    y = data.target[data.target != <span class="number">0</span>]</span><br><span class="line">    y[y == <span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y[y == <span class="number">2</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, seed=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    clf = LogisticRegression()</span><br><span class="line">    clf.fit(X_train, y_train)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    y_pred = np.reshape(y_pred, y_test.shape)</span><br><span class="line"></span><br><span class="line">    accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy)</span><br><span class="line"></span><br><span class="line">    Plot().plot_in_2d(X_test, y_pred, title=<span class="string">"Logistic Regression"</span>, accuracy=accuracy)</span><br></pre></td></tr></table></figure>
<p>Python结果展示如下【基于PCA将高维数据投影而得】：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/logistic.png" alt></p>
<h4 id="2-softmax回归"><a href="#2-softmax回归" class="headerlink" title="2.softmax回归"></a>2.softmax回归</h4><h5 id="2-1-算法原理和步骤"><a href="#2-1-算法原理和步骤" class="headerlink" title="2.1 算法原理和步骤"></a>2.1 算法原理和步骤</h5><p>对逻辑回归模型做推广，可以支持多个类别了。</p>
<p>原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。</p>
<ol>
<li><p>用零（或小的随机值）初始化权重矩阵和偏置值.</p>
</li>
<li><p>对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:</p>
<script type="math/tex; mode=display">s_k(x)= x\cdot w_k</script><p>向量化表示上式的话，可以写为</p>
<script type="math/tex; mode=display">socres = X \cdot W</script><p>$X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.</p>
</li>
<li><p>应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:</p>
<script type="math/tex; mode=display">\hat{p}_k=\sigma(s(x))_k=\frac{exp(s_k(x))}{\sum_{j=1}^{K}exp(s_j(x))}</script></li>
<li><p>计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:</p>
<script type="math/tex; mode=display">J(W)=-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(\hat{p}_k^{(i)})</script></li>
<li><p>对于类别k的交叉熵梯度向量:</p>
<script type="math/tex; mode=display">\Delta_{w_k}J(W)=\frac{1}{m}\sum_{i=1}^{m}(\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}</script></li>
<li><p>更新每个类的权重$W$</p>
<script type="math/tex; mode=display">w_k = w_k - \eta \Delta_{w_k}J</script></li>
</ol>
<p>​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：</p>
<script type="math/tex; mode=display">H(p,q)=-\sum_xp(x)logq(x)</script><h5 id="2-2-scala-代码"><a href="#2-2-scala-代码" class="headerlink" title="2.2 scala 代码"></a>2.2 scala 代码</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ml.scrath.classification</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span> =&gt; <span class="type">BDM</span>, <span class="type">DenseVector</span> =&gt; <span class="type">BDV</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> breeze.numerics._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">softMax</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> dataS = scala.io.<span class="type">Source</span>.fromFile(<span class="string">"D:/data/iris.csv"</span>).getLines().toSeq.tail</span><br><span class="line">      .map &#123;</span><br><span class="line">        _.split(<span class="string">","</span>).filter(_.length() &gt; <span class="number">0</span>).map(_.toDouble)</span><br><span class="line">      &#125;</span><br><span class="line">      .toArray</span><br><span class="line">    <span class="keyword">val</span> data = <span class="type">BDM</span>(dataS: _*)</span><br><span class="line">    <span class="keyword">val</span> features = data(::, <span class="number">0</span> to <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> labels = data(::, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> soft = <span class="keyword">new</span> <span class="type">SoftMaxRegression</span>()</span><br><span class="line">    <span class="keyword">val</span> w = soft.fit(features, labels)</span><br><span class="line">    println(w)</span><br><span class="line">    <span class="keyword">val</span> predictions = soft.predict(w, features)</span><br><span class="line">    <span class="keyword">val</span> predictionsNlabels = predictions.toArray.zip(labels.toArray)</span><br><span class="line">    <span class="keyword">val</span> rate = predictionsNlabels.filter(f =&gt; f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble</span><br><span class="line">    println(<span class="string">"正确率为："</span> + rate) <span class="comment">// 正确率为0.9664</span></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftMaxRegression</span>(<span class="params">var lr: <span class="type">Double</span> = 0.01, var tolerance: <span class="type">Double</span> = 1e-6, var num_iters: <span class="type">Int</span> = 1000</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(x: <span class="type">BDM</span>[<span class="type">Double</span>], y: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_train = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ncol = x_train.cols</span><br><span class="line">    <span class="keyword">val</span> nclasses = y.toArray.distinct.length</span><br><span class="line">    <span class="keyword">var</span> weights = <span class="type">BDM</span>.ones[<span class="type">Double</span>](ncol, nclasses) :* <span class="number">1.0</span> / nclasses</span><br><span class="line">    <span class="keyword">val</span> n_samples = x_train.rows</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (iterations &lt;- <span class="number">0</span> to num_iters) &#123;</span><br><span class="line">      <span class="keyword">val</span> logits = x_train * weights</span><br><span class="line">      <span class="keyword">val</span> probs = softmax(logits)</span><br><span class="line">      <span class="keyword">val</span> y_one_hot = one_hot(y)</span><br><span class="line"><span class="comment">//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble</span></span><br><span class="line">      <span class="keyword">val</span> error: <span class="type">BDM</span>[<span class="type">Double</span>] = probs - y_one_hot</span><br><span class="line">      <span class="keyword">val</span> gradients = (x_train.t * error) :/ n_samples.toDouble</span><br><span class="line"></span><br><span class="line">      weights -= gradients :* lr</span><br><span class="line">    &#125;</span><br><span class="line">    weights</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">softmax</span></span>(logits: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> scores = exp(logits)</span><br><span class="line">    <span class="keyword">val</span> divisor = sum(scores(*, ::))</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to scores.cols - <span class="number">1</span>) &#123;</span><br><span class="line">      scores(::, i) := scores(::, i) :/ divisor</span><br><span class="line">    &#125;</span><br><span class="line">    scores</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">one_hot</span></span>(y: <span class="type">BDV</span>[<span class="type">Double</span>]): <span class="type">BDM</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = y.length</span><br><span class="line">    <span class="keyword">val</span> n_classes = y.toArray.toSet.size</span><br><span class="line">    <span class="keyword">val</span> one_hot = <span class="type">Array</span>.ofDim[<span class="type">Double</span>](n_samples, n_classes)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> to n_samples - <span class="number">1</span>) &#123;</span><br><span class="line">      one_hot(i)(y(i).toInt) = <span class="number">1.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">BDM</span>(one_hot: _*)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(weights: <span class="type">BDM</span>[<span class="type">Double</span>], x: <span class="type">BDM</span>[<span class="type">Double</span>]): <span class="type">BDV</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> ones = <span class="type">BDM</span>.ones[<span class="type">Double</span>](x.rows, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> x_test = <span class="type">BDM</span>.horzcat(ones, x)</span><br><span class="line">    <span class="keyword">val</span> predictions = argmax(x_test * weights, <span class="type">Axis</span>._1)</span><br><span class="line">    predictions</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-3-python-代码"><a href="#2-3-python-代码" class="headerlink" title="2.3 python 代码"></a>2.3 python 代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Wed Feb 12 11:58:06 2020</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: lixin</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.insert(<span class="number">0</span>, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> train_test_split, accuracy_score</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Plot</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SoftmaxRegressorII</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,learning_rate = <span class="number">0.1</span>,n_iters = <span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.n_iters = n_iters</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y_true, n_classes)</span>:</span></span><br><span class="line"></span><br><span class="line">        x_train = np.column_stack((np.ones(len(X)),X))</span><br><span class="line">        </span><br><span class="line">        self.n_samples, n_features = x_train.shape</span><br><span class="line">        self.n_classes = n_classes</span><br><span class="line">        </span><br><span class="line">        self.weights = np.random.rand(n_features,self.n_classes)</span><br><span class="line">        all_losses = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_iters):</span><br><span class="line">            logits = np.dot(x_train, self.weights)</span><br><span class="line">            probs = self.softmax(logits)</span><br><span class="line">            y_one_hot = self.one_hot(y_true)</span><br><span class="line">            loss = self.cross_entropy(y_one_hot, probs)</span><br><span class="line">            all_losses.append(loss)</span><br><span class="line"></span><br><span class="line">            gradients = (<span class="number">1</span> / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))</span><br><span class="line"></span><br><span class="line">            self.weights = self.weights - self.learning_rate * gradients</span><br><span class="line"></span><br><span class="line"><span class="comment">#            if i % 100 == 0:</span></span><br><span class="line"><span class="comment">#                print(f'Iteration number: &#123;i&#125;, loss: &#123;np.round(loss, 4)&#125;')</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.weights, all_losses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line"></span><br><span class="line">        x_test = np.column_stack((np.ones(len(X)), X))</span><br><span class="line">        scores = np.dot(x_test, self.weights)</span><br><span class="line">        probs = self.softmax(scores)</span><br><span class="line">        <span class="keyword">return</span> np.argmax(probs, axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(self, logits)</span>:</span></span><br><span class="line">        exp = np.exp(logits)</span><br><span class="line">        sum_exp = np.sum(np.exp(logits), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> exp / sum_exp</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span><span class="params">(self, y_true, scores)</span>:</span></span><br><span class="line">        loss = - (<span class="number">1</span> / self.n_samples) * np.sum(y_true * np.log(scores))</span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(self, y)</span>:</span></span><br><span class="line">        one_hot = np.zeros((self.n_samples, self.n_classes))</span><br><span class="line">        one_hot[np.arange(self.n_samples), y.T] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = datasets.load_iris()</span><br><span class="line">    X= data.data</span><br><span class="line">    y = data.target</span><br><span class="line"></span><br><span class="line">    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.33</span>, seed=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    clf = SoftmaxRegressorII()</span><br><span class="line">    ll = clf.train(X_train, y_train,<span class="number">3</span>)</span><br><span class="line">    y_pred = clf.predict(X_test)</span><br><span class="line">    y_pred = np.reshape(y_pred, y_test.shape)</span><br><span class="line"></span><br><span class="line">    accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line">    print(<span class="string">"Accuracy:"</span>, accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Reduce dimension to two using PCA and plot the results</span></span><br><span class="line">    Plot().plot_in_2d(X_test, y_pred, title=<span class="string">"SoftMax Regression"</span>, accuracy=accuracy)</span><br></pre></td></tr></table></figure>
<p>结果如图所示：</p>
<p><img src="/2020/02/12/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%92%8Csoftmax%E5%88%86%E7%B1%BB/softmax.png" alt></p>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud" target="_blank" rel="noopener">AirCloud</a></p>
</footer>




<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
