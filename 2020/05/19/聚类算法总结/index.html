<!DOCTYPE html>


  <html class="light page-post">


<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>聚类算法总结 | Lixin</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="机器学习、数据挖掘" />
  

  <meta name="description" content="聚类算法总结聚类是一种经典的无监督学习方法，无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。 聚类直观上来说是将相似的样本聚在一起，从而形成一个类簇（cluster）。那首先的问题是如何来度量相似性（similarity measure）呢？这便">
<meta property="og:type" content="article">
<meta property="og:title" content="聚类算法总结">
<meta property="og:url" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;index.html">
<meta property="og:site_name" content="Lixin">
<meta property="og:description" content="聚类算法总结聚类是一种经典的无监督学习方法，无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。 聚类直观上来说是将相似的样本聚在一起，从而形成一个类簇（cluster）。那首先的问题是如何来度量相似性（similarity measure）呢？这便">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;gmm_算法步骤.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;dbscan_密度直达.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;dbscan_算法步骤.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;lvq_算法步骤.png">
<meta property="og:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;hc_算法步骤.png">
<meta property="og:updated_time" content="2020-05-19T09:50:00.640Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;stringsli.github.io&#x2F;2020&#x2F;05&#x2F;19&#x2F;%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93&#x2F;gmm_算法步骤.png">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    <link rel="stylesheet" href="/css/personal-style.css">
  

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-38189205-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?57e94d016e201fba3603a8a2b0263af0";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>


  
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span id="toolbox-mobile" class="toolbox-mobile">盒子</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">盒子</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/link/"
            rel="noopener noreferrer"
            target="_self"
            >
            友链
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/about/"
            rel="noopener noreferrer"
            target="_self"
            >
            关于
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/atom.xml"
            rel="noopener noreferrer"
            target="_blank"
            >
            RSS
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/search/"
            rel="noopener noreferrer"
            target="_self"
            >
            搜索
          </a>
        </li>
      
    </ul>
  </div>


</div>


  <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#聚类算法总结"><span class="toc-text">聚类算法总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Kmeans聚类"><span class="toc-text">1 Kmeans聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-1-算法流程"><span class="toc-text">1.1 算法流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#1-2-算法实现"><span class="toc-text">1.2  算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-GMM-聚类"><span class="toc-text">2 GMM 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-算法过程"><span class="toc-text">2.1 算法过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-算法流程"><span class="toc-text">2.2  算法流程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-算法实现"><span class="toc-text">2.3 算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-DBSCAN聚类"><span class="toc-text">3 DBSCAN聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-算法步骤"><span class="toc-text">3.1 算法步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-算法实现"><span class="toc-text">3.2 算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-PAM聚类算法"><span class="toc-text">4 PAM聚类算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-算法步骤"><span class="toc-text">4.1  算法步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-算法实现"><span class="toc-text">4.2 算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-LVQ聚类"><span class="toc-text">5. LVQ聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-算法步骤"><span class="toc-text">5.1 算法步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-2-算法实现"><span class="toc-text">5.2 算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-层次聚类"><span class="toc-text">6 层次聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#6-1-算法步骤"><span class="toc-text">6.1  算法步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#6-2-算法实现"><span class="toc-text">6.2 算法实现</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#参考文献"><span class="toc-text">参考文献</span></a></li></ol></li></ol>
  </div>



<div class="content content-post CENTER">
   <article id="post-聚类算法总结" class="article article-type-post" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">聚类算法总结</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2020.05.19</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>Lixin</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <h3 id="聚类算法总结"><a href="#聚类算法总结" class="headerlink" title="聚类算法总结"></a>聚类算法总结</h3><p>聚类是一种经典的无监督学习方法，无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。</p>
<p>聚类直观上来说是将相似的样本聚在一起，从而形成一个类簇（cluster）。那首先的问题是如何来度量相似性（similarity measure）呢？这便是距离度量，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是性能度量，性能度量为评价聚类结果的好坏提供了一系列有效性指标。</p>
<h4 id="1-Kmeans聚类"><a href="#1-Kmeans聚类" class="headerlink" title="1 Kmeans聚类"></a>1 Kmeans聚类</h4><p>K-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛：</p>
<p>假设输入空间 $\cal X \in \R^n  $ 为$ n $维向量的集合，$ \cal{X}=\{x^{(1)} ,x^{(2)},\cdots,x^{(m)} \}  $，$  \mathcal  C $为输入空间$ \cal X $的一个划分，不妨令$ \mathcal C=\{ \mathbb C_1,\mathbb C_2,\cdots,\mathbb C_K \}  $，因此可以定义$ k\text{-}means $算法的损失函数为</p>
<script type="math/tex; mode=display">
J(\mathcal C)=\sum\limits_{k=1}^K\sum\limits_{x^{(i)}\in \mathbb C_k}\Vert x^{(i)}-\mu^{(k)} \Vert_2^2  \tag{1}</script><p>其中$ \mu^{(k)}=\frac{1}{\vert \mathbb C_k \vert}\sum\limits_{x^{(i)}\in\mathbb C_k}x^{(i)}   $是簇$ \mathbb C_k $的聚类中心。</p>
<p>事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数$J(\mathcal C)$</p>
<h5 id="1-1-算法流程"><a href="#1-1-算法流程" class="headerlink" title="1.1 算法流程"></a>1.1 算法流程</h5><ol>
<li><p>首先随机初始化$ K $个聚类中心，$ \mu^{(1)},\mu^{(2)},\cdots,\mu^{(K)}  $；</p>
</li>
<li><p>然后根据这$ K $个聚类中心给出输入空间$ \mathcal X  $的一个划分，$ \mathbb C_1,\mathbb C_2,\cdots,\mathbb C_K  $；</p>
<ul>
<li>样本离哪个簇的聚类中心最近，则该样本就划归到那个簇<script type="math/tex; mode=display">
\mathop{\arg\min}_{k}\ \Vert x^{(i)}-\mu^{(k)} \Vert_2^2 \tag{2}</script></li>
</ul>
</li>
<li><p>再根据这个划分来更新这$ K $个聚类中心</p>
<script type="math/tex; mode=display">
\mu^{(k)}=\frac{1}{\vert \mathbb C_k \vert}\sum\limits_{x^{(i)}\in\mathbb C_k}x^{(i)} \tag{3}</script></li>
<li><p>重复2、3步骤直至收敛</p>
<ul>
<li>即$ K $个聚类中心不再变化</li>
</ul>
</li>
</ol>
<h5 id="1-2-算法实现"><a href="#1-2-算法实现" class="headerlink" title="1.2  算法实现"></a>1.2  算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>, squaredDistance&#125;</span><br><span class="line"><span class="keyword">import</span> com.strings.model.<span class="type">Model</span></span><br><span class="line"><span class="keyword">import</span> com.strings.data.<span class="type">Data</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ListBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Kmeans</span>(<span class="params">val k:<span class="type">Int</span> = 3,</span></span></span><br><span class="line"><span class="class"><span class="params">             val max_iter:<span class="type">Int</span> = 100,</span></span></span><br><span class="line"><span class="class"><span class="params">             val seed:<span class="type">Long</span> = 1234L,</span></span></span><br><span class="line"><span class="class"><span class="params">             val tolerance: <span class="type">Double</span> = 1e-4</span>) <span class="keyword">extends</span> <span class="title">Model</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> centroids = <span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]()</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> cluster = <span class="type">ListBuffer</span>[(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>])]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> iterations:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_init_random_centroids</span></span>(data : <span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]):<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> rng  = <span class="keyword">new</span> <span class="type">Random</span>(seed)</span><br><span class="line">    rng.shuffle(data).take(k)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_closest_centroid2</span></span>(centroids:<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]],row:<span class="type">DenseVector</span>[<span class="type">Double</span>]):(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>]) = &#123;</span><br><span class="line">        <span class="keyword">var</span> close_i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">var</span> closest_dist = <span class="number">-1.0</span></span><br><span class="line">        centroids.zipWithIndex.foreach(centroid =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> distance = squaredDistance(centroid._1,row)</span><br><span class="line">          <span class="keyword">if</span>(closest_dist&gt;distance || closest_dist == <span class="number">-1.0</span>)&#123;</span><br><span class="line">            closest_dist = distance</span><br><span class="line">            close_i = centroid._2</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">    (close_i,row)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_closest_centroid</span></span>(centroids:<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]],row:<span class="type">DenseVector</span>[<span class="type">Double</span>]):(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>]) = &#123;</span><br><span class="line">      <span class="keyword">val</span> distWithIndex =  centroids.zipWithIndex.map(x =&gt;</span><br><span class="line">                          (squaredDistance(x._1,row),x._2)</span><br><span class="line">                          ).minBy(_._1)</span><br><span class="line">      (distWithIndex._2,row)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span></span>(data:<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]):<span class="type">Unit</span> = &#123;</span><br><span class="line">    centroids = _init_random_centroids(data)</span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span>(_ &lt;- <span class="type">Range</span>(<span class="number">0</span>,max_iter) <span class="keyword">if</span> flag)&#123;</span><br><span class="line">      iterations += <span class="number">1</span></span><br><span class="line">      data.foreach&#123;d =&gt;</span><br><span class="line">        <span class="keyword">val</span> b = _closest_centroid(centroids, d)</span><br><span class="line">        cluster.append(b)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> prev_centroid = centroids</span><br><span class="line">      centroids = _calculate_centroids(cluster)</span><br><span class="line">      cluster = <span class="type">ListBuffer</span>[(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>])]()</span><br><span class="line">      <span class="keyword">val</span> diff = prev_centroid.zip(centroids).map(x =&gt; squaredDistance(x._2,x._1))</span><br><span class="line">      <span class="keyword">if</span>( diff.sum &lt; tolerance)&#123;</span><br><span class="line">        flag = <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predit</span></span>(data:<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]):<span class="type">List</span>[(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>])]= &#123;</span><br><span class="line">    data.map(x =&gt; _closest_centroid(centroids,x))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_calculate_centroids</span></span>(cluster:<span class="type">ListBuffer</span>[(<span class="type">Int</span>,<span class="type">DenseVector</span>[<span class="type">Double</span>])]):<span class="type">List</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]= &#123;</span><br><span class="line">    cluster.groupBy(_._1).map &#123; x =&gt;</span><br><span class="line">      <span class="keyword">val</span> temp = x._2.map(_._2)</span><br><span class="line">      temp.reduce((a, b) =&gt; a :+ b).map(_ / temp.length)</span><br><span class="line">    &#125;.toList</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @param x input matrix</span></span><br><span class="line"><span class="comment">   * @return predict vector value</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(x: <span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">DenseVector</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="type">DenseVector</span>[<span class="type">Double</span>]()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Kmeans</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> irisData = <span class="type">Data</span>.irisData</span><br><span class="line">    <span class="keyword">val</span> data = irisData.map(_.slice(<span class="number">0</span>,<span class="number">4</span>)).map(<span class="type">DenseVector</span>(_)).toList</span><br><span class="line">    <span class="keyword">val</span> kmeans = <span class="keyword">new</span> <span class="type">Kmeans</span>(max_iter = <span class="number">100</span>)</span><br><span class="line">    kmeans.train(data)</span><br><span class="line">    println(kmeans.centroids)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细请参考：<a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala</a> </p>
<h4 id="2-GMM-聚类"><a href="#2-GMM-聚类" class="headerlink" title="2 GMM 聚类"></a>2 GMM 聚类</h4><p>GMM聚类又称高斯混合聚类，即采用高斯分布来描述原型。现假设每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成。</p>
<h5 id="2-1-算法过程"><a href="#2-1-算法过程" class="headerlink" title="2.1 算法过程"></a>2.1 算法过程</h5><p>高斯分布的定义，对于$n$维样本空间$\mathcal X$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为</p>
<script type="math/tex; mode=display">
p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}  \tag{4}</script><p>其中$\mu$是$n$维均值向量，$\Sigma$是$n\times n$的协方差矩阵,由(4)可以看出，高斯分布完全由均值向量和协方差矩阵$\Sigma$这两个参数确定。为了明确高斯分布与相应的参数的依赖关系，将概率密度函数记为$p(x|\mu,\Sigma)$.</p>
<p>我们可以定义高斯混合分布</p>
<script type="math/tex; mode=display">
p_{\mathcal M}(x) = \sum_{i=1}{k}\alpha_i.p(x|\mu_i,\Sigma_i) \tag{5}</script><p>$α_i$称为混合系数,满足$\sum_{i=1}^{k}\alpha_i=1$，假设样本的生成过程由高斯混合分布给出：首先，根据$\alpha_1,\alpha_2,…,\alpha_k$定义的先验分布选择高斯混合成分，其中$α_i$为选择第$i$混合成分的概率，然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p>
<p>若训练集$D={x_1,x_2,…,x_m}$由上述过程生成，令随机变量$z_j \in{1,2,…,k}$表示生成样本的高斯混合成分，根据贝叶斯定理，$z_j$的后验分布对应于</p>
<script type="math/tex; mode=display">
p_{\mathcal M}(z_j = i | x_j) = \frac{P(z_j=i).p_{\mathcal M}(x_j|z_j=i)}{p_{\mathcal M}(x_j)} \\
=\frac{\alpha_i.p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^{k}\alpha_l.p(x_j|\mu_l,\Sigma_l)}   \tag{6}</script><p>当高斯混合分布(5)已知时，高斯混合聚类将样本集$D$划分为$k$个簇$\mathcal C = {C_1,C_2,…,C_k}$,将每个样本$x_j$的簇标记为$\lambda_j$如下确定：</p>
<script type="math/tex; mode=display">
\lambda_j = \arg \max _{i\in{1,2...,k}}\gamma_{ji} \tag{7}</script><p>对于(5)式，模型参数$\{(\alpha_i,\mu_i,\Sigma_i)|1\leq i \leq k\}$如何求解？显然给定样本集$D$，可以采用极大似然估计，即最大化对数似然：</p>
<script type="math/tex; mode=display">
LL(D) = \ln(\prod_{j}^mp_{\mathcal M}(x_j)) \\
=\sum_{j=1}^{m}ln(\sum_{i=1}^{k}\alpha_i.p(x_j|\mu_i,\Sigma_i)) \tag{8}</script><p>采用EM算法进行迭代优化求解，下面做个简单地推导：</p>
<p>若参数$\{(\alpha_i,\mu_i,\Sigma_i)|1\leq i \leq k\}$能使(8)式最大化，则由$\frac{\partial LL(D)}{\partial \mu_i} = 0$有：</p>
<script type="math/tex; mode=display">
\sum_{j=1}^{m}\frac{\alpha_i.p(x_j|\mu_i,\Sigma_i)}{\sum_{l=1}^{k}\alpha_l.p(x_j|\mu_l,\Sigma_l)}(x_j - \mu_i) \tag{9}</script><p>由(6)式以及$\gamma_{ji} = p_{\mathcal M}(z_j = i |x_j)$有</p>
<script type="math/tex; mode=display">
\mu_i = \frac{\sum_{j=1}^{m}\gamma_{ji}x_j}{\sum_{j=1}^m\gamma_{ji}} \tag{10}</script><p>即混合成分的均值可以通过样本的加权平均来估计，样本权重是每个样本属于该成分的后验概率，类似的，由$\frac{\partial LL(D)}{\partial \Sigma_i} = 0$可以得到：</p>
<script type="math/tex; mode=display">
\Sigma_i = \frac{\sum_{j=1}^{m}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum_{j=1}^m\gamma_{ji}} \tag{11}</script><p>对于混合系数，使用拉格朗日法可以得到：</p>
<script type="math/tex; mode=display">
\alpha_i = \frac{1}{m}\sum_{j=1}^{m}\gamma_{ji} \tag{12}</script><p>即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定。</p>
<p>由上述推导可得高斯混合模型的EM算法：在每步迭代中，先根据当前参数来计算每个样本属于高斯成分的后验概率$\gamma_{ji}$(<strong>E步</strong>), 再根据式(10)-(12)更新参数模型$\{(\alpha_i,\mu_i,\Sigma_i)|1\leq i \leq k\}$（<strong>M步</strong>）.</p>
<h5 id="2-2-算法流程"><a href="#2-2-算法流程" class="headerlink" title="2.2  算法流程"></a>2.2  算法流程</h5><p>GMM算法步骤如下：</p>
<p><img src="/2020/05/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/gmm_算法步骤.png" alt></p>
<h5 id="2-3-算法实现"><a href="#2-3-算法实现" class="headerlink" title="2.3 算法实现"></a>2.3 算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;*, <span class="type">Axis</span>, <span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>, argmax, det, max, norm, pinv, sum&#125;</span><br><span class="line"><span class="keyword">import</span> com.strings.data.<span class="type">Data</span></span><br><span class="line"><span class="keyword">import</span> com.strings.model.metric.<span class="type">Metric</span></span><br><span class="line"><span class="keyword">import</span> com.strings.utils.<span class="type">MatrixUtils</span></span><br><span class="line"><span class="keyword">import</span> org.slf4j.<span class="type">LoggerFactory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.breakable</span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.<span class="keyword">break</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GMM</span>(<span class="params">k:<span class="type">Int</span> = 3,</span></span></span><br><span class="line"><span class="class"><span class="params">          max_iterations:<span class="type">Int</span> = 2000,</span></span></span><br><span class="line"><span class="class"><span class="params">          tolerance:<span class="type">Double</span> = 1e-8</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> logger = <span class="type">LoggerFactory</span>.getLogger(classOf[<span class="type">GMM</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> means:<span class="type">Array</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]](k)</span><br><span class="line">  <span class="keyword">var</span> vars:<span class="type">Array</span>[<span class="type">DenseMatrix</span>[<span class="type">Double</span>]] = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">DenseMatrix</span>[<span class="type">Double</span>]](k)</span><br><span class="line">  <span class="keyword">var</span> sample_assignments:<span class="type">DenseVector</span>[<span class="type">Int</span>] = _</span><br><span class="line">  <span class="keyword">var</span> priors:<span class="type">DenseVector</span>[<span class="type">Double</span>] = _</span><br><span class="line">  <span class="keyword">var</span> responsibility:<span class="type">DenseMatrix</span>[<span class="type">Double</span>] = _</span><br><span class="line">  <span class="keyword">var</span> responsibilities:<span class="type">ArrayBuffer</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_initialize</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    <span class="keyword">val</span> <span class="type">X_lst</span> = (<span class="number">0</span> until n_samples).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">val</span> rng =  <span class="keyword">new</span> scala.util.<span class="type">Random</span>()</span><br><span class="line">    priors = <span class="type">DenseVector</span>.ones[<span class="type">Double</span>](k) :/ k.toDouble</span><br><span class="line">    means = rng.shuffle(<span class="type">X_lst</span>).take(k).toArray</span><br><span class="line"><span class="comment">//    means = X_lst.take(k).toArray</span></span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until k)&#123;</span><br><span class="line">      vars(i) = <span class="type">MatrixUtils</span>.calculate_covariance_matrix(<span class="type">X</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">multivariate_gaussian</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>],i:<span class="type">Int</span>):<span class="type">DenseVector</span>[<span class="type">Double</span>]=&#123;</span><br><span class="line">    <span class="keyword">val</span> n_features = <span class="type">X</span>.cols</span><br><span class="line">    <span class="keyword">val</span> mean = means(i)</span><br><span class="line">    <span class="keyword">val</span> covar = vars(i)</span><br><span class="line">    <span class="keyword">val</span> determinant = det(covar)</span><br><span class="line">    <span class="keyword">val</span> likelihoods = <span class="type">DenseVector</span>.zeros[<span class="type">Double</span>](<span class="type">X</span>.rows)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">X_arr</span> = (<span class="number">0</span> until <span class="type">X</span>.rows).map(<span class="type">X</span>.t(::,_))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>((sample,index) &lt;- <span class="type">X_arr</span>.zipWithIndex)&#123;</span><br><span class="line">      <span class="keyword">val</span> d = n_features</span><br><span class="line">      <span class="keyword">val</span> coeff = <span class="number">1.0</span> / (math.pow(<span class="number">2</span> * <span class="type">Math</span>.<span class="type">PI</span>,d/<span class="number">2</span>) * math.sqrt(determinant))</span><br><span class="line">      <span class="keyword">val</span> gram = (sample :- mean).t * pinv(covar) * (sample :- mean)</span><br><span class="line">      <span class="keyword">val</span> exponent = math.exp(<span class="number">-0.5</span> * gram)</span><br><span class="line">      likelihoods(index) = coeff * exponent</span><br><span class="line">    &#125;</span><br><span class="line">    likelihoods</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_likelihoods</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]):<span class="type">DenseMatrix</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    <span class="keyword">val</span> likelihoods = <span class="type">DenseMatrix</span>.zeros[<span class="type">Double</span>](n_samples,k)</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until k)&#123;</span><br><span class="line">      likelihoods(::,i) := multivariate_gaussian(<span class="type">X</span>,i)</span><br><span class="line">    &#125;</span><br><span class="line">    likelihoods</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_expectation</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> weighted_likelihoods = _get_likelihoods(<span class="type">X</span>)(*,::).map(x =&gt; x :* priors)</span><br><span class="line">    <span class="keyword">val</span> sum_likelihoods = sum(weighted_likelihoods,<span class="type">Axis</span>._1)</span><br><span class="line">    responsibility = weighted_likelihoods(::,*).map(x =&gt; x :/ sum_likelihoods) <span class="comment">// 列除</span></span><br><span class="line">    sample_assignments = argmax(responsibility,<span class="type">Axis</span>._1)</span><br><span class="line">    responsibilities.append(max(responsibility,<span class="type">Axis</span>._1))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_maximization</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until k)&#123;</span><br><span class="line">      <span class="keyword">val</span> resp = responsibility(::,i)</span><br><span class="line">      <span class="keyword">val</span> mean = sum(<span class="type">X</span>(::,*).map(f =&gt; resp :* f),<span class="type">Axis</span>._0) :/ sum(resp)</span><br><span class="line">      means(i) = mean.t</span><br><span class="line">      <span class="keyword">val</span> diff = <span class="type">X</span>(*,::).map(f =&gt; f :- mean.t)</span><br><span class="line">      <span class="keyword">val</span> covariance = diff.t * diff(::,*).map(f =&gt; f :* resp) :/sum(resp) <span class="comment">// 注意diff(::,*)是取列运算</span></span><br><span class="line">      vars(i) = covariance</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    priors = sum(responsibility,<span class="type">Axis</span>._0).t :/ n_samples.toDouble</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">DenseVector</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    _initialize(<span class="type">X</span>)</span><br><span class="line">    <span class="keyword">var</span> iter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> flag = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until max_iterations <span class="keyword">if</span> flag) &#123;</span><br><span class="line">      iter += <span class="number">1</span></span><br><span class="line">      _expectation(<span class="type">X</span>)</span><br><span class="line">      _maximization(<span class="type">X</span>)</span><br><span class="line">      breakable &#123;</span><br><span class="line">        <span class="keyword">if</span> (responsibilities.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">          <span class="keyword">break</span>()</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          <span class="keyword">val</span> n = responsibilities.length</span><br><span class="line">          <span class="keyword">val</span> diff = norm(responsibilities(n<span class="number">-1</span>) - responsibilities(n<span class="number">-2</span>), <span class="number">2</span>)</span><br><span class="line">          println(diff)</span><br><span class="line">          <span class="keyword">if</span> (diff &lt;= tolerance) flag = <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">    logger.info(<span class="string">s"<span class="subst">$iter</span> 之后收敛"</span>)</span><br><span class="line">    _expectation(<span class="type">X</span>)</span><br><span class="line">    sample_assignments.map(_.toDouble)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GMM</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> irisData = <span class="type">Data</span>.irisData</span><br><span class="line">    <span class="keyword">val</span> data = irisData.map(_.slice(<span class="number">0</span>,<span class="number">4</span>)).toList</span><br><span class="line">    <span class="keyword">val</span> target = irisData.map(_.apply(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> gmm = <span class="keyword">new</span> <span class="type">GMM</span>(max_iterations = <span class="number">100</span>)</span><br><span class="line">    gmm._initialize(<span class="type">DenseMatrix</span>(data:_*))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pred = gmm.predict(<span class="type">DenseMatrix</span>(data:_*))</span><br><span class="line">    println(pred)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细请参考:<a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/GMM.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/GMM.scala</a></p>
<h4 id="3-DBSCAN聚类"><a href="#3-DBSCAN聚类" class="headerlink" title="3 DBSCAN聚类"></a>3 DBSCAN聚类</h4><p>密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。</p>
<p>DBSCAN是一种著名的密度聚类算法，它是一组“邻域”（neighbhood）参数($\epsilon $,MinPts)来刻画样本分布的紧密程度.给定数据集$D = \{x_1,x_2,…,x_m\}$,定义下面几个概念：</p>
<ol>
<li>$\epsilon$-邻域：对于$x_j \in D$,其$\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\epsilon$的样本，即$N_{\epsilon}(x_j) = {x_i \in D | dist(x_i,x_j) \leq \epsilon}$;</li>
<li>核心对象(core object):若$x_j$的$\epsilon$-邻域至少包含MinPts个样本，即$|N_{\epsilon}(x_j)| \geq MinPts$,则$x_j$是一个核心对象；</li>
<li>密度直达(directly density-reachable):若$x_j$位于$x_i$的$\epsilon$-邻域中，且$x_i$是核心对象，则称$x_j$是由$x_i$密度直达；</li>
<li>密度可达(density-reachable)：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,…,p_n$,其中$p_1 = x_i,p_n = x_j$,且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；</li>
<li>密度相连(density-connected):对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连；</li>
</ol>
<p>下图给出了上述概念的直观显示：</p>
<p><img src="/2020/05/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/dbscan_密度直达.png" alt></p>
<h5 id="3-1-算法步骤"><a href="#3-1-算法步骤" class="headerlink" title="3.1 算法步骤"></a>3.1 算法步骤</h5><p>DBSCAN的算法的思想是找出一个核心对象所有密度可达的样本集合形成簇。首先从数据集中任选一个核心对象$A$，找出所有$A$密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<p><img src="/2020/05/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/dbscan_算法步骤.png" alt></p>
<h5 id="3-2-算法实现"><a href="#3-2-算法实现" class="headerlink" title="3.2 算法实现"></a>3.2 算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>, sum&#125;</span><br><span class="line"><span class="keyword">import</span> com.strings.data.<span class="type">Data</span></span><br><span class="line"><span class="keyword">import</span> com.strings.model.metric.<span class="type">Metric</span></span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.breakable</span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.<span class="keyword">break</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DBSCAN</span>(<span class="params">eps:<span class="type">Double</span> = 1.0,</span></span></span><br><span class="line"><span class="class"><span class="params">             min_samples:<span class="type">Int</span> = 5</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> visited_samples:<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">  <span class="keyword">var</span> neighbors: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Map</span>()</span><br><span class="line">  <span class="keyword">var</span> <span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>] = _</span><br><span class="line">  <span class="keyword">var</span> clusters:<span class="type">ArrayBuffer</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Array</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">euclidean_distance</span></span>(x1:<span class="type">DenseVector</span>[<span class="type">Double</span>],x2:<span class="type">DenseVector</span>[<span class="type">Double</span>]): <span class="type">Double</span> =&#123;</span><br><span class="line">    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_neighbors</span></span>(sample_i:<span class="type">Int</span>): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">val</span> neighbors:<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">val</span> <span class="type">X_arr</span> = (<span class="number">0</span> until <span class="type">X</span>.rows).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">val</span> <span class="type">X_arr2</span> =  <span class="type">X_arr</span>.indices.filter(i =&gt; i != sample_i).map(<span class="type">X_arr</span>(_))</span><br><span class="line">    <span class="keyword">for</span>((_sample,inx) &lt;- <span class="type">X_arr2</span>.zipWithIndex)&#123;</span><br><span class="line">      <span class="keyword">val</span> dist = euclidean_distance(_sample,<span class="type">X_arr</span>(sample_i))</span><br><span class="line">      <span class="keyword">if</span>(dist &lt; eps)&#123;</span><br><span class="line">        neighbors.append(inx)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    neighbors.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_expand_cluster</span></span>(sample_i:<span class="type">Int</span>, neighbor:<span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">val</span> cluster:<span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">    cluster.append(sample_i)</span><br><span class="line">    <span class="keyword">for</span>(neighbor_i &lt;- neighbor)&#123;</span><br><span class="line">      <span class="keyword">if</span>(!visited_samples.contains(neighbor_i))&#123;</span><br><span class="line">        visited_samples.append(neighbor_i)</span><br><span class="line">        neighbors += (neighbor_i -&gt; _get_neighbors(neighbor_i) )</span><br><span class="line">        <span class="keyword">if</span> (neighbors(neighbor_i).length &gt;= min_samples)&#123;   <span class="comment">//        neighbors.get(neighbor_i).size 结果是1</span></span><br><span class="line">          <span class="keyword">val</span> expanded_cluster = _expand_cluster(neighbor_i,neighbors(neighbor_i))</span><br><span class="line">          cluster.append(expanded_cluster:_*)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          cluster.append(neighbor_i)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cluster.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_cluster_labels</span></span>(): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">val</span> labels = <span class="type">Array</span>.fill(<span class="type">X</span>.rows)(clusters.length)</span><br><span class="line">    <span class="keyword">for</span>((cluster,cluster_i) &lt;- clusters.zipWithIndex)&#123;</span><br><span class="line">      <span class="keyword">for</span>(sample_i &lt;- cluster)&#123;</span><br><span class="line">        labels(sample_i) = cluster_i</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    labels</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(<span class="type">XX</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="type">X</span> = <span class="type">XX</span></span><br><span class="line">    visited_samples = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">    neighbors = <span class="type">Map</span>()</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    <span class="keyword">for</span>(sample_i &lt;- <span class="number">0</span> until n_samples)&#123;</span><br><span class="line">      breakable &#123;</span><br><span class="line">        <span class="keyword">if</span> (visited_samples.contains(sample_i)) &#123;</span><br><span class="line">          <span class="keyword">break</span>()</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">          neighbors += (sample_i -&gt; _get_neighbors(sample_i))</span><br><span class="line">          <span class="keyword">if</span>(neighbors.get(sample_i).size &gt;= min_samples)&#123;</span><br><span class="line">            visited_samples.append(sample_i)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> new_cluster = _expand_cluster(sample_i,neighbors(sample_i))</span><br><span class="line">          clusters.append(new_cluster)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    _get_cluster_labels()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DBSCAN</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> irisData = <span class="type">Data</span>.irisData</span><br><span class="line">    <span class="keyword">val</span> data = irisData.map(_.slice(<span class="number">0</span>,<span class="number">4</span>)).toList</span><br><span class="line">    <span class="keyword">val</span> target = irisData.map(_.apply(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dbscan = <span class="keyword">new</span> <span class="type">DBSCAN</span>(eps = <span class="number">.7</span>,min_samples = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pred = dbscan.predict(<span class="type">DenseMatrix</span>(data:_*))</span><br><span class="line">    println(pred.toList)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细请参考：<a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/DBSCAN.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/DBSCAN.scala</a></p>
<h4 id="4-PAM聚类算法"><a href="#4-PAM聚类算法" class="headerlink" title="4 PAM聚类算法"></a>4 PAM聚类算法</h4><p>K-means是每次选<strong>簇的</strong>均值<strong>作为新的中心，迭代直到簇中对象分布不再变化。其缺点是对于离群点是敏感的，因为一个具有很大极端值的对象会扭曲数据分布。那么我们可以考虑新的簇中心不选择均值而是选择</strong>簇内的某个对象**，只要使总的代价降低就可以。</p>
<p>PAM（partitioning around medoid，围绕中心点的划分）是具有代表性的k-medoids算法。</p>
<p>它最初随机选择k个对象作为中心点，该算法反复的用非代表对象（非中心点）代替代表对象，试图找出更好的中心点，以改进聚类的质量。 K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此， PAM可以容纳混合数据类型，并且不仅限于连续变量。</p>
<h5 id="4-1-算法步骤"><a href="#4-1-算法步骤" class="headerlink" title="4.1  算法步骤"></a>4.1  算法步骤</h5><p>​       PAM算法如下：<br>​       (1) 随机选择K个观测值（每个都称为中心点）；<br>​       (2) 计算观测值到各个中心的距离/相异性；<br>​       (3) 把每个观测值分配到最近的中心点；<br>​       (4) 计算每个中心点到每个观测值的距离的总和（总成本）；<br>​       (5) 选择一个该类中不是中心的点，并和中心点互换；<br>​       (6) 重新把每个点分配到距它最近的中心点；<br>​       (7) 再次计算总成本；<br>​       (8) 如果总成本比步骤(4)计算的总成本少，把新的点作为中心点；<br>​       (9) 重复步骤(5)～(8)直到中心点不再改变。</p>
<h5 id="4-2-算法实现"><a href="#4-2-算法实现" class="headerlink" title="4.2 算法实现"></a>4.2 算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>, squaredDistance&#125;</span><br><span class="line"><span class="keyword">import</span> com.strings.data.<span class="type">Data</span></span><br><span class="line"><span class="keyword">import</span> com.strings.model.metric.<span class="type">Metric</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.breakable</span><br><span class="line"><span class="keyword">import</span> util.control.<span class="type">Breaks</span>.<span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param k nums of cluster</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PAM</span>(<span class="params">k:<span class="type">Int</span> = 2,seed:<span class="type">Long</span> = 1234L</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_init_random_medoids</span></span>(<span class="type">X</span>: <span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">IndexedSeq</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] =&#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    <span class="keyword">val</span> data = (<span class="number">0</span> until n_samples).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">val</span> rng  = <span class="keyword">new</span> <span class="type">Random</span>(seed)</span><br><span class="line">    rng.shuffle(data).take(k)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_closet_medoid</span></span>(sample:<span class="type">DenseVector</span>[<span class="type">Double</span>],medoids:<span class="type">IndexedSeq</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]): <span class="type">Int</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> distWithIndex =  medoids.zipWithIndex.map(x =&gt;</span><br><span class="line">      (squaredDistance(x._1,sample),x._2)</span><br><span class="line">    ).minBy(_._1)</span><br><span class="line">    distWithIndex._2</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_create_clusters</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>],medoids:<span class="type">IndexedSeq</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]): <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] =&#123;</span><br><span class="line">    <span class="keyword">val</span> clusterss = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](<span class="type">X</span>.rows)</span><br><span class="line">    <span class="keyword">val</span> data = (<span class="number">0</span> until <span class="type">X</span>.rows).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">for</span>((sample,inx) &lt;- data.zipWithIndex)&#123;</span><br><span class="line">      <span class="keyword">val</span> medoid_i = _closet_medoid(sample,medoids)</span><br><span class="line">      clusterss(inx) = medoid_i</span><br><span class="line">    &#125;</span><br><span class="line">    clusterss.zipWithIndex.groupBy(_._1).toArray.sortBy(_._1).map(_._2.map(_._2))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_calculate_cost</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>],clusters:<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]],medoids:<span class="type">IndexedSeq</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]):<span class="type">Double</span>=&#123;</span><br><span class="line">    <span class="keyword">var</span> cost = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">val</span> data = (<span class="number">0</span> until <span class="type">X</span>.rows).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">for</span>((cluster,i) &lt;- clusters.zipWithIndex)&#123;</span><br><span class="line">      <span class="keyword">val</span> medoid = medoids(i)</span><br><span class="line">      <span class="keyword">for</span>(sample_i &lt;- cluster)&#123;</span><br><span class="line">        cost += squaredDistance(data(sample_i),medoid)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cost</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_cluster_labels</span></span>(clusters:<span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]],<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">val</span> y_pred = <span class="type">Array</span>.fill(<span class="type">X</span>.rows)(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span>(cluster_i &lt;- <span class="number">0</span> until clusters.length)&#123;</span><br><span class="line">      <span class="keyword">val</span> cluster = clusters(cluster_i)</span><br><span class="line">      <span class="keyword">for</span>(sample_i &lt;- cluster)&#123;</span><br><span class="line">        y_pred(sample_i) = cluster_i</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    y_pred</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_no_medoids</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>],medoids:<span class="type">IndexedSeq</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]): <span class="type">Array</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] =&#123;</span><br><span class="line">    <span class="keyword">val</span> non_medoids:<span class="type">ArrayBuffer</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]]()</span><br><span class="line">    <span class="keyword">val</span> data = (<span class="number">0</span> until <span class="type">X</span>.rows).map(<span class="type">X</span>.t(::,_))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(sample &lt;- data)&#123;</span><br><span class="line">      <span class="keyword">if</span>(!medoids.contains(sample)) non_medoids.append(sample)</span><br><span class="line">    &#125;</span><br><span class="line">    non_medoids.toArray</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Array</span>[<span class="type">Int</span>] =&#123;</span><br><span class="line">    <span class="keyword">var</span> medoids = _init_random_medoids(<span class="type">X</span>)</span><br><span class="line">    <span class="keyword">val</span> clusters = _create_clusters(<span class="type">X</span>,medoids)</span><br><span class="line">    <span class="keyword">var</span> cost = _calculate_cost(<span class="type">X</span>, clusters, medoids)</span><br><span class="line">    breakable &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">var</span> best_medoids = medoids</span><br><span class="line">        <span class="keyword">var</span> lowest_cost = cost</span><br><span class="line">        <span class="keyword">for</span> (medoid &lt;- medoids) &#123;</span><br><span class="line">          <span class="keyword">val</span> non_medoids = _get_no_medoids(<span class="type">X</span>, medoids)</span><br><span class="line">          <span class="keyword">for</span> (sample &lt;- non_medoids) &#123;</span><br><span class="line">            <span class="keyword">val</span> new_medoids = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">DenseVector</span>[<span class="type">Double</span>]](medoids.length)</span><br><span class="line">            <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until medoids.length) &#123;</span><br><span class="line">              new_medoids(i) = medoids(i)</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">val</span> inx: <span class="type">IndexedSeq</span>[<span class="type">Int</span>] = medoids.indices.filter(i =&gt; medoids(i) == medoid)</span><br><span class="line">            inx.foreach(i =&gt; new_medoids(i) = sample)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> new_clusters = _create_clusters(<span class="type">X</span>, new_medoids)</span><br><span class="line">            <span class="keyword">val</span> new_cost = _calculate_cost(<span class="type">X</span>, new_clusters, new_medoids)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (new_cost &lt; lowest_cost) &#123;</span><br><span class="line">              lowest_cost = new_cost</span><br><span class="line">              best_medoids = new_medoids</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (lowest_cost &lt; cost) &#123;</span><br><span class="line">          cost = lowest_cost</span><br><span class="line">          medoids = best_medoids</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">break</span>()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> finaly_clusters = _create_clusters(<span class="type">X</span>,medoids)</span><br><span class="line">    _get_cluster_labels(finaly_clusters,<span class="type">X</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PAM</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> irisData = <span class="type">Data</span>.irisData</span><br><span class="line">    <span class="keyword">val</span> data = irisData.map(_.slice(<span class="number">0</span>,<span class="number">4</span>)).toList</span><br><span class="line">    <span class="keyword">val</span> target = irisData.map(_.apply(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pam = <span class="keyword">new</span> <span class="type">PAM</span>(k = <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pred = pam.predict(<span class="type">DenseMatrix</span>(data:_*))</span><br><span class="line">    println(pred.toList)</span><br><span class="line">    <span class="keyword">val</span> acc =  <span class="type">Metric</span>.accuracy(pred.map(_.toDouble),target) * <span class="number">100</span></span><br><span class="line">    println(<span class="string">f"准确率为: <span class="subst">$acc</span>%-5.2f%%"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细请参考：<a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/PAM.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/PAM.scala</a></p>
<h4 id="5-LVQ聚类"><a href="#5-LVQ聚类" class="headerlink" title="5. LVQ聚类"></a>5. LVQ聚类</h4><p>LVQ又称“学习向量量化”(Learning Vector Quantization)也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ假设样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。</p>
<p>给定样本集$\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\}$,每个样本$x_j$是由$n$个属性描述的特征向量$(x_{j1};x_{j2};…;x_{jn})$,$y_j \in \mathcal Y$是样本$x_j$的类别标记. LVQ的目标是学得一组$n$维原型向量$\{p_1,p_2,…,p_q\}$,每个原型向量代表一个聚类簇，簇标记为$t_i \in \mathcal Y$.</p>
<h5 id="5-1-算法步骤"><a href="#5-1-算法步骤" class="headerlink" title="5.1 算法步骤"></a>5.1 算法步骤</h5><p>LVQ的算法步骤如下：</p>
<p><img src="/2020/05/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/lvq_算法步骤.png" alt></p>
<h5 id="5-2-算法实现"><a href="#5-2-算法实现" class="headerlink" title="5.2 算法实现"></a>5.2 算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>, argmin, sum&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LVQ</span>(<span class="params">t:<span class="type">Array</span>[<span class="type">Int</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">          lr:<span class="type">Double</span> = 0.1,</span></span></span><br><span class="line"><span class="class"><span class="params">          nums_iters:<span class="type">Int</span> = 400</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> c = t.distinct.length</span><br><span class="line">  <span class="keyword">val</span> q = t.length</span><br><span class="line">  <span class="keyword">var</span> <span class="type">C</span>: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]] = <span class="type">Map</span>()</span><br><span class="line">  <span class="keyword">var</span> p: <span class="type">DenseMatrix</span>[<span class="type">Double</span>] = _</span><br><span class="line">  <span class="keyword">var</span> labels: <span class="type">DenseVector</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">euclidean_distance</span></span>(x1: <span class="type">DenseVector</span>[<span class="type">Double</span>], x2: <span class="type">DenseVector</span>[<span class="type">Double</span>]): <span class="type">Double</span> = &#123;</span><br><span class="line">    require(x1.length == x2.length)</span><br><span class="line">    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(<span class="type">X</span>: <span class="type">DenseMatrix</span>[<span class="type">Double</span>], y: <span class="type">DenseVector</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">    p = <span class="type">DenseMatrix</span>.zeros[<span class="type">Double</span>](q, <span class="type">X</span>.cols)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until q) &#123;</span><br><span class="line">      <span class="type">C</span> += (i -&gt; <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]())</span><br><span class="line">      <span class="keyword">val</span> candidate_indices = y.toArray.indices.filter(f =&gt; y(f) == t(i))</span><br><span class="line">      <span class="keyword">val</span> target_indice = <span class="type">Random</span>.shuffle(candidate_indices.toList).take(<span class="number">1</span>).apply(<span class="number">0</span>)</span><br><span class="line">      p(i, ::) := <span class="type">X</span>(target_indice, ::)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> p_arr = (<span class="number">0</span> until p.rows).map(p.t(::, _))</span><br><span class="line">    <span class="keyword">for</span> (_ &lt;- <span class="number">0</span> until nums_iters) &#123;</span><br><span class="line">      <span class="keyword">val</span> j = <span class="type">Random</span>.shuffle(<span class="type">Range</span>(<span class="number">0</span>, y.length).toList).take(<span class="number">1</span>).apply(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> x_j = <span class="type">X</span>(j, ::).t</span><br><span class="line">      <span class="keyword">val</span> d = p_arr.map(f =&gt; euclidean_distance(f, x_j))</span><br><span class="line">      <span class="keyword">val</span> idx: <span class="type">Int</span> = argmin(d.toArray)</span><br><span class="line">      <span class="keyword">if</span> (y(j) == t(idx)) &#123;</span><br><span class="line">        p(idx, ::) := p(idx, ::) :+ ((<span class="type">X</span>(j, ::) :- p(idx, ::)) :* lr)  <span class="comment">// :+ 和 :* 运算优先级一致</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        p(idx, ::) := p(idx, ::) :- ((<span class="type">X</span>(j, ::) :- p(idx, ::)) :* lr)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    p_arr = (<span class="number">0</span> until p.rows).map(p.t(::, _))</span><br><span class="line">    <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until <span class="type">X</span>.rows) &#123;</span><br><span class="line">      <span class="keyword">val</span> d = p_arr.map(f =&gt; euclidean_distance(f, <span class="type">X</span>(j, ::).t))</span><br><span class="line">      <span class="keyword">val</span> idx: <span class="type">Int</span> = argmin(<span class="type">DenseVector</span>(d.toArray))</span><br><span class="line">      <span class="type">C</span>(idx).append(j)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    labels = <span class="type">DenseVector</span>.zeros[<span class="type">Int</span>](<span class="type">X</span>.rows)</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until q) &#123;</span><br><span class="line">      <span class="keyword">for</span> (j &lt;- <span class="type">C</span>(i)) &#123;</span><br><span class="line">        labels(j) = i</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(<span class="type">X</span>: <span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">DenseVector</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> p_arr = (<span class="number">0</span> until p.rows).map(p.t(::, _))</span><br><span class="line">    <span class="keyword">val</span> preds_y: <span class="type">ArrayBuffer</span>[<span class="type">Int</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">Int</span>]()</span><br><span class="line">    <span class="keyword">for</span> (j &lt;- <span class="number">0</span> until <span class="type">X</span>.rows) &#123;</span><br><span class="line">      <span class="keyword">val</span> d = p_arr.map(f =&gt; euclidean_distance(f, <span class="type">X</span>(j, ::).t))</span><br><span class="line">      <span class="keyword">val</span> idx: <span class="type">Int</span> = argmin(<span class="type">DenseVector</span>(d.toArray))</span><br><span class="line">      preds_y.append(t(idx))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">DenseVector</span>(preds_y.toArray)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LVQ</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">X</span> = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">0.697</span>,<span class="number">0.460</span>),<span class="type">Array</span>(<span class="number">0.774</span>,<span class="number">0.376</span>),<span class="type">Array</span>(<span class="number">0.634</span>,<span class="number">0.264</span>),<span class="type">Array</span>(<span class="number">0.608</span>,<span class="number">0.318</span>),<span class="type">Array</span>(<span class="number">0.556</span>,<span class="number">0.215</span>),</span><br><span class="line">                  <span class="type">Array</span>(<span class="number">0.403</span>,<span class="number">0.237</span>),<span class="type">Array</span>(<span class="number">0.481</span>,<span class="number">0.149</span>),<span class="type">Array</span>(<span class="number">0.437</span>,<span class="number">0.211</span>),<span class="type">Array</span>(<span class="number">0.666</span>,<span class="number">0.091</span>),<span class="type">Array</span>(<span class="number">0.243</span>,<span class="number">0.267</span>),</span><br><span class="line">                  <span class="type">Array</span>(<span class="number">0.245</span>,<span class="number">0.057</span>),<span class="type">Array</span>(<span class="number">0.343</span>,<span class="number">0.099</span>),<span class="type">Array</span>(<span class="number">0.639</span>,<span class="number">0.161</span>),<span class="type">Array</span>(<span class="number">0.657</span>,<span class="number">0.198</span>),<span class="type">Array</span>(<span class="number">0.360</span>,<span class="number">0.370</span>),</span><br><span class="line">                  <span class="type">Array</span>(<span class="number">0.593</span>,<span class="number">0.042</span>),<span class="type">Array</span>(<span class="number">0.719</span>,<span class="number">0.103</span>),<span class="type">Array</span>(<span class="number">0.359</span>,<span class="number">0.188</span>),<span class="type">Array</span>(<span class="number">0.339</span>,<span class="number">0.241</span>),<span class="type">Array</span>(<span class="number">0.282</span>,<span class="number">0.257</span>),</span><br><span class="line">                  <span class="type">Array</span>(<span class="number">0.748</span>,<span class="number">0.232</span>),<span class="type">Array</span>(<span class="number">0.714</span>,<span class="number">0.346</span>),<span class="type">Array</span>(<span class="number">0.483</span>,<span class="number">0.312</span>),<span class="type">Array</span>(<span class="number">0.478</span>,<span class="number">0.437</span>),<span class="type">Array</span>(<span class="number">0.525</span>,<span class="number">0.369</span>),</span><br><span class="line">                  <span class="type">Array</span>(<span class="number">0.751</span>,<span class="number">0.489</span>),<span class="type">Array</span>(<span class="number">0.532</span>,<span class="number">0.472</span>),<span class="type">Array</span>(<span class="number">0.473</span>,<span class="number">0.376</span>),<span class="type">Array</span>(<span class="number">0.725</span>,<span class="number">0.445</span>),<span class="type">Array</span>(<span class="number">0.446</span>,<span class="number">0.459</span>))</span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> <span class="type">XX</span> = <span class="type">DenseMatrix</span>(<span class="type">X</span>:_*)</span><br><span class="line">   <span class="keyword">val</span> y = <span class="type">DenseVector</span>.zeros[<span class="type">Int</span>](<span class="type">XX</span>.rows)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i &lt;- <span class="number">9</span> until <span class="number">21</span>)&#123;</span><br><span class="line">      y(i) = <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> t = <span class="type">Array</span>(<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">    println(y)</span><br><span class="line">    <span class="keyword">val</span> lvq = <span class="keyword">new</span> <span class="type">LVQ</span>(t)</span><br><span class="line">    lvq.fit(<span class="type">XX</span>,y)</span><br><span class="line"></span><br><span class="line">    println(lvq.<span class="type">C</span>)</span><br><span class="line">    println(lvq.labels)</span><br><span class="line">    println(lvq.predict(<span class="type">XX</span>))</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>详细代码请参考：</p>
<p><a href="https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/LVQ.scala" target="_blank" rel="noopener">https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/LVQ.scala</a></p>
<p>代码实现参考了python代码：</p>
<p><a href="https://github.com/fengyang95/tiny_ml/blob/master/tinyml/cluster/LVQ.py" target="_blank" rel="noopener">https://github.com/fengyang95/tiny_ml/blob/master/tinyml/cluster/LVQ.py</a></p>
<h4 id="6-层次聚类"><a href="#6-层次聚类" class="headerlink" title="6 层次聚类"></a>6 层次聚类</h4><p>层次聚类(hierarchical clustering)是一种基于树形结构的聚类方法，常用的是<strong>自底向上</strong>的结合策略（<strong>AGNES算法</strong>），它将数据集中的每个样本看作一个初始聚类簇，然后再算法运行的每一步中找到距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。这里的关键是如何计算聚类簇之间的距离。实际上，每个簇是一个样本集合，只需要采用集合的某种距离即可。例如，给定聚类簇$C_i$与$C_j$,可以通过下面的式子来计算距离：</p>
<p>最小距离：</p>
<script type="math/tex; mode=display">
d_{\min}(C_i,C_j) = \min _{x \in C_i,z \in C_j}dist(x,z)  \tag{15}</script><p>最大距离：</p>
<script type="math/tex; mode=display">
d_{\max}(C_i,C_j) = \max_{x \in C_i,z \in C_j}dist(x,z)  \tag{16}</script><p>平均距离：</p>
<script type="math/tex; mode=display">
d_{avg}(C_i,C_j) = \frac{1}{|C_i||C_j|}dist(x,z)  \tag{17}</script><p>显然，最小距离由两个簇的最近的样本决定；最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本决定。</p>
<h5 id="6-1-算法步骤"><a href="#6-1-算法步骤" class="headerlink" title="6.1  算法步骤"></a>6.1  算法步骤</h5><p>AGNES 算法步骤如下,在1-9行，算法先对仅含一个样本的初始聚类簇和相应的距离进行初始化，然后在11-23行，AGNES不断合并距离最近的聚类簇，并对合并得到的聚类簇的距离矩阵进行更新；上述过程不断重复，直至达到预设的聚类簇数。</p>
<p><img src="/2020/05/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/hc_算法步骤.png" alt></p>
<h5 id="6-2-算法实现"><a href="#6-2-算法实现" class="headerlink" title="6.2 算法实现"></a>6.2 算法实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.strings.model.cluster</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> breeze.linalg.&#123;<span class="type">DenseMatrix</span>, <span class="type">DenseVector</span>&#125;</span><br><span class="line"><span class="keyword">import</span> com.strings.data.<span class="type">Data</span></span><br><span class="line"><span class="keyword">import</span> com.strings.utils.<span class="type">MatrixUtils</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ClusterNode</span>(<span class="params">vector:<span class="type">DenseVector</span>[<span class="type">Double</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">                       id:<span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                       left:<span class="type">ClusterNode</span> = null,</span></span></span><br><span class="line"><span class="class"><span class="params">                       right:<span class="type">ClusterNode</span> = null,</span></span></span><br><span class="line"><span class="class"><span class="params">                       distance:<span class="type">Double</span> = -1.0,</span></span></span><br><span class="line"><span class="class"><span class="params">                       count:<span class="type">Int</span> = 1</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">HierarchicalCluster</span>(<span class="params">k:<span class="type">Int</span></span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> labels:<span class="type">DenseVector</span>[<span class="type">Int</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(<span class="type">X</span>:<span class="type">DenseMatrix</span>[<span class="type">Double</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">val</span> n_samples = <span class="type">X</span>.rows</span><br><span class="line">    <span class="keyword">val</span> n_features = <span class="type">X</span>.cols</span><br><span class="line">    <span class="keyword">val</span> <span class="type">X_arr</span> = (<span class="number">0</span> until n_samples).map(<span class="type">X</span>.t(::,_))</span><br><span class="line">    <span class="keyword">val</span> nodes:<span class="type">ArrayBuffer</span>[<span class="type">ClusterNode</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">ClusterNode</span>]()</span><br><span class="line">    <span class="keyword">for</span>((sample,inx) &lt;- <span class="type">X_arr</span>.zipWithIndex)&#123;</span><br><span class="line">      nodes.append(<span class="type">ClusterNode</span>(sample,inx))</span><br><span class="line">    &#125;</span><br><span class="line">    labels = <span class="type">DenseVector</span>.ones[<span class="type">Int</span>](n_samples) :* (<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">var</span> distances: <span class="type">Map</span>[(<span class="type">Int</span>, <span class="type">Int</span>), <span class="type">Double</span>] = <span class="type">Map</span>()</span><br><span class="line">    <span class="keyword">var</span> curret_cluster_id = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">while</span> (nodes.length &gt; k)&#123;</span><br><span class="line">      <span class="keyword">var</span> min_dist = <span class="type">Double</span>.<span class="type">MaxValue</span></span><br><span class="line">      <span class="keyword">val</span> nodes_len = nodes.length</span><br><span class="line">      <span class="keyword">var</span> closest_part:(<span class="type">Int</span>, <span class="type">Int</span>) = <span class="number">0</span> -&gt; <span class="number">0</span></span><br><span class="line">      <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until nodes_len - <span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span>(j &lt;- i+<span class="number">1</span> until nodes_len)&#123;</span><br><span class="line">          <span class="keyword">val</span> d_key = nodes(i).id -&gt; nodes(j).id</span><br><span class="line">          <span class="keyword">if</span>(!distances.contains(d_key))&#123;</span><br><span class="line">            distances += (d_key -&gt; <span class="type">MatrixUtils</span>.euclidean_distance(nodes(i).vector,nodes(j).vector))</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> d = distances(d_key)</span><br><span class="line">          <span class="keyword">if</span>(d &lt; min_dist)&#123;</span><br><span class="line">            min_dist = d</span><br><span class="line">            closest_part = i -&gt; j</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> part1 = closest_part._1</span><br><span class="line">      <span class="keyword">val</span> part2 = closest_part._2</span><br><span class="line">      <span class="keyword">val</span> node1 = nodes(part1)</span><br><span class="line">      <span class="keyword">val</span> node2 = nodes(part2)</span><br><span class="line">      <span class="keyword">val</span> new_vec = <span class="type">DenseVector</span>.ones[<span class="type">Double</span>](n_features)</span><br><span class="line">      <span class="keyword">for</span>(i &lt;- <span class="number">0</span> until n_features)&#123;</span><br><span class="line">        new_vec(i) = (node1.vector(i) * node1.count + node2.vector(i) * node2.count)/</span><br><span class="line">          (node1.count + node2.count)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> new_count = node1.count + node2.count</span><br><span class="line">      <span class="keyword">val</span> new_node = <span class="type">ClusterNode</span>(new_vec,curret_cluster_id,node1,node2, min_dist,new_count)</span><br><span class="line"></span><br><span class="line">      curret_cluster_id -= <span class="number">1</span></span><br><span class="line">      nodes.remove(part2)</span><br><span class="line">      nodes.remove(part1)</span><br><span class="line">      nodes.append(new_node)</span><br><span class="line">    &#125;</span><br><span class="line">    calc_label(nodes)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">calc_label</span></span>(nodes:<span class="type">ArrayBuffer</span>[<span class="type">ClusterNode</span>]): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">for</span>((node,inx) &lt;- nodes.zipWithIndex)&#123;</span><br><span class="line">      leaf_traveral(node,inx)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">leaf_traveral</span></span>(node:<span class="type">ClusterNode</span>,label:<span class="type">Int</span>): <span class="type">Unit</span> =&#123;</span><br><span class="line">    <span class="keyword">if</span>(node.left == <span class="literal">null</span> &amp;&amp; node.right == <span class="literal">null</span>)&#123;</span><br><span class="line">      labels(node.id) = label</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(node.left != <span class="literal">null</span>)&#123;</span><br><span class="line">      leaf_traveral(node.left,label)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(node.right != <span class="literal">null</span>)&#123;</span><br><span class="line">      leaf_traveral(node.right,label)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HierarchicalCluster</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> irisData = <span class="type">Data</span>.irisData</span><br><span class="line">    <span class="keyword">val</span> data = irisData.map(_.slice(<span class="number">0</span>,<span class="number">4</span>))</span><br><span class="line">    <span class="keyword">val</span> dd = <span class="type">DenseMatrix</span>(data:_*)</span><br><span class="line">    <span class="keyword">val</span> hc = <span class="keyword">new</span> <span class="type">HierarchicalCluster</span>(k=<span class="number">3</span>)</span><br><span class="line">    hc.fit(dd)</span><br><span class="line">    println(hc.labels)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>算法实现参考了：</p>
<p><a href="https://zhuanlan.zhihu.com/p/32438294" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32438294</a></p>
<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ol>
<li>周志华 机器学习 - 聚类部分</li>
</ol>
<p>p.s. 该总结主要介绍每个聚类算法的算法步骤，以及scala的简单实现，并没有多注重效率，仅供自己学习使用。</p>

    
  </div>

</article>


   
  <div class="text-center donation">
    <div class="inner-donation">
      <span class="btn-donation">支持一下</span>
      <div class="donation-body">
        <div class="tip text-center">扫一扫，支持forsigner</div>
        <ul>
        
          <li class="item">
            
              <span>微信扫一扫</span>
            
            <img src="/images/qr-wechat.png" alt="">
          </li>
        
          <li class="item">
            
              <span>支付宝扫一扫</span>
            
            <img src="/images/qr-alipay.png" alt="">
          </li>
        
        </ul>
      </div>
    </div>
  </div>


   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2020/05/08/adaboost%E7%AE%97%E6%B3%95/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2020/06/29/leetcode%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E7%B3%BB%E5%88%971-%E5%A0%86/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">关闭</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/link/"
              rel="noopener noreferrer"
              target="_self"
              >
              友链
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/about/"
              rel="noopener noreferrer"
              target="_self"
              >
              关于
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/atom.xml"
              rel="noopener noreferrer"
              target="_blank"
              >
              RSS
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/search/"
              rel="noopener noreferrer"
              target="_self"
              >
              搜索
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-post">
    

    

    
    

    

    
    

    

<!-- Gitalk评论插件通用代码 -->
<div id="gitalk-container"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<script>
const gitalk = new Gitalk({
  clientID: '277317290454635afa73',
  clientSecret: '78223c859304da5e8d71bfce136d436c6ff95c37',
  repo: 'blog-comment',
  owner: 'forsigner',
  // 在这里设置一下截取前50个字符串, 这是因为 github 对 label 的长度有了要求, 如果超过
  // 50个字符串则会报错.
  // id: location.pathname.split('/').pop().substring(0, 49),
  id: md5(location.pathname),
  admin: ['forsigner'],
  // facebook-like distraction free mode
  distractionFreeMode: false
})
gitalk.render('gitalk-container')
</script>
<!-- Gitalk代码结束 -->



  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
