[{"title":"adaboost算法","url":"/2020/05/08/adaboost算法/","content":"Boosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。其中最为成功应用的是，Yoav Freund和Robert Schapire在1995年提出的AdaBoost算法。\n\n​      AdaBoost是英文\"Adaptive Boosting\"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。\n\n##### 算法过程\n\n给定训练数据集： $(x_1,y_1),...,(x_N,y_N)$，其中 $y_i \\in \\{1,-1\\}$，用于表示训练样本的类别标签$i=1,...,N$。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。\n\n##### 算法详细步骤\n\n1. 初始化数据的权值分布\n   $$\n   D_1 = (w_{11},...,w_{1i},...,w_{1N}),w_{1i} = \\frac{1}{N},i=1,2,...,N\n   $$\n\n2. 对$m=1,2,...,M$\n\n   (a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n   $$\n   G_m(x):f \\rightarrow  \\{-1,+1\\}\n   $$\n   (b) 计算$G_m(x)$在训练数据集上的分类误差率\n   $$\n   e_m = P(G_m(x_i) \\neq y_i) = \\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)\n   $$\n   (c) 计算$G_m(x)$的系数\n   $$\n   \\alpha_m = \\frac{1}{2}\\log\\frac{1-e_m}{e_m}\n   $$\n   这里的对数是自然对数\n\n   (d) 更新训练数据集的权值分布\n   $$\n   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\\\\n   w_{m+1,i} = \\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),i=1,2,..,N\n   $$\n   这里，$Z_m$是规范化因子\n   $$\n   Z_m = \\sum_{i=1}^Nw_{mi}\\exp(-\\alpha_my_iG_m(x_i))\n   $$\n   它使$D_{m+1}$成为一个概率分布\n\n3. 构建基本分类器的线性组合\n   $$\n   f(x) = \\sum_{m=1}^{M}\\alpha_mG_m(x)\n   $$\n   得到最终的分类器\n   $$\n   G(x) = sign(f(x)) = sign(\\sum_{m=1}^M\\alpha_mG_m(x))\n   $$\n   对AdaBoost算法做下面的说明：\n\n    （1）首先，是初始化训练数据的权值分布$D_1$。假设有$N$个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值$w_1 = \\frac{1}{N}$。\n\n    （2）然后，训练弱分类器$h_i$。具体训练过程中是：如果某个训练样本点，被弱分类器$h_i$准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。\n\n    （3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n     换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。"},{"title":"降维方法-总结","url":"/2020/05/06/降维方法-总结/","content":"#### 1. 降维概述\n\n样本的特征数称为维数（dimensionality），当维数非常大时，也就是现在所说的“维数灾难”，具体表现在：在高维情形下，数据样本将变得十分稀疏，因为此时要满足训练样本为“密采样”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉...训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力；同时当维数很高时，计算距离也变得十分复杂，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数“低维计算，高维表现”的原因。\n\n缓解维数灾难的一个重要途径就是降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个低维嵌入，例如：数据属性中存在噪声属性、相似属性或冗余属性等，对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果。\n\n#### 2.降维方法分类\n\n![这里写图片描述](http://img.blog.csdn.net/20150522194801297)\n\n#### 3 线性方法\n\n##### 3.1 PCA主成分分析\n\n主成分分析（PCA）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中。简单来理解这一过程便是：PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。\n\n假设使用$d^{’}$个新基向量来表示原来样本，实质上是将样本投影到一个由$d^{’}$个基向量确定的一个超平面上（即舍弃了一些维度），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：\n\n> **最近重构性**：样本点到超平面的距离足够近，即尽可能在超平面附近；\n> **最大可分性**：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。\n\nPCA的算法描述如下：\n\n![](.\\降维方法-总结\\pca_算法步骤.png)\n\n##### 3.2 LDA 线性判别分析\n\n参考之前的博文\n\n#### 4 非线性方法\n\n##### 4.1 MDS \n\n不管是使用核函数升维还是对数据降维，我们都希望**原始空间样本点之间的距离在新空间中基本保持不变**，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。**“多维缩放”（MDS）**正是基于这样的思想，**MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持**。\n\n假定$m$个样本在原始空间中任意两两样本之间的距离矩阵为$D \\in R ^{m \\times m}$，其中第$i$行$j$列的元素$dist_{ij}$为样本$x_i$到$x_j$的距离。我们的目标便是获得样本在低维空间中的表示$Z \\in R^{d^{’} \\times m} $, $d'< d$，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即$||zi-zj||=dist_{ij}$。因此接下来我们要做的就是根据已有的距离矩阵$D$来求解出降维后的坐标矩阵$Z$。\n\n令降维后的样本坐标矩阵Z被中心化，**中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量**。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。\n\n![4.png](https://i.loli.net/2018/10/18/5bc851a4a4ee2.png)\n\n令$B = Z^TZ \\in R^{m \\times m}$,其中$B$为降维后的样本的內积矩阵，$b_{ij} = z_i^Tz_j$,有：\n$$\ndist_{ij}^{2} = ||z_i||^2 + ||z_j||^2 -2z_i^Tz_j\n= b_{ii} + b_{jj} - 2b_{ij}\n$$\n为了方便讨论，令降维后的样本$Z$被中心化，即$\\sum_{i=1}^{m}z_i = 0$,显然$B$的行与列之和均为0，即$\\sum_{i=1}^{m}b_{ij} =\\sum_{j=1}^{m}b_{ij} =  0$,容易知道：\n$$\n\\sum_{i=1}^{m}dist_{ij}^2 = tr(B) +mb_{jj}\n$$\n\n$$\n\\sum_{j=1}^{m}dist_{ij}^2 = tr(B) +mb_{ii}\n$$\n\n$$\n\\sum_{i=1}^m\\sum_{i=1}^{m}dist_{ij}^2 = 2m * tr(B)\n$$\n\n其中$tr(.)$表示矩阵的迹(trace),$tr(B) = \\sum_{i=1}^{m}||z_i||^2$,令\n$$\ndist_{i.}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{.j}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{..}^2=\\frac{1}{m^2}\\sum_{i=1}^{m}\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n由上面三个等式可知：\n$$\nb_{ij} = -0.5(dist_{ij}^2 - dist_{i.}^2 -dist_{.j}^2 + dist_{..}^2)\n$$\n通过降维前后保持不变的距离矩阵$D$求取內积矩阵$B$.\n\n对$B$做特征值分解(**eigenvalue decompostion**), $B = VUV^T$,其中$U=diag(\\lambda _1,\\lambda _2,...,\\lambda _d)$为特征值构成的对角矩阵，$\\lambda _1 \\geq \\lambda _2 \\geq ... \\geq \\lambda _d$,$V$为特征向量矩阵。假定其中有$d^*$个非零特征值，他们构成的对角矩阵 $U_* = diag(\\lambda _1,\\lambda _2,...,\\lambda _{d^{*}})$ ,其中$V_*$表示对应的特征向量矩阵，则$Z$可以表示为：\n$$\nZ = U_*^{\\frac{1}{2}}V_*^T \\in R^{d^*\\times m}\n$$\nMDS 的算法描述如下：\n\n![](.\\降维方法-总结\\mds_算法步骤.png)\n\n##### 4.2 Isomap\n\n等度量映射（Isomap)属于流行学习，流形学习（manifold learning）是一种借助拓扑流形概念的降维方法**，**流形是指在局部与欧式空间同胚的空间，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种**“邻域保持”**的思想 ，等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系。\n\n等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。**因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离**，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的**Dijkstra算法**或**Floyd算法**计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。\n\n![](./降维方法-总结/测地距离.png)\n\n从**MDS**算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵$B$，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量$w$，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。\n\nISOMAP的算法步骤如下：\n\n![](./降维方法-总结/isomap_算法步骤.png)\n\n对于近邻图的构建，常用的有两种方法：**一种是指定近邻点个数**，像**KNN**一样选取k个最近的邻居；**另一种是指定邻域半径**，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：\n\n> 若**邻域范围指定过大，则会造成“短路问题”**，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。\n> 若**邻域范围指定过小，则会造成“断路问题”**，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。\n\n##### 4.3 LLE \n\n不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本$x_i$的坐标可以通过它的邻域样本线性表出：\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE希望上式的关系在低维空间中得以保持。\n\n![](./降维方法-总结/LLE空间保持.png)\n\nLLE先为每个样本$x_i$找到其近邻下标集合$Q_i$,然后计算基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$.\n$$\n\\min_{w_1,w_2,...,w_m}\\sum_{i=1}^m||x_i - \\sum_{j \\in Q_i}w_{ij}{x_j}||_2^2\n$$\n\n$$\ns.t. \\sum_{j\\in Q_i}w_{ij} = 1\n$$\n\n其中$x_i$和$x_j$均为已知，令$C_{jk} = (x_i- x_j)^T(x_i-x_j)$,$w_{ij}$有闭式解\n$$\nw_{ij} = \\frac{\\sum_{k \\in Q_i}C_{jk}^{-1}}{\\sum_{l,s \\in Q_i}C_{ls}^{-1}}\n$$\n**LLE**在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解：\n$$\n\\min_{z_1,z_2,...,z_m}\\sum_{i=1}^m||z_i - \\sum_{j \\in Q_i}w_{ij}{z_j}||_2^2\n$$\n上式需要确定的是$x_i$对应的低维空间坐标$z_i$.\n\n令$Z =(z_1,z_2,...,z_m) \\in R^{d'\\times m}$,$(W)_{ij} = w_{ij}$,\n$$\nM = (I-W)^T(I-W)\n$$\n则优化目标可以重写为下式：\n$$\n\\begin{equation}\n\\min_z tr(ZMZ^T) \\\\\ns.t. ZZ^T = I \n\\end{equation}\n$$\n可以通过特征值分解求解，$M$最小的$d'$的特征值对应的特征向量组成的矩阵即为$Z^T$\n\nLLE算法步骤如下：\n\n![](./降维方法-总结/lle_算法步骤.png)\n\n##### 4.4 核PCA kernel PCA\n\n说起机器学习你中有我/我中有你/水乳相融...在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，**即先将样本映射到高维空间，再在高维空间中使用线性降维的方法**。下面主要介绍**核化主成分分析（KPCA）**的思想。\n\n若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：**即空间中的任一向量，都可以由该空间中的所有样本线性表示**。\n\n假定我们将在高维特征空间中的数据投影到由$W$确定的超平面上，即PCA欲求解\n$$\n(\\sum_{i=1}^mz_iz_i^T)W = \\lambda W\n$$\n其中$z_i$是样本点$x_i$在高维特征空间的像，可知：\n$$\nW = \\frac{1}{\\lambda}(\\sum_{i=1}^mz_iz^T_i)W = \\sum_{i=1}^m z_i\\frac{z_i^TW}{\\lambda} \\\\\n=\\sum_{i=1}^m z_i\\alpha _i\n$$\n其中$\\alpha _i = \\frac{1}{\\lambda}z_i^TW$.假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i = \\phi (x_i),i= 1,2,...,m$,若$\\phi$能被显式的表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。\n\n即（17）可变为\n$$\n(\\sum_{i=1}^m \\phi (x_i) \\phi (x_i)^T)W = \\lambda W\n$$\n（18）式可变为：\n$$\nW= \\sum_{i=1}^m \\phi(x_i)\\alpha_i\n$$\n一般情形下，我们不清楚$\\phi$的具体形式，于是引入核函数:\n$$\n\\kappa(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\n$$\n将（21)(20)带入到(19)式中可得：\n$$\nKA= \\lambda A\n$$\n其中$K$为$\\kappa$ 对应的核矩阵，$(K)_{ij} = \\kappa(x_i,x_j),A = (\\alpha_1;\\alpha_2;...;\\alpha_m)$,显然上式是个特征值分解问题，取$K$最大的$d'$个特征值对应的特征向量即可。\n\n##### 4.5 DiffusionMap\n\n扩散映射是一种降维方法\n\n1. 其通过 整合数据的局部几何关系 揭示 数据集在不同尺度的几何结构。\n2. 与PCA (principal component analysis)、MDS (Multidimensional Scaling) 这些降维方法相比，扩散映射 非线性，聚焦于发现数据集潜在的流形结构。\n3. 优点：对噪声鲁棒，计算代价较低\n\n算法步骤如下:\n\n![](./降维方法-总结/diffmap_算法步骤.png)"},{"title":"xgboost","url":"/2020/03/18/xgboost/","content":"#### 1. XGBoost 简介\n\n$XGBoost$的全称是$e\\textcolor{red}Xtreme \\textcolor{red}Gradient \\textcolor{red}{Boosting}$，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。$XGBoost$是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用$XGBoost$进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，$XGBoost$的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从$XGBoost$的数学原理和工程实现上进行介绍，然后介绍$XGBoost$的优缺点，并在最后给出面试中经常遇到的关于$XGBoost$的问题。\n\n#### 2.  $XGBoost \\ $算法\n\n​    $XGBoost$是由 $k$个基模型组成的一个加法模型，假设我们第 $t$ 次迭代要训练的树模型是 $f_t(x)$ ，则有：\n\n​\t\n$$\n\\hat{y_i}^{(t)} = \\sum_{k=1}^{t}f_k(x_i) = \\hat{y_i}^{(t-1)} + f_t(x_i)\n$$\n其中，$\\hat{y_i}^{(t)}$是第$t$次迭代后样本的$i$的预测结果，$\\hat{y_i}^{(t-1)}$是前$t-1$棵树的预测结果，$f_t(x_i)$是第$t$棵树的模型。\n\n​\t$XGBoost  \\ $算法是$\\ GBDT\\ $算法的改进版本，其目标函数为：\n$$\n\\begin{aligned}\nObj^{(k)}&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_k)+\\sum\\limits_{i=1}^T\\Omega(f_i)\\\\\n&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\\Omega(f_k)+C\n\\end{aligned}\n$$\n同理为了求损失函数$\\ l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处的二阶展开，不妨先对$\\ l(y^{(i)},x)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处进行二阶展开可得：\n$$\nl(y^{(i)},x)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+\\nabla_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})+\\dfrac{1}{2}\\nabla^2_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})^2\n$$\n令$\\ x=\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}) \\ $，且记$\\ \\nabla_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ g_i\\ $、$\\ \\nabla^2_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ h_i\\ $则有：\n$$\nl\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\n$$\n其中，$g_i$为损失函数的一阶导数，$h_i$为损失函数的二阶导数，注意这里是对$\\hat{y}^{(t-1)}_{i}$求导。\n\n**以平方损失为例：**\n$$\nl(y^{(i)},\\hat{y}^{(t-1)}_i) = (y^{(i)},\\hat{y}^{(t-1)}_i)^2\n$$\n则：\n$$\n\\begin{aligned}\ng_i &= \\frac{\\partial{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{\\hat{y}^{(t-1)}_i}} = -2(y_i - \\hat{y}_i^{(t-1)}) \\\\\nh_i &= \\frac{\\partial ^2{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{(\\hat{y}^{(t-1)}_i})^2} = 2\n\\end{aligned}\n$$\n又因为在第$\\ k\\ $步$\\ \\hat{y}^{(i)}_{k-1} \\ $其实是已知的，所以$\\ l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\ $是一个常数函数，故对优化目标函数不会产生影响，将上述结论带入目标函数$\\ Obj^{(k)}\\ $可得：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\n$$\n\n##### 2.1  优化目标函数\n\n​\t以$\\ XGBoost\\ $算法的目标函数为例，对于任意决策树$\\ f_k \\ $，**假设其叶子结点个数$\\ T\\ $，该决策树是由所有结点对应的值组成的向量$\\ w\\in\\mathbb{R}^T\\ $，以及能够把特征向量映射到叶子结点的函数$\\ q(*):\\mathbb{R}^d\\rightarrow \\{1,2,\\cdots,T \\} \\ $构造而成的，且每个样本数据都存在唯一的叶子结点上。因此决策树$\\ f_k\\ $可以定义为$\\ f_k(x)=w_{q(x)} \\ $。**决策树的复杂度可以由正则项$\\ \\Omega(f_k)=\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2 \\ $来定义，该正则项表明决策树模型的复杂度可以由叶子结点的数量和叶子结点对应值向量$\\ w \\ $的$\\ L2\\ $范数决定。定义集合$\\ I_j=\\{i|q(x^{(i)})=j \\}\\ $为划分到叶子结点$\\ j \\ $的所有训练样本的集合，即之前训练样本的集合，现在都改写成叶子结点的集合，因此$\\ XGBoost\\ $算法的目标函数可以改写为：\n$$\n\\begin{aligned}\nObj^{(k)}&\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\\\\\n&=\\sum\\limits_{i=1}^m\\bigg[g_iw_{q(x^{(i)})}+\\dfrac{1}{2}h_jw^2_{q(x^{(i)})} \\bigg]+\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2\\\\\n&=\\sum\\limits_{j=1}^T\\bigg[(\\sum\\limits_{i\\in I_j}g_i)w_j+\\dfrac{1}{2}(\\sum\\limits_{i\\in I_j}h_i+\\lambda)w_j^2 \\bigg]+\\gamma T\n\\end{aligned}\n$$\n令$\\ G_j=\\sum\\limits_{i\\in I_j}g_i ,\\ H_j=\\sum\\limits_{i\\in I_j}h_i \\ $则有：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{j=1}^T\\bigg[G_jw_j+\\dfrac{1}{2}(H_j+\\lambda)w_j^2 \\bigg]\n$$\n分析可知当更新到第$\\ k\\ $步时，此时**决策树结构固定的情况下**，每个叶子结点有哪些样本是已知的，那么$\\ q(*)\\ $和$\\ I_j\\ $也是已知的；又因为$\\ g_i\\ $和$\\ h_i\\ $是第$\\ k-1\\ $步的导数，那么也是已知的，因此$\\ G_j\\ $和$\\ H_j\\ $都是已知的。令目标函数$\\ Obj^{(k)}\\ $的一阶导数为$\\ 0\\ $，即可求得叶子结点$\\ j\\ $对应的值为：\n$$\nw^*_j=-\\dfrac{G_j}{H_j+\\lambda}\n$$\n因此针对于结构固定的决策树，最优的目标函数$\\ Obj\\ $为：\n$$\nObj=-\\dfrac{1}{2}\\sum\\limits_{j=1}^T\\dfrac{G_j^2}{H_j+\\lambda}+\\gamma T\n$$\n上面的推导是建立在决策树结构固定的情况下，然而决策树结构数量是无穷的，所以实际上并不能穷举所有可能的决策树结构，什么样的决策树结构是最优的呢？通常使用贪心策略来生成决策树的每个结点，$\\ XGBoost \\ $算法的在决策树的生成阶段就对过拟合的问题进行了处理，因此无需独立的剪枝阶段，具体步骤可以归纳为：\n\n1. 从深度为$\\ 0\\ $的树开始对每个叶子结点穷举所有的可用特征；\n2. 针对每一个特征，把属于该结点的训练样本的该特征升序排列，通过线性扫描的方式来决定该特征的**最佳分裂点**，并采用最佳分裂点时的**收益**；\n3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该结点生成出左右两个新的叶子结点，并为每个新结点关联新的样本集；\n4. 退回到第一步，继续递归操作直到满足特定条件。\n\n因为对某个结点采取的是二分策略，分别对应左子结点和右子结点，除了当前待处理的结点，其他结点对应的$\\ Obj \\ $值都不变，所以对于收益的计算只需要考虑当前结点的$\\ Obj \\ $值即可，分裂前针对该结点的最优目标函数为：\n$$\nObj^{(before)}=-\\dfrac{1}{2}\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda}+\\gamma\n$$\n分裂后的最优目标函数为：\n$$\nObj^{(later)}=-\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda} \\bigg]+2\\gamma\n$$\n那么对于该目标函数来说，分裂后的收益为：\n$$\n\\begin{aligned}\nGain&=Obj^{(before)}-Obj^{(later)}\\\\\n&=\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda}-\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda} \\bigg]-\\gamma\n\\end{aligned}\n$$\n故可以用上述公式来决定最有分裂特征和最优特征分裂点。\n\n##### 2.2  总结\n\n​\t$XGBoost  \\ $算法的过程可以归纳为：\n\n1. 前向分布算法的每一步都生成一棵决策树；\n2. 拟合该决策树之前，先计算损失函数在每个样本数据上的一阶导$\\ g_i \\ $和二阶导$\\ h_i \\ $；\n3. 通过贪心策略生成一棵决策树，计算每个叶子结点的$\\ G_j\\ $和$\\ H_j\\ $并计算预测值$\\ w\\ $；\n4. 把新生成的决策树$\\ f_k(x)\\ $加入$\\ \\hat{y}^{(i)}_k=\\hat{y}^{(i)}_{k-1}+\\epsilon f_k(x^{(i)}) \\ $，其中$\\ \\epsilon\\ $是学习率主要控制模型的过拟合。\n\n#### 3 $XGBoost\\ $的优缺点\n\n​\t相比于普通的$\\ GBDT \\ $算法$\\ XGBoost\\ $算法的主要优点在于：\n\n- 不仅支持决策树作为基分类器，还支持其它线性分类器；\n- 使用了损失函数的二阶泰勒展开，因此与损失函数更接近，收敛速度更快；\n-  在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的  范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统GBDT的一个特性。；\n- $Shrinkage\\ $也就是之前说的$\\ \\epsilon\\ $，主要用于削弱每棵决策树的影响，让后面有更大的学习空间，实际应用中一般把$\\ \\epsilon\\ $设置的小点，迭代次数设置的大点；\n- 列抽样，$\\ XGBoost\\ $从随机森林算法中借鉴而来，支持列抽样可以降低过拟合，并且减少计算；\n- 支持对缺失值的处理，对于特征值缺失的样本，$\\ XGBoost\\ $可以学习这些缺失值的分裂方向；\n- 支持并行，boosting不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是tree粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第$t$次迭代的代价函数里包含了前面$t-1$次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。；\n- 近似算法，决策树结点在分裂时需要穷举每个可能的分裂点，当数据没法全部加载到内存中时，这种方法会比较慢，$\\ XGBoost\\ $提出了一种近似的方法去高效的生成候选分割点。\n\n缺点\n\n- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；\n- 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。\n\n#### 4. 关于XGBosst的若干问题\n\n##### 4.1 XGBoost与GBDT的联系和区别有哪些？\n\n1. GBDT是机器学习算法，XGBoost是该算法的工程实现。\n2. **正则项：** 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。\n3. **导数信息：** GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。\n4. **基分类器：** 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。\n5. **子采样：** 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。\n6. **缺失值处理：** 传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。\n7. **并行化：** 传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。\n\n##### 4.2 为什么XGBoost泰勒二阶展开后效果就比较好呢？\n\n（1）**从为什么会想到引入泰勒二阶的角度来说（可扩展性）：** XGBoost官网上有说，当目标函数是`MSE`时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如`logistic loss`的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把`MSE`推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与`MSE`统一？是因为`MSE`是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与`MSE`统一了，那就只用推导`MSE`就好了。\n\n（2）**从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：** 二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。\n\n##### 4.3 XGBoost对缺失值是怎么处理的？\n\n在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。\n\n#### 5 参考文献 \n\n- 陈天奇论文原文 XGBoost: A Scalable Tree Boosting System\n- [深入理解 XGBoost：Kaggle 最主流的集成算法！](mp.weixin.qq.com/s/0dehrF5Zvn5ILkadfsqfUw)\n\n"},{"title":"梯度提升树","url":"/2020/03/18/梯度提升树/","content":"提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。\n\n##### 1.1 提升树模型\n\n提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。\n\n提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示成决策树的加法模型。\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n​\t其中，$T(x;\\Theta_m)$表示决策树；$\\Theta_m$表示决策树的参数；$M$为树的个数.\n\n##### 1.2 提升树算法\n\n提升树算法采取前向分步算法。首先确定初始提升树$f_0(x) = 0$,第$m$步的模型是\n$$\nf_m(x) = f_{m-1}(x)+T(x;\\Theta_m)\n$$\n其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$,\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n由于树的线性组合可以很好的拟合训练数据，即数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。\n\n下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的一般决策问题。\n\n对于二分类分类问题，提升树算法只需要将$Adaboost$算中基本分类器限制为二类分类树即可，可以说这时的提升树算法是$Adaboost$算法的特殊情况，这里不再详细叙述。下面重点叙述回归问题的提升树。\n\n已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间。如果将输入空间$\\chi$划分成$J$个互不相交的区域$R_1,R_2,...,R_J$，并且在每个区域上确定输出的常量$c_j$,那么树可以表示为\n$$\nT(x;\\Theta) = \\sum_{j=1}^{J}c_jI(x \\in R_j)\n$$\n其中参数$\\Theta={(R_1,c_1),(R_2,c_2),...,(R_J,c_J)}$表示树的区域划分和各区域上常数，$J$是回归树的复杂度即叶子节点的个数。\n\n回归问题提升树使用以下前向分布算法：\n$$\n\\begin{aligned}\nf_0(x) &= 0\\\\\nf_m(x) &= f_{m-1}(x) + T(x;\\Theta_m),m = 1,2,...,M \\\\\nf_M(x) &= \\sum_{m=1}^{M}T(x;\\Theta_m) \\\\\n\\end{aligned}\n$$\n在前向分布算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n得到$\\hat{\\Theta}_m$,即第$m$棵树的参数。\n\n当采用平方损失函数时，\n$$\nL(y,f(x)) = (y - f(x))^{2}\n$$\n其损失变为\n$$\n\\begin{aligned}\nL(y,f_{m-1}(x) - T(x;\\Theta_m)) &=(y - f_{m-1}(x) - T(x;\\Theta_m))^{m}\\\\\n &= (r - T(x;\\Theta_m))^2\n\\end{aligned}\n$$\n这里,\n$$\nr = y - f_{m-1}(x)\n$$\n是当前模型拟合数据的**残差(residual)**,所以，对回归问题的提升树来说，只需要简单地拟合当前模型的残差。这样，算法是相当简单。 \n\n**算法1 回归问题的提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出：提升树$f_M(x)$\n\n(1) 初始化$f_0(x) = 0$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 按照式$r = y - f_{m-1}(x)$计算残差\n$$\nr_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N\n$$\n​     (b) 拟合残差$r_{mi}$学习一棵回归树，得到$T(x;\\Theta_m)$\n\n​\t (c) 更新$f_m(x) = f_{m-1}(x) + T(x;\\Theta_m)$\n\n(3) 得到回归问题提升树\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n\n##### 1.3 梯度提升\n\n梯度提升(Gradient Boosting）是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失时，每一步优化是很简单的。对于一般的损失函数而言，往往每一步优化并不是那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，**其关键是利用损失函数的负梯度在当前模型的值**：\n$$\n-[\\frac{\\partial{L(y,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n**作为回归问题提升树算法中残差的近似值**，拟合一个回归树。\n\n**算法2 梯度提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出: 回归树$\\hat{f}(x)$\n\n(1) 初始化$f_0(x) = \\arg \\min_{c}\\sum_{i=1}^{N}L(y_i,c)$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 对$i = 1,2, ...,N$计算残差\n$$\nr_{mi} = -[\\frac{\\partial{L(y_i,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n​     (b) 拟合$r_{mi}$学习一棵回归树，得到第$m$棵树的叶节点区域$R_{mj},j  = 1,2,...,J$\n\n​\t (c) 对于$j  = 1,2,...,J$，计算\n$$\nc_{mj} = \\arg \\min_{c}\\sum_{x_i \\in R_{mj}}L(y_i,f_{m-1}(x_i) +c)\n$$\n​    (d）更新$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^Jc_{mi}I(x \\in R_{mj})$\n\n(3) 得到回归问题提升树\n$$\n\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x\\in R_{mj})\n$$\n算法的第一步初始化，估计使损失函数极小化的常数值，它只有一个根节点的树，第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数来说，它的负梯度其实就是常说的残差，对于一般的损失函数，它就是残差的近似值。第2（b)步，估计回归树叶子节点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶子终点区域的值，使损失函数极小化。第2（d）步更新回归树。第3步，得到输出的最终模型。\n\n##### 1.4 提升树主要损失函数\n\n下面我们对提升树所用的损失函数做一个总结：\n\n1) 对于分类算法来说：其损失函数一般有对数损失函数和指数损失函数：\n\na）**指数损失函数**\n$$\nL(y_i,f(x_i)) = exp(-y_if(x_i))\n$$\n其负梯度误差为：\n$$\n-y_i.exp(-f(x_i))\n$$\nb）**对数损失函数**\n$$\nL(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))\n$$\n其负梯度为：\n$$\n\\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}\n$$\n化简为：\n$$\n\\frac{y_i}{(1+exp(y_if(x_i)))}\n$$\n2) 回归算法：常见的有以下四种\n\n1. **均方差损失函数**\n   $$\n   L(y_i,f(x_i)) = (y_i - f(x_i))^2\n   $$\n   其负梯度为：\n   $$\n   y_i - f(x_i)\n   $$\n   p.s. 损失函数为$L(y,f(x))=(y-f(x))^2$,我们需要最小化$J= \\sum_iL(y_i,f(x_i))$通过调整$f(x_1),f(x_2),...,f(x_n)$.我们把$f(x_i)$当成参数并求导\n   $$\n   \\frac{\\partial}{\\partial f(x_i)} = \\frac{\\partial \\sum_iL(y_i,f(x_i))}{\\partial f(x_i)} = \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} = f(x_i) - y_i\n   $$\n   所以，**我们在均方差损失函数下，可以把残差理解成负梯度。**\n   $$\n   y_i-f(x_i) = - \\frac{\\partial}{\\partial f(x_i)}\n   $$\n   \n2. 绝对损失函数：\n   $$\n   L(y_i,f(x_i) = |y_i - f(x_i)|\n   $$\n   其对应的负梯度为：\n   $$\n   sign (y_i - f(x_i))\n   $$\n\n3. Huber损失函数：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：\n   $$\n   \\begin{aligned}\n   L(y_i,f(x_i)) = \n   \\begin{cases} \n   \\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta(|y_i -f(x_i)| - \\frac{\\delta}{2}), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n   其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   y_i-f(x_i), |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n\n4. 分位数损失。它对应的是分位数回归的损失函数，表达式为\n   $$\n   L(x_i,f(x_i)) = \\sum_{y_i \\geq f(x_i)}\\theta|y_i - f(x_i)| + \\sum_{y_i < f(x_i)}(1-\\theta)|y_i - f(x_i)| \n   $$\n   其中，$\\theta$为分位数，需要在回归钱设置，其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   \\theta,  y_i \\geq f(x_i)\\\\\n   \\theta - 1, y_i < f(x_i)\n   \\end{cases}\n   \\end{aligned}\n   $$\n   ![](.\\梯度提升树\\损失函数.png)\n\n#####    1.5 总结及优缺点\n\n本文介绍了boosting族的提升树算法和梯度提升树（GBDT)算法，提升树算法的每轮弱学习器是拟合上一轮的残差生成的，GBDT算法的每轮弱学习器是拟合上一轮损失函数的负梯度生成的。提升树算法和GBDT算法都是用CART回归树作为弱学习器，只要确定模型的损失函数，提升树和GBDT就可以通过前向分布算法进行构建。\n\n\n\n梯度提升树主要的优点有：\n\n1） 可以灵活处理各种类型的数据，包括连续值和离散值。\n\n2）在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。\n\n3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。\n\n梯度提升树的主要缺点有：\n\n1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。\n\n##### 1.6 参考文献\n\n李航 《统计学习》\n\n"},{"title":"逻辑回归分类和softmax分类","url":"/2020/02/12/逻辑回归分类和softmax分类/","content":"### 逻辑回归分类和softmax分类\n\n#### 1.逻辑回归\n\n##### 1.1 算法原理\n\n一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。\n\n逻辑回归模型概率估算:\n$$\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdot x)$$\n\n逻辑函数：\n$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$\n\n预测模型：\n$$\\hat{y}=\n\\begin{cases}\n0 & (\\hat{p}<0.5)\\\\\n1 & (\\hat{p}\\geq0.5)\n\\end{cases}$$\n\n单个训练实例的损失函数:\n$$c(\\theta)=\n\\begin{cases}\n-log(\\hat{p}) & (y=1)\\\\\n-log(1-\\hat{p}) & (y=0)\n\\end{cases}$$\n\n我们可以看到，当$p$接近于$0$的时候，$-\\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。\n\n逻辑回归成本函数:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$$\n\n坏消息是，这个函数没有已知的闭式方程(也就是不尊在一个标准方差的等价方程)。好消息，这是个凸函数，通过梯度下降算法保证能够找出全局最小值。\n\nLogistic损失函数的偏导数:\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^T \\cdot x^{(i)})-y^{(i)})x_j^{(i)}$$\n\n$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^T\\cdot x^{i}-y^{i})x_j^{i}$$\n\n##### 1.2 scala 代码实现\n\n```scala\npackage ml.scrath.classification\n\nimport scala.collection.mutable.ArrayBuffer\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n\n\nobject LogitRegression{\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map{_.split(\",\").filter(_.length() > 0).map(_.toDouble)}\n      .toArray\n    val data = BDM(dataS:_*)\n\n    val features = data(0 to 98, 0 to 3)\n    val labels = data(0 to 98, 4)\n\n    val model = new LogitRegression\n    val w = model.fit(features,labels)\n    val predictions = model.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate)\n  }\n}\n\n\nclass LogitRegression (var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = (x_train * weights).map(sigmoid(_))\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def sigmoid(inX: Double) = {\n    1.0 / (1 + scala.math.exp(-inX))\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = (x_test * weights).map(sigmoid(_)).map(x => if(x > 0.5) 1.0 else 0.0)\n    output\n  }\n\n}\n```\n\n\n\n##### 1.3 python 代码实现\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # 为X增加一列特征x1，x1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # 梯度训练n_iterations轮\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n\n\nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X = data.data[data.target != 0]\n    y = data.target[data.target != 0]\n    y[y == 1] = 0\n    y[y == 2] = 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    Plot().plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n```\n\nPython结果展示如下【基于PCA将高维数据投影而得】：\n\n![](./逻辑回归分类和softmax分类/logistic.png)\n\n#### 2.softmax回归\n\n##### 2.1 算法原理和步骤\n\n对逻辑回归模型做推广，可以支持多个类别了。\n\n原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。\n\n1. 用零（或小的随机值）初始化权重矩阵和偏置值.\n\n2. 对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:\n   $$s_k(x)= x\\cdot w_k $$\n\n   向量化表示上式的话，可以写为\n\n   $$ socres = X \\cdot W $$\n\n   $X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.\n\n3. 应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:\n   $$\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(x))}$$\n\n4.  计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:\n   $$J(W)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}log(\\hat{p}_k^{(i)})$$\n\n5. 对于类别k的交叉熵梯度向量:\n   $$\\Delta_{w_k}J(W)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}$$\n\n6. 更新每个类的权重$W$\n\n   $$w_k = w_k - \\eta \\Delta_{w_k}J$$\n\n​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：\n$$H(p,q)=-\\sum_xp(x)logq(x)$$\n\n\n\n##### 2.2 scala 代码\n\n```scala\npackage ml.scrath.classification\n\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, _}\nimport breeze.numerics._\n\nobject softMax {\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map {\n        _.split(\",\").filter(_.length() > 0).map(_.toDouble)\n      }\n      .toArray\n    val data = BDM(dataS: _*)\n    val features = data(::, 0 to 3)\n    val labels = data(::, 4)\n\n    val soft = new SoftMaxRegression()\n    val w = soft.fit(features, labels)\n    println(w)\n    val predictions = soft.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate) // 正确率为0.9664\n\n  }\n}\n\nclass SoftMaxRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y: BDV[Double]): BDM[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n\n    val ncol = x_train.cols\n    val nclasses = y.toArray.distinct.length\n    var weights = BDM.ones[Double](ncol, nclasses) :* 1.0 / nclasses\n    val n_samples = x_train.rows\n\n    for (iterations <- 0 to num_iters) {\n      val logits = x_train * weights\n      val probs = softmax(logits)\n      val y_one_hot = one_hot(y)\n//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble\n      val error: BDM[Double] = probs - y_one_hot\n      val gradients = (x_train.t * error) :/ n_samples.toDouble\n\n      weights -= gradients :* lr\n    }\n    weights\n  }\n\n  def softmax(logits: BDM[Double]): BDM[Double] = {\n    val scores = exp(logits)\n    val divisor = sum(scores(*, ::))\n    for (i <- 0 to scores.cols - 1) {\n      scores(::, i) := scores(::, i) :/ divisor\n    }\n    scores\n  }\n\n  def one_hot(y: BDV[Double]): BDM[Double] = {\n    val n_samples = y.length\n    val n_classes = y.toArray.toSet.size\n    val one_hot = Array.ofDim[Double](n_samples, n_classes)\n    for (i <- 0 to n_samples - 1) {\n      one_hot(i)(y(i).toInt) = 1.0\n    }\n    BDM(one_hot: _*)\n  }\n\n  def predict(weights: BDM[Double], x: BDM[Double]): BDV[Int] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_test = BDM.horzcat(ones, x)\n    val predictions = argmax(x_test * weights, Axis._1)\n    predictions\n  }\n\n}\n```\n\n\n\n##### 2.3 python 代码\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 12 11:58:06 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\nclass SoftmaxRegressorII:\n\n    def __init__(self,learning_rate = 0.1,n_iters = 1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n\n    def train(self, X, y_true, n_classes):\n\n        x_train = np.column_stack((np.ones(len(X)),X))\n        \n        self.n_samples, n_features = x_train.shape\n        self.n_classes = n_classes\n        \n        self.weights = np.random.rand(n_features,self.n_classes)\n        all_losses = []\n        \n        for i in range(self.n_iters):\n            logits = np.dot(x_train, self.weights)\n            probs = self.softmax(logits)\n            y_one_hot = self.one_hot(y_true)\n            loss = self.cross_entropy(y_one_hot, probs)\n            all_losses.append(loss)\n\n            gradients = (1 / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))\n\n            self.weights = self.weights - self.learning_rate * gradients\n\n#            if i % 100 == 0:\n#                print(f'Iteration number: {i}, loss: {np.round(loss, 4)}')\n\n        return self.weights, all_losses\n\n    def predict(self, X):\n\n        x_test = np.column_stack((np.ones(len(X)), X))\n        scores = np.dot(x_test, self.weights)\n        probs = self.softmax(scores)\n        return np.argmax(probs, axis=1)[:, np.newaxis]\n\n    def softmax(self, logits):\n        exp = np.exp(logits)\n        sum_exp = np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        return exp / sum_exp\n\n    def cross_entropy(self, y_true, scores):\n        loss = - (1 / self.n_samples) * np.sum(y_true * np.log(scores))\n        return loss\n\n    def one_hot(self, y):\n        one_hot = np.zeros((self.n_samples, self.n_classes))\n        one_hot[np.arange(self.n_samples), y.T] = 1\n        return one_hot\n    \nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X= data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = SoftmaxRegressorII()\n    ll = clf.train(X_train, y_train,3)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Reduce dimension to two using PCA and plot the results\n    Plot().plot_in_2d(X_test, y_pred, title=\"SoftMax Regression\", accuracy=accuracy)\n```\n\n结果如图所示：\n\n![](./逻辑回归分类和softmax分类/softmax.png)"},{"title":"降维_线性判别分析","url":"/2020/01/16/降维_线性判别分析/","content":"#### 1. 算法概述\n\nLDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”，如下图所示。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。\n\n![](./降维_线性判别分析/lda_1.png)\n\n#### 2. 算法推导\n\nLDA多分类：假定存在$N$个类，且第$i$类示例树为$m_i$,我们定义全局散度矩阵：\n$$\nS_t = S_b + S_w= \\sum_{i= 1}^{m}(x_i - \\mu)(x_i - \\mu)^T\n$$\n其中$\\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即\n$$\nS_w = \\sum_{i=1}^{N}S_{w_i}\n$$\n其中\n$$\nS_{w_i} = \\sum_{x \\in X_i}(x - \\mu_i)(x - \\mu_i)^T\n$$\n由式$(1)-(3)$可得：\n$$\nS_b = S_t - S_w = \\sum_{i=1}^{N}(\\mu_i - \\mu)(\\mu_i - \\mu)^T\n$$\n多分类LDA有多种实现方式：使用$S_b,S_w,S_t$中的任意两即可。常见的一种实现是采用优化目标\n$$\n\\max_W \\dfrac{tr(W^TS_bW)}{tr(W^{T}S_wW)}\n$$\n其中$W \\in R^{d\\times(N-1)}$,$tr(.)$表示矩阵的迹。式$(5)$可以通过求解如下式的广义特征问题\n$$\nS_bW = \\lambda S_wW\n$$\n$W$的闭式解则是$S_w^{-1}S_b$的N-1个广义特征值所对应的特征向量组成的矩阵。\n\n​\t\t若将$W$视为一个投影矩阵，则多分类LDA将样本矩阵投影到$N-1$维空间。$N-1$通常原小于数据原来的维数，且投影过程中使用了类别信息，因此LDA也被视为一种数据降维的技术。\n\n#### 3. 实现步骤\n\n1. 对于每一类别，计算$d$维数据的均值向量；\n2. 构造类间散度矩阵$S_b$和类内散度矩阵$S_w$;\n3. 计算矩阵$S_w^{-1}S_b$的特征值及对应的特征向量；\n4. 选取前$k$特征值所对应的特征向量，构造$d \\times k$维的转换矩阵$W$,其中特征值以列的形式排列；\n5. 使用转换矩阵$W$将样本映射到新的特征子空间上。\n\n#### 4. LDA与PCA\n\nLDA和PCA都可以用作降维技术，下面比较一下相同点和不同点：\n\n##### 4.1 相同点\n\n​\t1）两者均可以对数据进行降维;\n\n​\t2）两者在降维时均使用了矩阵特征分解的思想;\n\n​\t3）两者都假设数据符合高斯分布.\n\n##### 4.2 不同点\n\n​\t1） LDA是有监督的降维技术，PCA是无监督的降维技术；\n\n​\t2） LDA还可以用于分类，后续在分类时，贴上分类的处理方法；\n\n​\t3） LDA最多可降低到$k$维（$k$是分类的个数),而PCA最多可降低到$n-1$维（$n$是数据的维数)；\n\n​\t4） LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。\n\n#### 5.  LDA的实现代码\n\n##### 5.1 Scala lda实现\n\n```scala\npackage ml.scrath.lda\n\nimport breeze.linalg._\nimport breeze.stats._\nimport org.apache.spark.rdd.RDD\n\nclass LinearDiscriminantAnalysis extends Serializable {\n\n  def fit(data: RDD[DenseVector[Double]], labels: RDD[Int],k:Int) = {\n    val sample = labels.zip(data)\n    computeLDA(sample,k)\n  }\n\n  def computeLDA(dataAndLabels: RDD[(Int, DenseVector[Double])],k:Int)= {\n\n    val featuresByClass = dataAndLabels.groupBy(_._1).values.map(x => rowsToMatrix(x.map(_._2)))\n    val meanByClass = featuresByClass.map(f => mean(f(::, *))) // 对行向量求平均值 each mean is a row vector, not col\n\n    //类内散度矩阵\n    val Sw = featuresByClass.zip(meanByClass).map(f => {\n      val featuresMinusMean: DenseMatrix[Double] = f._1(*, ::) - f._2.t // row vector, not column\n      featuresMinusMean.t * featuresMinusMean: DenseMatrix[Double]\n    }).reduce(_+_)\n\n    val numByClass = featuresByClass.map(_.rows : Double)\n    val features = rddToMatrix(dataAndLabels.map(_._2))\n    val totalMean = mean(features(::, *)) // A row-vector, not a column-vector\n\n    val Sb = meanByClass.zip(numByClass).map {\n      case (classMean, classNum) => {\n        val m = classMean - totalMean\n        (m.t * m : DenseMatrix[Double]) :* classNum : DenseMatrix[Double]\n      }\n    }.reduce(_+_)\n\n    val eigen = eig((inv(Sw): DenseMatrix[Double]) * Sb)\n\n    val eigenvectors = (0 until eigen.eigenvectors.cols).map(eigen.eigenvectors(::, _).toDenseMatrix.t)\n\n    val topEigenvectors = eigenvectors.zip(eigen.eigenvalues.toArray).sortBy(x => -scala.math.abs(x._2)).map(_._1).take(k)\n    val W = DenseMatrix.horzcat(topEigenvectors:_*)\n    (W,Sb,Sw)\n  }\n\n  def rowsToMatrix(in: TraversableOnce[DenseVector[Double]]): DenseMatrix[Double] = {\n    rowsToMatrix(in.toArray)\n  }\n\n  def rowsToMatrix(inArr: Array[DenseVector[Double]]): DenseMatrix[Double] = {\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n\n\n  def rddToMatrix(inArr1: RDD[DenseVector[Double]]): DenseMatrix[Double] = {\n    val inArr = inArr1.collect()\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n}\n```\n\n##### 5.2 scala 测试代码\n\n测试代码（其中数据iris.csv是由下面python代码生成）\n\n```scala\npackage ml.scrath.lda\n\nimport org.apache.spark.sql.SparkSession\nimport breeze.linalg.DenseVector\n\nobject TestLDA extends App {\n\n  val spark =\n    SparkSession.builder()\n      .appName(\"DataFrame-Basic\")\n      .master(\"local[4]\")\n      .config(\"spark.sql.warehouse.dir\", \"file:///E:/spark-warehouse\")\n      .getOrCreate()\n  val sc = spark.sparkContext\n  val irisData = sc.textFile(\"D:\\\\data\\\\iris.csv\")\n\n  val trainData = irisData.map {\n    _.split(\",\").dropRight(1).map(_.toDouble)\n  }.map(new DenseVector(_))\n\n  val labels = irisData.map {\n    _.split(\",\").apply(4).map(_.toInt).apply(0)\n  }\n\n  val start = System.currentTimeMillis()\n  val model = new LinearDiscriminantAnalysis\n  val k = 2\n  val res = model.fit(trainData, labels, k)\n\n  println(\"=====W======\")\n  println(res._1)\n  println(\"=======Sb====\")\n  println(res._2)\n  println(\"=======Sw====\")\n  println(res._3)\n\n}\n```\n\n##### 5.3 scala 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$\n\n![](./降维_线性判别分析/scala_lda_res.png)\n\n##### 5.4 Python lda 代码\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef LDA(X, y, k):\n    '''\n    X为数据集，y为label，k为目标维数\n    '''\n    label_ = np.unique(y)\n    mu = np.mean(X, axis=0)\n\n    Sw = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sw += np.dot((_X - _mean).T,\n                     _X - _mean)\n\n    print(Sw)\n    \n    Sb = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sb += len(_X) * np.dot(( _mean - mu).reshape(\n            (len(mu), 1)), (_mean - mu).reshape((1, len(mu))))\n        \n    print(Sb)\n\n    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # 计算Sw-1*Sb的特征值和特征矩阵\n\n    sorted_indices = np.argsort(eig_vals)\n    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]]  # 提取前k个特征向量\n    return topk_eig_vecs,Sb,Sw\n\n\nif '__main__' == __name__:\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n## 将数据写出到iris.csv文件，供scala使用，保证数据的一致性。\n    data_cat =  np.c_[X,y]\n    import pandas\n    iris_df = pandas.DataFrame(data_cat)\n    iris_df.to_csv(\"iris.csv\",index = 0,header = False)\n\n    W,Sb,Sw = LDA(X, y, 2)\n    X_new = np.dot((X), W)\n    plt.figure(1)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    \n    print(\"========W=========\")\n    print(W)\n    print(\"========Sb========\")\n    print(Sb)\n    print(\"========Sw========\")\n    print(Sw)\n    \n    \n    # 与sklearn中的LDA函数对比\n    lda = LinearDiscriminantAnalysis(n_components=2)\n    lda.fit(X, y)\n    X_new = lda.transform(X)\n#    print(X_new)\n    plt.figure(2)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    plt.show()\n\n```\n\n##### 5.5 Python 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$,可见scala和python得到结果是一致的。\n\n![](./降维_线性判别分析/python_lda_res.png)\n\n"},{"title":"线性回归","url":"/2020/01/09/线性回归/","content":"### 线性回归的代码实现\n\n#### 1. 原理\n\n多元线性回归的损失函数为：\n$$\nJ=\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2\n$$\n其中：$\\hat{y}^{(i)} = \\theta_{0} + \\theta_{1}X_{1}^{(i)} + \\theta_{2}X_{2}^{(i)} + ... + \\theta_{n}X_{n}^{(i)}$ 。\n\n对 $J$ 求导为：\n$$\n\\nabla J=(\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})\n$$\n其中：$\\frac{\\partial J}{\\partial \\theta_i}$ 为偏导数，与导数的求法一样。\n\n\n\n对 $\\nabla J$ 进一步计算：\n$$\n\\nabla J(\\theta) =  \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} =   \\begin{pmatrix} \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-1) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_1^{(i)}) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_2^{(i)}) \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_n^{(i)}) \\end{pmatrix} = 2·\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n\n其中：$X_b = \\begin{pmatrix}\n1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n1 & X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)}\n\\end{pmatrix}$\n\n\n\n​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：\n$$\n\\nabla J(\\theta)  = \\frac{2}{m}\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n​        此时，目标函数就成了使 $\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：\n$$\nJ(\\theta) = MSE(y, \\hat{y})\n$$\n​\t\t\n\n注1. 有时候目标函数也去 $\\frac{1}{\\boldsymbol{2}m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ ,其中的**2**是为了抵消梯度中的2，实际的效果有限。\n\n注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。\n\n\n\n#### 2.实现代码\n\n##### 2.1 python版本实现\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  8 17:25:39 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\n\nclass LinearRegression():\n    def __init__(self,lr=.01, num_iters=10000,tolerance=1e-8):\n        self.lr = lr\n        self.num_iters = num_iters\n        self.w = None\n        self.toterance = tolerance\n\n    def fit(self,x_train, y_train):\n        n_samples = len(x_train)\n        x_train = np.column_stack((np.ones(n_samples), x_train)) #也可以写成np.c_\n        \n        self.w = .01 * np.ones(x_train.shape[1])\n        self.loss_ = [0]\n        \n        # w_{i} = w_{i} - lr * (h(w_{i}) - y)*x_{i} 迭代公式\n        self.count = 0\n        for iteration in range(self.num_iters):\n            self.count += 1\n            raw_output = np.matmul(x_train, self.w)\n            errors =  raw_output - y_train\n            loss = 1/(2 * n_samples) * errors.dot(errors)\n            delta_loss = loss - self.loss_[-1]\n\n            self.loss_.append(loss)\n            if np.abs(delta_loss) < self.toterance:\n                break\n            else:\n                grad = (1.0 /n_samples) *np.matmul(x_train.T, np.array(errors))\n                self.w -= self.lr * grad\n\n    def predict(self,x_test):\n        x_test = np.column_stack((np.ones(len(x_test)), x_test))\n        \n        output = np.matmul(x_test, self.w)\n        return output\n    \nif __name__ == '__main__':\n    \n    num_inputs = 2\n    num_examples = 10000\n    true_w = [6.4, -3.2]\n    true_b = 2.3\n    features = np.random.random((num_examples, num_inputs))\n    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n    labels += np.random.normal(0, 0.1,size = len(labels))\n    \n    import sklearn.model_selection\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    print(\"回归系数:\",lr.w)\n    print(\"迭代次数:\",lr.count)\n    \n    y_pred = lr.predict(x_test)\n    from sklearn import metrics\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    print(\"MSE: %.4f\" % mse)\n\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    print(\"MAE: %.4f\" % mae)\n\n    R2 = metrics.r2_score(y_test,y_pred)\n    print(\"R2: %.4f\" % R2)\n```\n\n##### 2.2 Scala版本实现\n\n```scala\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n * Scala 版本的实现\n */\n\nobject LinearRegression {\n  def main(args: Array[String]): Unit = {\n    val num_inputs = 2\n    val num_examples = 1000\n    val x_train: BDM[Double] = BDM.rand(num_examples, num_inputs)\n    val ones = BDM.ones[Double](num_examples, 1)\n    val x_cat = BDM.horzcat(ones, x_train)\n    val y_train = x_cat * BDV(2.3, 6.4, -3.2)\n\n    val model = new LinearRegression(num_iters = 10000)\n    val weights = model.fit(x_train, y_train)\n    val predictions = model.predict(weights, x_train)\n    println(\"梯度下降求解的权重为：\" + weights)\n    println(predictions)\n  }\n}\n\nclass LinearRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = x_train * weights\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = x_test * weights\n    output\n  }\n}\n```\n\n参考文献：1. [机器学习之梯度下降法与线性回归](https://segmentfault.com/a/1190000017048213)\n\n\n\n"},{"title":"神经网络","url":"/2019/11/27/神经网络-1/","content":"\n### 神经网络的numpy实现和公式推导\n\n\n\n\n\n​       过去10多年是神经网络发展的黄金时期，神经网络(深度学习)成为了新时代的一种浪潮，所以今天借用国外一个小哥实现纯numpy的神经网络，来记录神经网络的实现过程。\n\n![](./神经网络-1/nn_architecture.png)\n\n\n\n#### 概览\n\n​        在开始编程之前，先让我们准备一份基本的路线图。我们的目标是创建一个特定架构（层数、层大小、激活函数）的密集连接神经网络。然后训练这一神经网络并做出预测。\n\n![](./神经网络-1/blueprint.gif)\n\n上面的示意图展示了训练网络【特别是正向传播和反向传播的操作】时进行的操作，以及单次迭代不同阶段需要更新和读取的参数。\n\n#### 初始化神经网络层\n\n  让我们从初始化每一层的权重矩阵$W$和偏置向量$b$开始。下图展示了网络层l的权重矩阵和偏置向量，其中，上标$[l]$表示当前层的索引，$n$表示给定层中的神经元数量。\n\n![](./神经网络-1/params_sizes.png)\n\n我们的程序也将以类似的列表形式描述神经网络架构。列表的每一项是一个字典，描述单个网络层的基本参数：`input_dim`是网络层输入的信号向量的大小，`output_dim`是网络层输出的激活向量的大小，`activation`是网络层所用的激活函数。\n\n```python\nnn_architecture = [\n    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n]\n```\n\n值得注意的是，一个网络层的输出向量同时也是下一层的输入。\n\n```python\ndef init_layers(nn_architecture, seed = 99):\n    np.random.seed(seed)\n    number_of_layers = len(nn_architecture)\n    params_values = {}\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        layer_input_size = layer[\"input_dim\"]\n        layer_output_size = layer[\"output_dim\"]\n\n        params_values['W' + str(layer_idx)] = np.random.randn(\n            layer_output_size, layer_input_size) * 0.1\n        params_values['b' + str(layer_idx)] = np.random.randn(\n            layer_output_size, 1) * 0.1\n\n    return params_values\n```\n\n​        上面的代码初始化了网络层的参数。注意我们用随机的小数字填充矩阵**W**和向量**b**。这并不是偶然的。权重值无法使用相同的数字初始化，否则会造成**破坏性的对称问题**。**基本上，如果权重都一样，不管输入X是什么，隐藏层的所有单元也都一样**。这样，我们就会陷入初始状态，不管训练多久，网络多深，都无望摆脱。线性代数不会原谅我们。\n\n[^初始化方法包含很多种，我们这里简便起见，使用随机初始化的方式生成权重]: \n\n​       小数值增加了算法的效率。我们可以看看下面的sigmoid函数图像，大数值处的函数图像几乎是扁平的，这会对神经网络的学习速度造成显著影响。所有参数使用小随机数是一个简单的方法，但它保证了算法有一个**足够好**的开始。\n\n![](./神经网络-1/activations.gif)\n\n#### 激活函数\n\n​        激活函数只需一行代码就可以定义，但它们给神经网络带来了非线性和所需的表达力。**“没有它们，神经网络将变成线性函数的组合，也就是单个线性函数。”**激活函数有很多种，但在这个项目中，我决定使用其中两种——sigmoid和ReLU。为了同时支持前向传播和反向传播，我们还需要准备好它们的导数。\n\n```python\ndef sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0,Z)\n\ndef sigmoid_backward(dA, Z):\n    sig = sigmoid(Z)\n    return dA * sig * (1 - sig)\n\ndef relu_backward(dA, Z):\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0;\n    return dZ;\n```\n\n#### 前向传播\n\n​    我们设计的神经网络有一个简单的架构。输入矩阵**X**传入网络，沿着隐藏单元传播，最终得到预测向量**Y_hat**。为了让代码更易读，我将前向传播拆分成两个函数——单层前向传播，和整个神经网络前向传播。\n\n​\t    **前向传播的过程是：输入$a^{[l-1]}$, 输出$a^{[l]}$, 缓存为$z^{[l]}$,  从方便实现的角度上看，$z^{[l]}$是$w^{[l]}$，$b^{[l]}$的函数。**\n\n```python\ndef single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n    Z_curr = np.dot(W_curr, A_prev) + b_curr\n    if activation is \"relu\":\n        activation_func = relu\n    elif activation is \"sigmoid\":\n        activation_func = sigmoid\n    else:\n        raise Exception('Non-supported activation function')\n\n    return activation_func(Z_curr), Z_curr\n```\n这部分代码大概是最直接，最容易理解的。给定来自上一层的输入信号，我们计算仿射变换**Z**，接着应用选中的激活函数。基于NumPy，我们可以对整个网络层和整批样本一下子进行矩阵操作，无需迭代，这大大加速了计算。除了计算结果外，函数还返回了一个反向传播时需要用到的中间值**Z**。\n\n![](./神经网络-1/matrix_sizes_2.png)\n\n基于单层前向传播函数，编写整个前向传播步骤很容易。这是一个略微复杂一点的函数，它的角色不仅是进行预测，还包括组织中间值。\n$$\n\\begin{aligned}\nz^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}  \\\\\n\na^{[l]} = g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n\n\n```python\ndef full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n\n        activ_function_curr = layer[\"activation\"]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        b_curr = params_values[\"b\" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\n        memory[\"A\" + str(idx)] = A_prev\n        memory[\"Z\" + str(layer_idx)] = Z_curr\n\n    return A_curr, memory\n```\n\n#### 损失函数\n\n​          损失函数可以监测进展，确保我们向着正确的方向移动。**“一般来说，损失函数是为了显示我们离‘理想’解答还有多远。”**损失函数根据我们计划解决的问题而选用，Keras之类的框架提供了很多选项。因为我计划将神经网络用于二元分类问题，我决定使用交叉熵：\n$$\nJ(W,b) = \\dfrac{1}{m}{\\sum}_{i=1}^{m}L(\\hat{y}^{i} - y^{i})  \\\\\nL(\\hat{y} - y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))\n$$\n为了取得更多关于学习过程的信息，我决定另外实现一个计算精确度的函数。\n\n```python\n'''\nJ(W,b) = 1/m sum_{i}^{m}L(y^{hat}_{i} - y_{i})\nL(y^{hat}_{i} - y_{i}) = -(ylogy^{hat} + (1-y)log(1-y^{hat}))\n'''\n\ndef get_cost_value(Y_hat, Y):\n    m = Y_hat.shape[1]\n    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n    return np.squeeze(cost)\n\n# an auxiliary function that converts probability into class\ndef convert_prob_into_class(probs):\n    probs_ = np.copy(probs)\n    probs_[probs_ > 0.5] = 1\n    probs_[probs_ <= 0.5] = 0\n    return probs_\n\ndef get_accuracy_value(Y_hat, Y):\n    Y_hat_ = convert_prob_into_class(Y_hat)\n    return (Y_hat_ == Y).all(axis=0).mean()\n```\n\n#### 反向传播\n\n不幸的是，很多缺乏经验的深度学习爱好者都觉得反向传播很吓人，难以理解。微积分和线性代数的组合经常会吓退那些没有经过扎实的数学训练的人。所以不要过于担心你现在还不能理解这一切。相信我，我们都经历过这个过程。\n\n```python\ndef single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n    # number of examples\n    m = A_prev.shape[1]\n    \n    # selection of activation function\n    if activation is \"relu\":\n        backward_activation_func = relu_backward\n    elif activation is \"sigmoid\":\n        backward_activation_func = sigmoid_backward\n    else:\n        raise Exception('Non-supported activation function')\n    \n    # calculation of the activation function derivative\n    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n    \n    # derivative of the matrix W\n    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n    # derivative of the vector b\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n    # derivative of the matrix A_prev\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n\n    return dA_prev, dW_curr, db_curr\n\n```\n\n​        人们经常搞混反向传播和梯度下降，但事实上它们不一样。前者是为了高效地计算梯度，后者则是为了基于计算出的梯度进行优化。在神经网络中，我们计算损失函数在参数上的梯度，但反向传播可以用来计算任何函数的导数。**反向传播算法的精髓在于递归地使用求导的链式法则，通过组合导数已知的函数，计算函数的导数**。下面的公式描述了单个网络层上的反向传播过程。由于本文的重点在实际实现，所以我将省略求导过程。从公式上我们可以很明显地看到，为什么我们需要在前向传播时记住中间层的**A**、**Z**矩阵的值。\n\n$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} =\\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}} \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{W}^{[l]}} =  \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n\n$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}}  = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}   \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{b}^{[l]}}= \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n\n$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} =  \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}  \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n\n$$\\boldsymbol{dZ}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}= \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l]}} \\frac{\\partial {A}^{[l]} }{\\partial \\boldsymbol{Z}^{[l]}}=\\boldsymbol{dA}^{[l]} * g^{[l]}{'}(\\boldsymbol{Z}^{[l]})$$\n\n![](./神经网络-1/640.webp)\n\n和前向传播一样，我决定将计算拆分成两个函数。之前给出的是单个网络层的反向传播函数，基本上就是以NumPy方式重写上面的数学公式。而定义完整反向传播过程的函数，主要是读取、更新三个字典中的值。\n\n```python\ndef full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n    grads_values = {}\n    \n    # number of examples\n    m = Y.shape[1]\n    # a hack ensuring the same shape of the prediction vector and labels vector\n    Y = Y.reshape(Y_hat.shape)\n    \n    # initiation of gradient descent algorithm\n    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n    \n    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n        # we number network layers from 1\n        layer_idx_curr = layer_idx_prev + 1\n        # extraction of the activation function for the current layer\n        activ_function_curr = layer[\"activation\"]\n        \n        dA_curr = dA_prev\n        \n        A_prev = memory[\"A\" + str(layer_idx_prev)]\n        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n        \n        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n        \n        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n    \n    return grads_values\n```\n\n基于单个网络层的反向传播函数，我们从最后一层开始迭代计算所有参数上的导数，并最终返回包含所需梯度的python字典。\n\n\n\n#### 更新参数值\n\n反向传播是为了计算梯度，以根据梯度进行优化，更新网络的参数值。为了完成这一任务，我们将使用两个字典作为函数参数：`params_values`，其中保存了当前参数值；`grads_values`，其中保存了用于更新参数值所需的梯度信息。现在我们只需在每个网络层上应用以下等式即可。这是一个非常简单的优化算法，但我决定使用它作为更高级的优化算法的起点（大概会是我下一篇文章的主题）。\n\n$$\\boldsymbol{W}^{[l]} = \\boldsymbol{W}^{[l]} - \\alpha \\boldsymbol{dW}^{[l]} $$\n\n$$\\boldsymbol{b}^{[l]} = \\boldsymbol{b}^{[l]} - \\alpha \\boldsymbol{b}^{[l]} $$\n\n```\ndef update(params_values, grads_values, nn_architecture, learning_rate):\n\n    # iteration over network layers\n    for layer_idx, layer in enumerate(nn_architecture, 1):\n        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n\n    return params_values;\n```\n\n#### 整合一切\n\n万事俱备只欠东风。最困难的部分已经完成了——我们已经准备好了所需的函数，现在只需以正确的顺序把它们放到一起。\n\n```python\ndef train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n    # initiation of neural net parameters\n    params_values = init_layers(nn_architecture, 2)\n    # initiation of lists storing the history \n    # of metrics calculated during the learning process \n    cost_history = []\n    accuracy_history = []\n    \n    # performing calculations for subsequent iterations\n    for i in range(epochs):\n        # step forward\n        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n        \n        # calculating metrics and saving them in history\n        cost = get_cost_value(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        # step backward - calculating gradient\n        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n        # updating model state\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        if(i % 50 == 0):\n            if(verbose):\n                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n            if(callback is not None):\n                callback(i, params_values)\n            \n    return params_values\n```\n\n\n"}]