[{"title":"深度学习面试","url":"/2021/04/06/深度学习面试/","content":"\n\n# 深度学习\n\n## 神经网络中的Epoch、Iteration、Batchsize\n\n神经网络中epoch与iteration是不相等的\n\n- batchsize：中文翻译为批大小（批尺寸）。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；\n\n- iteration：中文翻译为迭代，1个iteration等于使用batchsize个样本训练一次；一个迭代 = 一个正向通过+一个反向通过\n\n- epoch：迭代次数，1个epoch等于使用训练集中的全部样本训练一次；一个epoch = 所有训练样本的一个正向传递和一个反向传递\n\n举个例子，训练集有1000个样本，batchsize=10，那么：训练完整个样本集需要：100次iteration，1次epoch。\n\n![img](https://gss0.baidu.com/-vo3dSag_xI4khGko9WTAnF6hhy/zhidao/wh%3D600%2C800/sign=36204981f1039245a1e0e909b7a488fa/e61190ef76c6a7ef3bf5176af0faaf51f2de66af.jpg)\n\n**参考资料**\n\n- [神经网络中的Epoch、Iteration、Batchsize](https://zhuanlan.zhihu.com/p/67414365)\n- [神经网络中epoch与iteration相等吗](https://zhidao.baidu.com/question/716300338908227765.html)\n\n## 反向传播（BP）\n\n详见：[神经网络公式推导](https://stringsli.github.io/2020/06/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/)\n\n**参考资料**\n\n- [一文搞懂反向传播算法](https://www.jianshu.com/p/964345dddb70)\n\n## CNN本质和优势\n\n- 局部卷积（提取局部特征）\n\n- 权值共享（减少参数数量，因此降低训练难度)\n\n- Pooling（降维，将低层次组合为高层次的特征）\n\n- 多层次结构(将低层次的局部特征组合成为较高层次的特征。不同层级的特征可以对应不同任务。)\n\n## 神经网络数据预处理方法有哪些？\n\n- 零均值 \n\n  ```python\n  X -= np.mean(X,axis = 0)\n  ```\n\n- 归一化（normalization）\n\n  ```python\n  X -= np.mean(X,axis = 0)\n   \n  X /= np.std(X,axis = 0)\n  ```\n\n  \n\n## 神经网络怎样进行参数初始化？\n\n- **全零初始化和随机初始化**\n\n  如果神经元的权重被初始化为0， 在第一次更新的时候，除了输出之外，所有的中间层的节点的值都为零。一般神经网络拥有对称的结构，那么在进行第一次误差反向传播时，更新后的网络参数将会相同，在下一次更新时，相同的网络参数学习提取不到有用的特征，因此深度学习模型都不会使用0初始化所有参数。\n  而随机初始化就是搞一些很小的值进行初始化，实验表明大了就容易饱和，小的就激活不动，再说了这个没技术含量，不必再讨论。\n\n- **标准初始化**\n\n- **Xavier初始化**\n\n## 卷积\n\n对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512803638_521.png)\n\n**参考资料**\n\n- [Feature Extraction Using Convolution](http://ufldl.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/)\n- [convolution](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html)\n\n- [理解图像卷积操作的意义](https://blog.csdn.net/chaipp0607/article/details/72236892?locationNum=9&fps=1)\n\n- [关于深度学习中卷积核操作](https://www.cnblogs.com/Yu-FeiFei/p/6800519.html)\n\n### 卷积的反向传播过程\n\n- [ ] TODO\n\n**参考资料**\n\n- [Notes on Convolutional Neural Network](http://cogprints.org/5869/1/cnn_tutorial.pdf)\n- [Deep Learning论文笔记之（四）CNN卷积神经网络推导和实现](https://blog.csdn.net/zouxy09/article/details/9993371)\n\n- [反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)\n\n- [Deep learning：五十一(CNN的反向求导及练习)](https://www.cnblogs.com/tornadomeet/p/3468450.html)\n\n- [卷积神经网络(CNN)反向传播算法](https://www.cnblogs.com/pinard/p/6494810.html)\n\n- [卷积神经网络(CNN)反向传播算法公式详细推导](https://blog.csdn.net/walegahaha/article/details/51945421)\n\n- [全连接神经网络中反向传播算法数学推导](https://zhuanlan.zhihu.com/p/61863634)\n\n- [卷积神经网络(CNN)反向传播算法推导](https://zhuanlan.zhihu.com/p/61898234)\n\n\n- https://github.com/Lyken17/pytorch-OpCounter)\n\n## 池化（Pooling）\n\n**平均池化（Mean Pooling）**\n\nmean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，还是比较理解的，图示如下 \n\n![](https://img-blog.csdn.net/20170615205352655?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n**最大池化（Max Pooling）**\n\nmax pooling也要满足梯度之和不变的原则，max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id，这个可以看caffe源码的pooling_layer.cpp，下面是caffe框架max pooling部分的源码\n\n```python\n\n// If max pooling, we will initialize the vector index part.\n\nif (this->layer_param_.pooling_param().pool() == PoolingParameter_PoolMethod_MAX && top.size() == 1)\n\n{max_idx_.Reshape(bottom[0]->num(), channels_, pooled_height_,pooled_width_); }\n\n```\n\n![](https://img-blog.csdn.net/20170615211413093?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMjExOTAwODE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)\n\n\n**参考资料**\n\n- [如何理解CNN中的池化？](https://zhuanlan.zhihu.com/p/35769417)\n- [深度学习笔记（3）——CNN中一些特殊环节的反向传播](https://blog.csdn.net/qq_21190081/article/details/72871704)\n\n### 平均池化（average pooling）\n\n- [ ] TODO\n\n### 最大池化（max pooling）\n\n- [ ] TODO\n\n## 正则化方法\n\n1. [参数范数惩罚](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#1)\n2. [L2参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#2)\n3. [L1参数正则化](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#3)\n4. [L1正则化和L2正则化的区别](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#4)\n5.  [数据集增强](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#5)\n6.  [噪音的鲁棒性](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#6)\n7. [向输出目标注入噪声](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#7)\n8.  [半监督学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#8)\n9.  [多任务学习](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#9)\n10. [提前终止](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#10)\n11. [参数绑定和共享](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#11)\n12. [稀疏表示](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#12)\n13. [集成化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin#13)\n\n**参考资料**\n\n- [ ] [正则化方法](https://baike.baidu.com/item/%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95/19145625?fr=aladdin)\n\n- https://blog.csdn.net/liuxiao214/article/details/81037416)\n\n## 优化算法\n\n- 随机梯度下降（SGD）\n- Mini-Batch\n- 动量（Momentum）\n- Nesterov 动量\n- AdaGrad\n- AdaDelta\n- RMSProp\n- Adam\n- Adamax\n- Nadam\n- [AMSGrad](http://ruder.io/optimizing-gradient-descent/index.html#amsgrad)\n- AdaBound\n\n**参考资料**\n\n- [《Deep Learning》第八章：深度模型中的优化](https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/)\n\n- [从 SGD 到 Adam —— 深度学习优化算法概览(一)](https://zhuanlan.zhihu.com/p/32626442)\n- [Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)](https://zhuanlan.zhihu.com/p/37269222)\n- [An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)\n\n### 梯度下降法\n\n- [ ] TODO\n\n### mini-batch梯度下降法\n\n- [ ] TODO\n\n### 随机梯度下降法（SGD）\n\n#### SGD每步做什么，为什么能online learning？\n\n- [ ] TODO\n\n### 动量梯度下降法（Momentum）\n\n- [ ] TODO\n\n**参考资料**\n\n- [简述动量Momentum梯度下降](https://blog.csdn.net/yinruiyang94/article/details/77944338)\n\n### RMSprop\n\n$$\nS_{dW}=\\beta S_{dW}+\\left ( 1-\\beta  \\right )dW^{2}\n$$\n\n$$\nS_{db}=\\beta S_{db}+\\left ( 1-\\beta  \\right )db^{2}\n$$\n\n$$\nW=W-\\alpha\\frac{dW}{\\sqrt{S_{dW}}}, b=b-\\alpha\\frac{db}{\\sqrt{S_{db}}}\n$$\n\n- [ ] TODO\n\n### Adagrad\n\n- [ ] TODO\n\n### Adam\n\nAdam算法结合了Momentum和RMSprop梯度下降法，是一种极其常见的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。\n$$\nv_{dW}=\\beta_{1} v_{dW}+\\left ( 1-\\beta_{1}  \\right )dW\n$$\n\n$$\nv_{db}=\\beta_{1} v_{db}+\\left ( 1-\\beta_{1}  \\right )db\n$$\n\n$$\nS_{dW}=\\beta_{2} S_{dW}+\\left ( 1-\\beta_{2}  \\right )dW^{2}\n$$\n\n$$\nS_{db}=\\beta_{2} S_{db}+\\left ( 1-\\beta_{2}  \\right )db^{2}\n$$\n\n$$\nv_{dW}^{corrected}=\\frac{v_{dW}}{1-\\beta_{1}^{t}}\n$$\n\n$$\nv_{db}^{corrected}=\\frac{v_{db}}{1-\\beta_{1}^{t}}\n$$\n\n$$\nS_{dW}^{corrected}=\\frac{S_{dW}}{1-\\beta_{2}^{t}}\n$$\n\n$$\nS_{db}^{corrected}=\\frac{S_{db}}{1-\\beta_{2}^{t}}\n$$\n\n$$\nW:=W-\\frac{av_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected}}+\\varepsilon }\n$$\n\n超参数：\n$$\n\\alpha ,\\beta _{1},\\beta_{2},\\varepsilon\n$$\n\n$$\n\\alpha ,\\beta _{1},\\beta_{2},\\varepsilon\n$$\n- [ ] TODO \n\n#### Adam 优化器的迭代公式\n\n- [ ] TODO\n\n## 激活函数\n\n| 激活函数   | 公式                                     | 缺点                                                   | 优点                                      |\n| ---------- | ---------------------------------------- | ------------------------------------------------------ | ----------------------------------------- |\n| Sigmoid    | $σ(x)=1/(1+e^{−x})$                      | 1、会有梯度弥散 2、不是关于原点对称 3、计算exp比较耗时 | -                                         |\n| Tanh       | $tanh(x)=2σ(2x)−1$                       | 梯度弥散没解决                                         | 1、解决了原点对称问题 2、比sigmoid更快    |\n| ReLU       | $f(x)=max(0,x)$                          | 梯度弥散没完全解决                                     | 1、解决了部分梯度弥散问题 2、收敛速度更快 |\n| Leaky ReLU | $f(x)=αx(x<=0)$ $α$固定   $f(x)=x(x<0)$  | -                                                      | 解决了神经死亡问题                        |\n| pRelu      | $f(x)=αx(x<=0)$ $α$可学习  $f(x)=x(x<0)$ | 增加了极少量的参数, 降低过拟合风险                     | -                                         |\n\n**参考资料**\n\n- [What is activate function?](https://yogayu.github.io/DeepLearningCourse/03/ActivateFunction.html)\n- [资源 | 从ReLU到Sinc，26种神经网络激活函数可视化](https://mp.weixin.qq.com/s/7DgiXCNBS5vb07WIKTFYRQ)\n\n#### **Sigmiod、Relu、Tanh三个激活函数的缺点和不足，有没有更好的激活函数？**\n\nsigmoid、Tanh、ReLU的缺点如上，为了解决ReLU的dead cell的情况，发明了Leaky Relu， 即在输入小于0时不让输出为0，而是乘以一个较小的系数，从而保证有导数存在。同样的目的，还有一个ELU，函数示意图如下。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512976776_132.jpg)\n\n还有一个激活函数是Maxout，即使用两套w,b参数，输出较大值。本质上Maxout可以看做Relu的泛化版本，因为如果一套w,b全都是0的话，那么就是普通的ReLU。Maxout可以克服Relu的缺点，但是参数数目翻倍。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512976811_634.png)\n\n#### **为什么引入非线性激活函数？**\n\n第一，对于神经网络来说，网络的每一层相当于$f(wx+b)=f(w'x)$，对于线性函数，其实相当于$f(x)=x$，那么在线性激活函数下，每一层相当于用一个矩阵去乘以x，那么多层就是反复的用矩阵去乘以输入。根据矩阵的乘法法则，多个矩阵相乘得到一个大矩阵。所以线性激励函数下，多层网络与一层网络相当。 第二，非线性变换是深度学习有效的原因之一。原因在于非线性相当于对空间进行变换，变换完成后相当于对问题空间进行简化，原来线性不可解的问题现在变得可以解了。 下图可以很形象的解释这个问题，左图用一根线是无法划分的。经过一系列变换后，就变成线性可解的问题了。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512979018_790.jpg)\n\n如果不用激励函数（其实相当于激励函数是$f(x) = x$），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。 正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入（以及一些人的生物解释）。\n\n### RNN神经网络\n\n#### **什么是RNN**\n\nRNNs的目的使用来处理序列数据。在传统的神经网络模型中，是从输入层到隐含层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题却无能无力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中前后单词并不是独立的。 RNNs之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。 理论上，RNNs能够对任何长度的序列数据进行处理。但是在实践中，为了降低复杂性往往假设当前的状态只与前面的几个状态相关，下图便是一个典型的RNNs：\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515688523_186.jpg)\n\nRNNs包含输入单元(Input units)，输入集标记为{x0,x1,...,xt,xt+1,...}，而输出单元(Output units)的输出集则被标记为{y0,y1,...,yt,yt+1.,..}。RNNs还包含隐藏单元(Hidden units)，我们将其输出集标记为{s0,s1,...,st,st+1,...}，这些隐藏单元完成了最为主要的工作。你会发现，在图中：有一条单向流动的信息流是从输入单元到达隐藏单元的，与此同时另一条单向流动的信息流从隐藏单元到达输出单元。在某些情况下，RNNs会打破后者的限制，引导信息从输出单元返回隐藏单元，这些被称为“Back Projections”，并且隐藏层的输入还包括上一隐藏层的状态，即隐藏层内的节点可以自连也可以互连。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515688554_151.png)\n\n上图将循环神经网络进行展开成一个全神经网络。例如，对一个包含5个单词的语句，那么展开的网络便是一个五层的神经网络，每一层代表一个单词。对于该网络的计算过程如下： 1. xt表示第t,t=1,2,3...步(step)的输入。比如，x1为第二个词的one-hot向量(根据上图，x0为第一个词)； 2. st为隐藏层的第t步的状态，它是网络的记忆单元。 st根据当前输入层的输出与上一步隐藏层的状态进行计算。st=f(Uxt+Wst−1)，其中f一般是非线性的激活函数，如tanh或ReLU，在计算s0时，即第一个单词的隐藏层状态，需要用到s−1，但是其并不存在，在实现中一般置为0向量； 3. ot是第t步的输出，如下个单词的向量表示，ot=softmax(Vst).\n\n#### **梯度爆炸会引发什么？**\n\n在深度多层感知机网络中，梯度爆炸会引起网络不稳定，最好的结果是无法从训练数据中学习，而最坏的结果是出现无法再更新的 NaN 权重值。 梯度爆炸导致学习过程不稳定。—《深度学习》，2016. 在循环神经网络中，梯度爆炸会导致网络不稳定，无法利用训练数据学习，最好的结果是网络无法学习长的输入序列数据。\n\n有很多方法可以解决梯度爆炸问题，本节列举了一些最佳实验方法。\n\n1\\. 重新设计网络模型 在深度神经网络中，梯度爆炸可以通过重新设计层数更少的网络来解决。 使用更小的批尺寸对网络训练也有好处。 在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。\n\n2\\. 使用 ReLU 激活函数 在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。 使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。\n\n3\\. 使用长短期记忆网络 在循环神经网络中，梯度爆炸的发生可能是因为某种网络的训练本身就存在不稳定性，如随时间的反向传播本质上将循环网络转换成深度多层感知机神经网络。 使用长短期记忆（LSTM）单元和相关的门类型神经元结构可以减少梯度爆炸问题。 采用 LSTM 单元是适合循环神经网络的序列预测的最新最好实践。\n\n4\\. 使用梯度截断（Gradient Clipping） 在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。 处理梯度爆炸有一个简单有效的解决方案：如果梯度超过阈值，就截断它们。 ——《Neural Network Methods in Natural Language Processing》，2017. 具体来说，检查误差梯度的值是否超过阈值，如果超过，则截断梯度，将梯度设置为阈值。 梯度截断可以一定程度上缓解梯度爆炸问题（梯度截断，即在执行梯度下降步骤之前将梯度设置为阈值）。 ——《深度学习》，2016. 在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。 默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/。\n\n5\\. 使用权重正则化（Weight Regularization） 如果梯度爆炸仍然存在，可以尝试另一种方法，即检查网络权重的大小，并惩罚产生较大权重值的损失函数。该过程被称为权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。 对循环权重使用 L1 或 L2 惩罚项有助于缓解梯度爆炸。 ——On the difficulty of training recurrent neural networks，2013. 在 Keras 深度学习库中，你可以通过在层上设置 kernel\\_regularizer 参数和使用 L1 或 L2 正则化项进行权重正则化。\n\n#### **什么是LSTM网络？**\n\nLong Short Term 网络—— 一般就叫做 LSTM ——是一种 RNN 特殊的类型，可以学习长期依赖信息。如@寒小阳所说：LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态ht−1和当前输入xt。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。 LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。 LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！ 所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857524_689.png)\n\n标准 RNN 中的重复模块包含单一的层 LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857583_596.png) LSTM 中的重复模块包含四个交互的层 不必担心这里的细节。我们会一步一步地剖析 LSTM 解析图。现在，我们先来熟悉一下图中使用的各种元素的图标。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857610_673.png)\n\nLSTM 中的图标 在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。 四、LSTM 的核心思想 LSTM 的关键就是细胞状态，水平线在图上方贯穿运行。 细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857686_304.png)\n\nLSTM 有通过精心设计的称作为“门”的结构来去除或者增加信息到细胞状态的能力。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857715_135.png)\n\nSigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！ LSTM 拥有三个门，来保护和控制细胞状态。 在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取 h\\_{t-1} 和 x\\_t，输出一个在 0 到 1 之间的数值给每个在细胞状态 C\\_{t-1} 中的数字。1 表示“完全保留”，0 表示“完全舍弃”。 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857798_931.png)\n\n决定丢弃信息 下一步是确定什么样的新信息被存放在细胞状态中。这里包含两个部分。第一，sigmoid 层称 “输入门层” 决定什么值我们将要更新。然后，一个 tanh 层创建一个新的候选值向量，\\\\tilde{C}\\_t，会被加入到状态中。下一步，我们会讲这两个信息来产生对状态的更新。 在我们语言模型的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515857956_610.png)\n\n确定更新的信息 现在是更新旧细胞状态的时间了，C\\_{t-1} 更新为 C\\_t。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。 我们把旧状态与 f\\_t 相乘，丢弃掉我们确定需要丢弃的信息。接着加上 i\\_t \\* \\\\tilde{C}\\_t。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。 在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515858026_294.png)\n\n更新细胞状态 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1515858060_635.png)\n\n#### **详细说说CNN工作原理**\n\n1 人工神经网络\n\n1.1 神经元 神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。 举个手写识别的例子，给定一个未知数字，让神经网络识别是什么数字。此时的神经网络的输入由一组被输入图像的像素所激活的输入神经元所定义。在通过非线性激活函数进行非线性变换后，神经元被激活然后被传递到其他神经元。重复这一过程，直到最后一个输出神经元被激活。从而识别当前数字是什么字。 神经网络的每个神经元如下\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764270_334.png)\n\n基本wx + b的形式，其中 x1、x2表示输入向量 w1、w2为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重 b为偏置bias g(z) 为激活函数 a 为输出 如果只是上面这样一说，估计以前没接触过的十有八九又必定迷糊了。事实上，上述简单模型可以追溯到20世纪50/60年代的感知器，可以把感知器理解为一个根据不同因素、以及各个因素的重要性程度而做决策的模型。 举个例子，这周末北京有一草莓音乐节，那去不去呢？决定你是否去有二个因素，这二个因素可以对应二个输入，分别用x1、x2表示。此外，这二个因素对做决策的影响程度不一样，各自的影响程度用权重w1、w2表示。一般来说，音乐节的演唱嘉宾会非常影响你去不去，唱得好的前提下 即便没人陪同都可忍受，但如果唱得不好还不如你上台唱呢。所以，我们可以如下表示： x1：是否有喜欢的演唱嘉宾。x1 = 1 你喜欢这些嘉宾，x1 = 0 你不喜欢这些嘉宾。嘉宾因素的权重w1 = 7 x2：是否有人陪你同去。x2 = 1 有人陪你同去，x2 = 0 没人陪你同去。是否有人陪同的权重w2 = 3。 这样，咱们的决策模型便建立起来了：g(z) = g(w1\\*x1 + w2\\*x2 + b )，g表示激活函数，这里的b可以理解成 为更好达到目标而做调整的偏置项。 一开始为了简单，人们把激活函数定义成一个线性函数，即对于结果做一个线性变化，比如一个简单的线性激活函数是g(z) = z，输出都是输入的线性变换。后来实际应用中发现，线性激活函数太过局限，于是人们引入了非线性激活函数。\n\n1.2 激活函数 常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。这里先简要介绍下最基础的sigmoid函数（btw，在本博客中SVM那篇文章开头有提过）。 sigmoid的函数表达式如下\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764421_303.png)\n\n其中z是一个线性组合，比如z可以等于：b + w1\\*x1 + w2\\*x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。 因此，sigmoid函数g(z)的图形表示如下（ 横轴表示定义域z，纵轴表示值域g(z) ）：\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764454_682.jpg)\n\n也就是说，sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。 压缩至0到1有何用处呢？用处是这样一来便可以把激活函数看作一种“分类的概率”，比如激活函数的输出为0.9的话便可以解释为90%的概率为正样本。 举个例子，如下图（图引自Stanford机器学习公开课）\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764516_976.png)\n\nz = b + w1\\*x1 + w2\\*x2，其中b为偏置项 假定取-30，w1、w2都取为20\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764562_932.jpg)\n\n如果x1 = 0 x2 = 0，则z = -30，g(z) = 1/( 1 + e^-z )趋近于0。此外，从上图sigmoid函数的图形上也可以看出，当z=-30的时候，g(z)的值趋近于0 如果x1 = 0 x2 = 1，或x1 =1 x2 = 0，则z = b + w1\\*x1 + w2\\*x2 = -30 + 20 = -10，同样，g(z)的值趋近于0 如果x1 = 1 x2 = 1，则z = b + w1\\*x1 + w2\\*x2 = -30 + 20\\*1 + 20\\*1 = 10，此时，g(z)趋近于1。 换言之，只有和都取1的时候，g(z)→1，判定为正样本；或取0的时候，g(z)→0，判定为负样本，如此达到分类的目的。\n\n1.3 神经网络 将下图的这种单个神经元\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764664_457.png)\n\n组织在一起，便形成了神经网络。下图便是一个三层神经网络结构\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764679_794.png)\n\n上图中最左边的原始输入信息称之为输入层，最右边的神经元称之为输出层（上图中输出层只有一个神经元），中间的叫隐藏层。 啥叫输入层、输出层、隐藏层呢？ 输入层（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。 输出层（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。 隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。 同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。比如下图中间隐藏层来说，隐藏层的3个神经元a1、a2、a3皆各自接受来自多个不同权重的输入（因为有x1、x2、x3这三个输入，所以a1 a2 a3都会接受x1 x2 x3各自分别赋予的权重，即几个输入则几个权重），接着，a1、a2、a3又在自身各自不同权重的影响下 成为的输出层的输入，最终由输出层输出最终结果。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764767_233.png)\n\n上图（图引自Stanford机器学习公开课）中\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764777_706.png)表示第j层第i个单元的激活函数/神经元 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764783_937.png)\n\n表示从第j层映射到第j+1层的控制函数的权重矩阵 此外，输入层和隐藏层都存在一个偏置（bias unit)，所以上图中也增加了偏置项：x0、a0。针对上图，有如下公式\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516770180_891.png)\n\n此外，上文中讲的都是一层隐藏层，但实际中也有多层隐藏层的，即输入层和输出层中间夹着数层隐藏层，层和层之间是全连接的结构，同一层的神经元之间没有连接。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764808_830.jpg)\n\n2 卷积神经网络之层级结构 cs231n课程里给出了卷积神经网络各个层级结构，如下图\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516770378_513.jpg)\n\n上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车 所以 最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。 中间是 CONV：卷积计算层，线性乘积 求和。 RELU：激励层，上文2.2节中有提到：ReLU是激活函数的一种。 POOL：池化层，简言之，即取区域平均或最大。 最右边是 FC：全连接层 这几个部分中，卷积计算层是CNN的核心，下文将重点阐述。\n\n3 CNN之卷积计算层 3.1 CNN怎么进行识别 当我们给定一个\"X\"的图案，计算机怎么识别这个图案就是“X”呢？一个可能的办法就是计算机存储一张标准的“X”图案，然后把需要识别的未知图案跟标准\"X\"图案进行比对，如果二者一致，则判定未知图案即是一个\"X\"图案。 而且即便未知图案可能有一些平移或稍稍变形，依然能辨别出它是一个X图案。如此，CNN是把未知图案和标准X图案一个局部一个局部的对比，如下图所示\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764841_515.png)\n\n而未知图案的局部和标准X图案的局部一个一个比对时的计算过程，便是卷积操作。卷积计算结果为1表示匹配，否则不匹配。 接下来，我们来了解下什么是卷积操作。 3.2 什么是卷积 对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 非严格意义上来讲，下图中红框框起来的部分便可以理解为一个滤波器，即带着一组固定权重的神经元。多个滤波器叠加便成了卷积层。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764868_951.png)\n\nOK，举个具体的例子。比如下图中，图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764882_762.png)\n\n分解下上图 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764898_144.png)对应位置上是数字先相乘后相加 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764912_847.png) = ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764919_694.png)\n\n中间滤波器filter与数据窗口做内积，其具体计算过程则是：\n\n4\\*0 + 0\\*0 + 0\\*0 + 0\\*0 + 0\\*1 + 0\\*1 + 0\\*0 + 0\\*1 + -4\\*2 = -8 3.3\n\n图像上的卷积 在下图对应的计算过程中，输入是一定区域大小(width\\*height)的数据，和滤波器filter（带着一组固定权重的神经元）做内积后等到新的二维数据。 具体来说，左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。 如下图所示\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764936_540.png)\n\n3.4 GIF动态卷积图 在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数：\n\na. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。\n\nb. 步长stride：决定滑动多少步可以到边缘。\n\nc. 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。\n\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1516764965_191.png)\n\n神经网络中，哪些方法可以防止过拟合？**\n\n① Dropout ② 加L1/L2正则化 ③ BatchNormalization\n\n#### **CNN关键层有哪些？**\n\n① 输入层，对数据去均值，做data augmentation等工作 ② 卷积层，局部关联抽取feature ③ 激活层，非线性变化 ④ 池化层，下采样 ⑤ 全连接层，增加模型非线性 ⑥ 高速通道，快速连接 ⑦ BN层，缓解梯度弥散\n\n#### **GRU是什么？GRU对LSTM做了哪些改动？**\n\nGRU是Gated Recurrent Units，是循环神经网络的一种。 GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM用memory cell 把hidden state 包装起来。\n\n#### **IOU评价函数**\n\n在目标检测的评价体系中，有一个参数叫做 IoU ，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。 具体我们可以简单的理解为：即检测结果DetectionResult与真实值Ground Truth的交集比上它们的并集，即为检测的准确率 IoU :\n![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494665_290.png)\n\n举个例子，下面是一张原图 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525495290_937.png)\n\n然后我们对其做下目标检测，其DR = DetectionResult，GT = GroundTruth。 ![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1525494780_911.png) \n\n黄色边框框起来的是： DR⋂GT 绿色框框起来的是： DR⋃GT 不难看出，最理想的情况就是DR与GT完全重合，即IoU = 1。\n\n#### **当神经网络调参效果不好时，应该从哪几个方面考虑？**\n\n1）是否找到合适的损失函数？（不同问题适合不同的损失函数）（理解不同损失函数的适用场景）\n\n2）batch size是否合适？batch size太大 -> loss很快平稳，batch size太小 -> loss会震荡（理解mini-batch）\n\n3）是否选择了合适的激活函数？（各个激活函数的来源和差异） 4）学习率，学习率小收敛慢，学习率大loss震荡（怎么选取合适的学习率） 5）是否选择了合适的优化算法？（比如adam）（理解不同优化算法的适用场景） 6）是否过拟合？(深度学习拟合能力强，容易过拟合)（理解过拟合的各个解决方案） a. Early Stopping b. Regularization（正则化） c. Weight Decay（收缩权重） d. Dropout（随机失活）e. 调整网络结构"},{"title":"sgd_sklean","url":"/2021/04/02/sgd_sklean/","content":"Given a set of training examples$ (x_1, y_1), \\ldots, (x_n, y_n)$ where$x_i \\in \\mathbf{R}^m$ and $y_i \\in \\mathcal{R}$( $y_i \\in {-1,1}$for classification), our goal is to learn a linear scoring function $f(x)=w^Tx+b$ with model parameters $w \\in \\mathbf{R}^m $and intercept $b∈R$. In order to make predictions for binary classification, we simply look at the sign of $f(x)$. To find the model parameters, we minimize the regularized training error given by\n$$\nE(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)\n$$\nwhere $L$ is a loss function that measures model (mis)fit and $R$ is a regularization term (aka penalty) that penalizes model complexity; $α>0$ is a non-negative hyperparameter that controls the regularization stength.\n\nDifferent choices for L entail different classifiers or regressors:\n\n- Hinge (soft-margin): equivalent to Support Vector Classification. $L(y_i,f(x_i))=max(0,1−y_if(x_i))$.\n- **Perceptron**: $L(y_i,f(x_i))=max(0,−y_if(x_i))$.\n- Modified Huber: $L(y_i,f(x_i))=max(0,1−y_if(x_i))2 if y_if(x_i)>1$, and $L(y_i,f(x_i))=−4y_if(x_i)$ otherwise.\n- Log: equivalent to Logistic Regression. $L(y_i,f(x_i))=log⁡(1+exp⁡(−y_if(x_i)))$.\n- Least-Squares: Linear regression (Ridge or Lasso depending on R). $L(y_i,f(x_i))=\\frac{1}{2}(y_i−f(x_i))^2$.\n- Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when $|y_i−f(x_i)|≤ε$, and $L(y_i,f(x_i))=ε|y_i−f(x_i)|−\\frac{1}{2}ε^2$ otherwise.\n- Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression. $L(y_i,f(x_i))=max(0,|y_i−f(x_i)|−ε)$.\n\nAll of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.\n\n[![../_images/sphx_glr_plot_sgd_loss_functions_0011.png](https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_loss_functions_0011.png)](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html)\n\nPopular choices for the regularization term R (the `penalty` parameter) include:\n\n> - L2 norm: $R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2$\n> - L1 norm: $R(w) := \\sum_{j=1}^{m} |w_j|$, which leads to sparse solutions.\n> - Elastic Net: $R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 +\n>   (1-\\rho) \\sum_{j=1}^{m} |w_j|$, a convex combination of L2 and L1, where ρ is given by `1 - l1_ratio`.\n\n\n\n\n\nLinear svr 的损失函数如下：\n$$\n\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),\n$$\nwhere we make use of the epsilon-insensitive loss, i.e. errors of less than ε are ignored. This is the form that is directly optimized by [`LinearSVR`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR).\n\n\n\n\n\n### LinearSVC[¶](https://scikit-learn.org/stable/modules/svm.html#linearsvc)\n\nThe primal problem can be equivalently formulated as\n$$\n\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0,1 - y_i (w^T \\phi(x_i) + b)),\n$$\nwhere we make use of the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss). This is the form that is directly optimized by [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC), but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by [`LinearSVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) (ϕ is the identity function).\n\n"},{"title":"pg数据库插入效率对比","url":"/2021/03/31/pg数据库插入效率对比/","content":"#### 1. 简介\n\n本文主要介绍6种postgresql数据的插入方式的效率；\n\n数据采用python faker包生成的假数据，样式如下：\n\n一条数据如下：谭磊\t湖南省兴安盟县孝南关岭路q座 578703\t南京市\tMMTE96835803777282\t维旺明科技有限公司\t341102209612219\tVISA 13 digit\t2005-09-28\t13168174481\n\n生成代码如下：\n\n```python\nfrom faker import Faker\nfake = Faker(locale='zh_CN')\n\n\ndef fake_row(i):\n    row = [fake.name(), fake.address(), fake.city(), fake.bban(),\n           fake.company(), fake.credit_card_number(card_type=None), fake.credit_card_provider(card_type=None),\n           fake.date(pattern=\"%Y-%m-%d\", end_datetime=None), fake.phone_number()]\n    return row\n\n\nstart = time.time()\nfake_data = [fake_row(i) for i in range(1000000)]\ndata = pd.DataFrame(fake_data)\ndata.columns = [\"col{}\".format(i) for i in range(1, 10)]\nprint(data)\n```\n\n\n\n\n\n```python\nfrom sqlalchemy import create_engine\nimport pandas as pd\nimport time\nimport io\nimport codecs\nimport csv\nfrom psycopg2 import extras as ex\nfrom faker import Faker\n\n\ndef coast_time(func):\n    def fun(*args, **kwargs):\n        t = time.perf_counter()\n        result = func(*args, **kwargs)\n        print(f'func {func.__name__} need time:{time.perf_counter() - t:.2f} s')\n        return result\n\n    return fun\n\n\ndef psql_insert_copy(table, conn, keys, data_iter):\n    dbapi_conn = conn.connection\n    with dbapi_conn.cursor() as cur:\n        s_buf = io.StringIO()\n        writer = csv.writer(s_buf)\n        writer.writerows(data_iter)\n        s_buf.seek(0)\n\n        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n        if table.schema:\n            table_name = '{}.{}'.format(table.schema, table.name)\n        else:\n            table_name = table.name\n\n        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n            table_name, columns)\n        cur.copy_expert(sql=sql, file=s_buf)\n\n\n@coast_time\ndef copy_insert(engine, data, table_name):\n    conn = engine.raw_connection()\n    cur = conn.cursor()\n    output = io.StringIO()\n    data.to_csv(output, sep='\\t', header=False, index=False)\n    output.seek(0)\n    cur.copy_from(output, table_name, null=\"\")\n    conn.commit()\n\n\n@coast_time\ndef copy_insert_sql(engine, data, table_name):\n    conn = engine.raw_connection()\n    cur = conn.cursor()\n    columns = data.columns\n    str_cols = \",\".join(columns)\n    f = io.BytesIO()\n\n    StreamWriter = codecs.getwriter(\"utf-8\")\n    csv_writer = csv.writer(StreamWriter(f))\n    for row in data.values:\n        csv_writer.writerow([_ for _ in row])\n    f.seek(0)\n    cur.copy_expert(sql=\"copy {}({}) from stdin WITH (FORMAT CSV)\".format(table_name, str_cols), file=f)\n    conn.commit()\n\n\n@coast_time\ndef insert_many(engine, data, table_name):\n    columns = data.columns\n    str_cols = \",\".join(columns)\n    str_sss = ','.join(len(columns) * ['%s'])\n    sql_query = \"\"\"INSERT INTO {}({}) VALUES ({});\"\"\".format(table_name, str_cols, str_sss)\n    data_list = [tuple(x) for x in data.values]\n    conn = engine.raw_connection()\n    cur = conn.cursor()\n    cur.executemany(sql_query, data_list)\n    conn.commit()\n\n\n@coast_time\ndef insert_values(engine, data, table_name):\n    columns = data.columns\n    str_cols = \",\".join(columns)\n    sql_query = \"\"\"INSERT INTO {}({}) VALUES %s;\"\"\".format(table_name, str_cols)\n    data_list = [tuple(x) for x in data.values]\n    conn = engine.raw_connection()\n    cur = conn.cursor()\n    ex.execute_values(cur, sql_query, data_list, page_size=10000)\n    conn.commit()\n\n\n@coast_time\ndef to_sql_insert_normal(engine, data, table_name):\n    data.to_sql(table_name, engine, index=False, if_exists='replace')\n\n\n@coast_time\ndef to_sql_insert_copy(engine, data, table_name):\n    data.to_sql(table_name, engine, method=psql_insert_copy, index=False, if_exists='replace')\n\n\nuser = 'postgres'\npassword = '121457'\nhost = '127.0.0.1'\nport = '5432'\ndatabase_name = 'test_data_base'\n\ndatabase_url = 'postgresql+psycopg2://{user}:{password}@{host}:{port}/{database_name}?client_encoding=utf8'.format(\n    user=user,\n    host=host,\n    port=port,\n    password=password,\n    database_name=database_name,\n)\n\nengine = create_engine(database_url)\nfake = Faker(locale='zh_CN')\n\n\ndef fake_row(i):\n    row = [fake.name(), fake.address(), fake.city(), fake.bban(),\n           fake.company(), fake.credit_card_number(card_type=None), fake.credit_card_provider(card_type=None),\n           fake.date(pattern=\"%Y-%m-%d\", end_datetime=None), fake.phone_number()]\n    return row\n\n\nstart = time.time()\nfake_data = [fake_row(i) for i in range(1000000)]\ndata = pd.DataFrame(fake_data)\ndata.columns = [\"col{}\".format(i) for i in range(1, 10)]\nprint(\"func generate data need:{:.2f}秒\".format(time.time() - start))\n\ninsert_many(engine, data, \"tb_faker1\")\ninsert_values(engine, data, \"tb_faker2\")\ncopy_insert(engine, data, \"tb_faker3\")\ncopy_insert_sql(engine, data, \"tb_faker4\")\nto_sql_insert_normal(engine, data, \"tb_faker5\")\nto_sql_insert_copy(engine, data, \"tb_faker6\")\n\n```\n\n\n\n50万数据的结果如下：\n\n```\nfunc insert_many need time:95.0008 s\nfunc insert_values need time:19.6026 s\nfunc copy_insert need time:7.7923 s\nfunc copy_insert_sql need time:9.4167 s\nfunc to_sql_insert_normal need time:117.4498 s\nfunc to_sql_insert_copy need time:9.9843 s\n```\n\n\n\n100万数据的结果如下：\n\n```\nfunc generate data need:340.35秒\nfunc insert_many need time:103.80 s\nfunc insert_values need time:19.59 s\nfunc copy_insert need time:7.57 s\nfunc copy_insert_sql need time:9.39 s\nfunc to_sql_insert_normal need time:124.07 s\nfunc to_sql_insert_copy need time:8.87 s\n```\n\n"},{"title":"面试","url":"/2021/03/16/面试/","content":"[TOC]\n\n# 机器学习\n\n## 1. 逻辑回归（LR）\n\n### 1.1 基本原理\n\n[逻辑回归（Logistic Regression，LR）](https://en.wikipedia.org/wiki/Logistic_regression)也称为\"对数几率回归\"，又称为\"逻辑斯谛\"回归。\n\n1.1.1 **知识点提炼**\n\n- **分类**，经典的二分类算法！\n- 逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。\n- Logistic 回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）\n- 回归模型中，y 是一个定性变量，比如 y = 0 或 1，logistic 方法主要应用于研究某些事件发生的概率。\n- 逻辑回归的本质：极大似然估计\n- 逻辑回归的激活函数：Sigmoid\n- 逻辑回归的代价函数：交叉熵\n\n**逻辑回归的优缺点**\n\n优点： \n\n1）速度快，适合二分类问题 \n\n2）简单易于理解，直接看到各个特征的权重 \n\n3）能容易地更新模型吸收新的数据 \n\n缺点： \n\n对数据和场景的适应能力有局限性，不如决策树算法适应性那么强\n\n**逻辑回归中最核心的概念是 [Sigmoid 函数](https://en.wikipedia.org/wiki/Sigmoid_function)**，Sigmoid函数可以看成逻辑回归的激活函数。\n\n下图是逻辑回归网络：\n\n![Logistic Regression.png](./面试/DLIB-0001.png)\n\n对数几率函数（Sigmoid）：$y = \\sigma (z) = \\frac{1}{1+e^{-z}}$\n\n\n![](./面试/DLIB-0002.png)\n\n通过对数几率函数的作用，我们可以将输出的值限制在区间[0，1]上，$p(x)$ 则可以用来表示概率 p(y=1|x)，即当一个x发生时，y被分到1那一组的概率。可是，等等，我们上面说 y 只有两种取值，但是这里却出现了一个区间[0, 1]，这是什么鬼？？其实在真实情况下，我们最终得到的y的值是在 [0, 1] 这个区间上的一个数，然后我们可以选择一个阈值，通常是 0.5，当 y > 0.5 时，就将这个 x 归到 1 这一类，如果 y< 0.5 就将 x 归到 0 这一类。但是阈值是可以调整的，比如说一个比较保守的人，可能将阈值设为 0.9，也就是说有超过90%的把握，才相信这个x属于 1这一类。了解一个算法，最好的办法就是自己从头实现一次。下面是逻辑回归的具体实现。\n\n**Regression 常规步骤**\n\n1. 寻找h函数（即预测函数）\n2. 构造J函数（损失函数）\n3. 想办法（迭代）使得J函数最小并求得回归参数（θ）\n\n函数h(x)的值有特殊的含义，它表示结果取1的概率，于是可以看成类1的后验估计。因此对于输入x分类结果为类别1和类别0的概率分别为： \n\n$P(y=1│x;θ)=hθ (x) $\n\n$P(y=0│x;θ)=1-hθ (x)$\n\n**代价函数**\n\n**逻辑回归一般使用交叉熵作为代价函数**。关于[代价函数](https://en.wikipedia.org/wiki/Loss_function)的具体细节，请参考[代价函数](http://www.cnblogs.com/Belter/p/6653773.html)。\n\n交叉熵是对「出乎意料]的度量。神经元的目标是去计算函数 $y$, 且 $y = y(x)$。但是我们让它取而代之计算函数 a, 且 a = a(x) 。假设我们把 a 当作 y 等于 1 的概率，1−a 是 y 等于 0 的概率。那么，交叉熵衡量的是我们在知道 y 的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。\n\n交叉熵代价函数如下所示：\n\n$$J(w)=-l(w)=-\\sum_{i = 1}^n y^{(i)}ln(\\phi(z^{(i)})) + (1 - y^{(i)})ln(1-\\phi(z^{(i)}))$$\n\n$$J(\\phi(z),y;w)=-yln(\\phi(z))-(1-y)ln(1-\\phi(z))$$\n\n注：为什么要使用交叉熵函数作为代价函数，而不是平方误差函数？请参考：[逻辑回归算法之交叉熵函数理解](https://blog.csdn.net/syyyy712/article/details/78252722)\n\n**逻辑回归伪代码**\n\n```\n初始化线性函数参数为1\n构造sigmoid函数\n重复循环I次\n\t计算数据集梯度\n\t更新线性函数参数\n确定最终的sigmoid函数\n输入训练（测试）数据集\n运用最终sigmoid函数求解分类\n```\n\n**逻辑回归算法之Python实现**\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # 为X增加一列特征x1，x1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # 梯度训练n_iterations轮\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n```\n\n**参考资料**\n\n- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n- 《统计学习方法》 (蓝书)  第6章  P77页\n- 《机器学习》 (西瓜书) 第3章  P57页\n- [《Machine Learning》 吴恩达 Logistic Regression](https://d19vezwu8eufl6.cloudfront.net/ml/docs%2Fslides%2FLecture6.pdf)\n- [逻辑回归 - 理论篇](https://blog.csdn.net/pakko/article/details/37878837)\n- [逻辑回归(logistic regression)的本质——极大似然估计](https://blog.csdn.net/zjuPeco/article/details/77165974)\n- [机器学习算法与Python实践之（七）逻辑回归（Logistic Regression）](https://www.cnblogs.com/zhizhan/p/4868555.html)\n- [机器学习之Logistic回归与Python实现](https://blog.csdn.net/moxigandashu/article/details/72779856)\n- [【机器学习】逻辑回归（Logistic Regression）](https://www.cnblogs.com/Belter/p/6128644.html)\n- [机器学习算法--逻辑回归原理介绍](https://blog.csdn.net/chibangyuxun/article/details/53148005)\n- [逻辑回归算法面经](https://zhuanlan.zhihu.com/p/46591702)\n- [Logistic Regression 模型简介](https://tech.meituan.com/2015/05/08/intro-to-logistic-regression.html)\n\n### 1.2 为什么 LR 要使用 sigmoid 函数？\n\n1.广义模型推导所得\n2.满足统计的最大熵模型\n3.性质优秀，方便使用（Sigmoid函数是平滑的，而且任意阶可导，一阶二阶导数可以直接由函数值得到不用进行求导，这在实现中很实用）\n\n**参考资料**\n\n- [为什么逻辑回逻辑回归为什么不用核函数呢？归 模型要使用 sigmoid 函数](https://blog.csdn.net/weixin_39881922/article/details/80366324)\n\n### 1.3 LR 可以用核函数么？\n\n LR是可以使用核函数，但是通常没有不会这么干。原因如下：\n\n核方法用于分类的时候用的是hinge loss，可以方便的转化为对偶形式求解，也就是SVM。\n\n逻辑回归中交叉熵这个损失函数，对kernel methods来说可能有点伤…转化易求解的形式比较难，而且损失是不是凹函数都不一定。\n\n但，如果分类函数为逻辑函数运用于rkhs方程上，还是可以用剃度方法求解的，但是就没有解析解了。svm的理论保障不知道容不容易适用这种情况。\n\n**参考资料**\n\n[逻辑回归为什么不用核函数呢？]([https://www.zhihu.com/question/350875722)\n\n### 1.4 为什么 LR 用交叉熵损失而不是平方损失？\n\nlogistic回归和softmax回归使用交叉熵而不用欧氏距离是因为前者的目标函数是凸函数，可以求得全局极小值点；用欧氏距离则无法保证。\n\n### 1.5 LR 能否解决非线性分类问题？\n\n逻辑回归本质上是线性回归模型，关于系数是线性函数，分离平面无论是线性还是非线性的，逻辑回归其实都可以进行分类。对于非线性的，需要自己去定义一个非线性映射。\n\n**参考资料**\n\n- [逻辑斯蒂回归能否解决非线性分类问题？](https://www.zhihu.com/question/29385169)\n\n### 1.6 LR为什么要离散特征？\n\n1. 在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列 0/1 的离散特征。\n\n   其优势有：\n\n   - 离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储。\n\n   - 离散化之后的特征对于异常数据具有很强的鲁棒性。\n\n     如：销售额作为特征，当销售额在 `[30,100)` 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。\n\n   - 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线性，提升模型的表达能力，增强拟合能力。\n\n     假设某个连续特征 $j$ ，它离散化为 $M$ 个 0/1 特征 $j_{1}, j_{2}, \\cdots, j_{M}$ 。则：$w_{j} * x_{j} \\rightarrow w_{j_{1}} * x_{j_{1}}^{\\prime}+w_{j_{2}} * x_{j_{2}}^{\\prime}+\\cdots+w_{j_{M}} * x_{j_{M}}^{\\prime}$ 。其中 $x_{j_{1}}^{\\prime}, \\cdots, x_{j_{\\mu}}^{\\prime}$ 是离散化之后的新的特征，它们的取值空间都是 $\\{0,1\\}$ 。\n\n     上式右侧是一个分段线性映射，其表达能力更强。\n\n   - 离散化之后可以进行特征交叉。假设有连续特征 $j$，离散化为 $M$ 个 0/1 特征；连续特征 $k$，离散化为 $N$ 个 0/1 特征，则分别进行离散化之后引入了 $M+N$ 个特征。\n\n     假设离散化时，并不是独立进行离散化，而是特征 $M+N$ 联合进行离散化，则可以得到 $M\\times N$ 个组合特征。这会进一步引入非线性，提高模型表达能力。\n\n   - 离散化之后，模型会更稳定。\n\n     如对销售额进行离散化，`[30,100)` 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。\n\n     但是处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理。\n\n2. 特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险。\n\n   能够对抗过拟合的原因：经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念。因此能够对抗数据的扰动，更具有鲁棒性。\n\n   另外它使得模型要拟合的值大幅度降低，也降低了模型的复杂度。\n\n### 1.7 逻辑回归是处理线性问题还是非线性问题的？ \n\n逻辑回归本质上是线性回归模型，关于系数是线性函数，分离平面无论是线性还是非线性的，逻辑回归其实都可以进行分类。对于非线性的，需要自己去定义一个非线性映射。\n\n## 2. 线性回归\n\n### 2.1 线性回归与逻辑回归（LR）的区别\n\n**不同点**：\n\n- 逻辑回归处理的是分类问题，线性回归处理的是回归问题；\n- 逻辑回归中认为y是因变量，即逻辑回归的因变量是离散的，线性回归的因变量是连续的。\n\n**相同点：**\n\n- 二者都使用了极大似然估计来对训练样本进行建模\n- 求解超参数过程中，都可以使用梯度下降的方法\n\n**联系**：\n\n如果把一个事件的几率（odds）定义为该事件发生的概率与不发生概率的比值 $\\frac{p}{1-p}$ ，那么逻辑回归可以看做是对于\"y=1|x\"这一事件的对数几率的线性回归\n\n**参考资料**\n\n- [线性回归和逻辑回归的比较](https://blog.csdn.net/ddydavie/article/details/82668141)\n\n## 3. 支持向量机（SVM）\n\n### 3.1 基本原理\n\n[支持向量机（supporr vector machine，SVM）](https://en.wikipedia.org/wiki/Support-vector_machine)是一种二类分类模型，该模型是定义在特征空间上的间隔最大的线性分类器。间隔最大使它有区别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是**间隔最大化**，可形式化为一个求解凸二次规划的最小化问题。\n\n**3.1.1 知识点提炼：**\n\n- SVM核函数\n  - 多项式核函数\n  - 高斯核函数\n  - 字符串核函数\n- SMO\n- SVM损失函数\n\n支持向量机的学习算法是求解凸二次规划的最优化算法。\n\n支持向量机学习方法包含构建由简至繁的模型：\n\n- 线性可分支持向量机\n- 线性支持向量机\n- 非线性支持向量机（使用核函数）\n\n当训练数据线性可分时，通过硬间隔最大化（hard margin maximization）学习一个线性的分类器，即线性可分支持向量机，又成为硬间隔支持向量机；\n\n当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization）也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；\n\n当训练数据不可分时，通过核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。\n\n注：以上各SVM的数学推导应该熟悉：硬间隔最大化（几何间隔）---学习的对偶问题---软间隔最大化（引入松弛变量）---非线性支持向量机（核技巧）。\n\n**3.1.2 SVM的主要特点**\n\n（1）非线性映射-理论基础 \n\n（2）最大化分类边界-方法核心 \n\n（3）支持向量-计算结果 \n\n（4）小样本学习方法 \n\n（5）最终的决策函数只有少量支持向量决定，避免了“维数灾难” \n\n（6）少数支持向量决定最终结果—->可“剔除”大量冗余样本+算法简单+具有鲁棒性（体现在3个方面） \n\n（7）学习问题可表示为凸优化问题—->全局最小值 \n\n（8）可自动通过最大化边界控制模型，但需要用户指定核函数类型和引入松弛变量 \n\n（9）适合于小样本，优秀泛化能力（因为结构风险最小） \n\n（10）泛化错误率低，分类速度快，结果易解释\n\n**SVM为什么采用间隔最大化？**\n\n当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。\n\n感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。\n\n线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。\n\n然后应该借此阐述，几何间隔，函数间隔，及从函数间隔—>求解最小化1/2 ||w||^2 时的w和b。即线性可分支持向量机学习算法—最大间隔法的由来。\n\n**为什么要将求解SVM的原始问题转换为其对偶问题？**\n\n1. 对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）\n2. 自然引入核函数，进而推广到非线性分类问题\n\n**为什么SVM要引入核函数？**\n\n当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。\n\n**SVM核函数有哪些？**\n\n- 线性（Linear）核函数：主要用于线性可分的情形。参数少，速度快。\n- 多项式核函数\n- 高斯（RBF）核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。\n- Sigmoid核函数\n- 拉普拉斯（Laplac）核函数\n\n注：如果feature数量很大，跟样本数量差不多，建议使用LR或者Linear kernel的SVM。如果feature数量较少，样本数量一般，建议使用Gaussian Kernel的SVM。\n\n**SVM如何处理多分类问题？**\n\n一般有两种做法：\n\n1. 直接法：直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。看似简单但是计算量却非常的大。\n\n2. 间接法：对训练器进行组合。其中比较典型的有一对一，和一对多。\n   - 一对多：对每个类都训练出一个分类器，由svm是二分类，所以将此而分类器的两类设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。这种方法效果不太好，bias比较高。\n   - 一对一：针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k) 个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。\n\n**SVM中硬间隔和软间隔**\n\n硬间隔分类即线性可分支持向量机，软间隔分类即线性不可分支持向量机，利用软间隔分类时是因为存在一些训练集样本不满足函数间隔（泛函间隔）大于等于1的条件，于是加入一个非负的参数 ζ （松弛变量），让得出的函数间隔加上 ζ 满足条件。于是软间隔分类法对应的拉格朗日方程对比于硬间隔分类法的方程就多了两个参数（一个ζ ，一个 β），但是当我们求出对偶问题的方程时惊奇的发现这两种情况下的方程是一致的。下面我说下自己对这个问题的理解。\n\n我们可以先考虑软间隔分类法为什么会加入ζ 这个参数呢？硬间隔的分类法其结果容易受少数点的控制，这是很危险的，由于一定要满足函数间隔大于等于1的条件，而存在的少数离群点会让算法无法得到最优解，于是引入松弛变量，从字面就可以看出这个变量是为了缓和判定条件，所以当存在一些离群点时我们只要对应给他一个ζi，就可以在不变更最优分类超平面的情况下让这个离群点满足分类条件。\n\n综上，我们可以看出来软间隔分类法加入ζ 参数，使得最优分类超平面不会受到离群点的影响，不会向离群点靠近或远离，相当于我们去求解排除了离群点之后，样本点已经线性可分的情况下的硬间隔分类问题，所以两者的对偶问题是一致的。\n\n### 3.2 支持向量中的向量是指什么？\n\n### 3.3  SVM为什么会对缺失值敏感？实际应用时候你是如何处理？\n\n涉及到距离度量(distance measurement)时，如计算两个点之间的距离，缺失数据就变得比较重要。如果缺失值处理不当就会导致效果很差，如SVM，KNN。\n\n常用的缺失值处理方法：\n\n（1）把数值型变量(numerical variables)中的缺失值用其所对应的类别中(class)的中位数(median)替换。把描述型变量(categorical variables)缺失的部分用所对应类别中出现最多的数值替代(most frequent non-missing value)。【快速简单但效果差】（平均数、中位数、众数、插值等）\n\n（2）将缺失值当成新的数值，NaN\n\n（3）忽略该项数据（当缺失少时）\n\n### 3.4 SVM为什么可以分类非线性问题？\n\n原输入空间是一个非线性可分问题，能用一个超曲面将正负例正确分开；\n\n通过核技巧的非线性映射，将输入空间的超曲面转化为特征空间的超平面，原空间的非线性可分问题就变成了新空间的的线性可分问题。低维映射到高维。\n\n在核函数 $K(x,z)$ 给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，在学习和预测中只定义核函数 $K(x,z)$，而不需要显式地定义特征空间和映射函数$\\phi$，这样的技巧成为核技巧。通常直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。\n\n对于给定核 $K(x,z)$，特征空间和映射函数的取法并不唯一。\n\n### 3.5 手推SVM\n\n**参考资料**\n\n- [Support-vector machine](https://en.wikipedia.org/wiki/Support-vector_machine)\n- [支持向量机通俗导论（理解SVM的三层境界）](https://blog.csdn.net/v_july_v/article/details/7624837)\n- [数据挖掘（机器学习）面试--SVM面试常考问题](https://blog.csdn.net/szlcw1/article/details/52259668)\n- [机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM](http://cuijiahua.com/blog/2017/11/ml_8_svm_1.html)\n- [支持向量机（SVM）入门理解与推导](https://blog.csdn.net/sinat_20177327/article/details/79729551)\n- [数据挖掘领域十大经典算法之—SVM算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79483057)\n\n### 3.6 LR 与 SVM的区别和联系\n\n**相同点**\n\n第一，LR和SVM都是分类算法。\n\n看到这里很多人就不会认同了，因为在很大一部分人眼里，LR是回归算法。我是非常不赞同这一点的，因为我认为判断一个算法是分类还是回归算法的唯一标准就是样本label的类型，如果label是离散的，就是分类算法，如果label是连续的，就是回归算法。很明显，LR的训练数据的label是“0或者1”，当然是分类算法。其实这样不重要啦，暂且迁就我认为它是分类算法吧，再说了，SVM也可以回归用呢。\n\n第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。\n\n这里要先说明一点，那就是LR也是可以用核函数的，至于为什么通常在SVM中运用核函数而不在LR中运用，后面讲到他们之间区别的时候会重点分析。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，决策树和SVM什么区别，你说一个非线性分类器和一个线性分类器有什么区别？\n\n第三，LR和SVM都是监督学习算法。\n\n这个就不赘述什么是监督学习，什么是半监督学习，什么是非监督学习了。\n\n第四，LR和SVM都是判别模型。\n\n判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。当然，这也是为什么很少有人问你朴素贝叶斯和LR以及朴素贝叶斯和SVM有什么区别（哈哈，废话是不是太多）。\n\n**不同点**\n\n第一，本质上是其损失函数（loss function）不同。\n\n注：lr的损失函数是 cross entropy loss， adaboost的损失函数是 expotional loss ,svm是hinge loss，常见的回归模型通常用 均方误差 loss。\n\n不同的loss function代表了不同的假设前提，也就代表了不同的分类原理，也就代表了一切！！！简单来说，​逻辑回归方法基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值，具体细节参考[逻辑回归](http://blog.csdn.net/pakko/article/details/37878837)。支持向量机​基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面，具体细节参考[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/macyang/article/details/38782399)\n\n第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。\n\n当​你读完上面两个网址的内容，深入了解了LR和SVM的原理过后，会发现影响SVM决策面的样本点只有少数的结构支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。用下图进行说明：\n\n支持向量机改变非支持向量样本并不会引起决策面的变化\n\n逻辑回归中改变任何样本都会引起决策面的变化\n\n理解了这一点，有可能你会问，然后呢？有什么用呢？有什么意义吗？对使用两种算法有什么帮助么？一句话回答：\n\n因为上面的原因，得知：线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。​（引自http://www.zhihu.com/question/26768865/answer/34078149）\n\n第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。\n\n这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。​\n\n第四，​线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。（引自http://www.zhihu.com/question/26768865/answer/34078149）\n\n一个机遇概率，一个机遇距离！​\n\n第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！\n\n以前一直不理解为什么SVM叫做结构风险最小化算法，**所谓结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化**。未达到结构风险最小化的目的，最常用的方法就是添加正则项，后面的博客我会具体分析各种正则因子的不同，这里就不扯远了。但是，你发现没，SVM的目标函数里居然自带正则项！！！再看一下上面提到过的SVM目标函数：\n\nSVM目标函数\n\n有木有，那不就是L2正则项吗？\n\n不用多说了，如果不明白看看L1正则与L2正则吧，参考http://www.mamicode.com/info-detail-517504.html\n\nhttp://www.zhihu.com/question/26768865/answer/34078149\n\n**快速理解LR和SVM的区别**\n\n两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。但是逻辑回归相对来说模型更简单，好理解，实现起来，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些。但是SVM的理论基础更加牢固，有一套结构化风险最小化的理论基础，虽然一般使用的人不太会去关注。还有很重要的一点，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。\n\n**SVM与LR的区别与联系**\n\n联系：（1）分类（二分类） （2）可加入正则化项 \n\n区别：（1）LR–参数模型；SVM–非参数模型？（2）目标函数：LR—logistical loss；SVM–hinge loss （3）SVM–support vectors；LR–减少较远点的权重 （4）LR–模型简单，好理解，精度低，可能局部最优；SVM–理解、优化复杂，精度高，全局最优，转化为对偶问题—>简化模型和计算 （5）LR可以做的SVM可以做（线性可分），SVM能做的LR不一定能做（线性不可分）\n\n**总结一下**\n\n- Linear SVM和LR都是线性分类器\n- Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要对数据先做balancing。\n- Linear SVM依赖数据表打对距离测度，所以需要对数据先做normalization；LR不受影响\n- Linear SVM依赖penalty的系数，实验中需要做validation\n- Linear SVM的LR的performance都会收到outlier的影响，就敏感程度而言，无法给出明确结论。\n\n**参考资料**\n\n- [LR与SVM的异同](https://www.cnblogs.com/zhizhan/p/5038747.html)\n- [SVM和logistic回归分别在什么情况下使用？](<https://www.zhihu.com/question/21704547/answer/20293255>)\n- [Linear SVM 和 LR 有什么异同？](https://www.zhihu.com/question/26768865/answer/34078149)\n\n### SVM 中有哪些核函数？\n\n**核函数定义**：设$\\mathcal{X}$是输入空间，又设$\\mathcal{H}$为特征空间，如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射\n$$\n\\phi(x) : \\mathcal{X} \\rightarrow \\mathcal{H}\n$$\n使得对所有$x, z \\in \\mathcal{X}$，函数$K(x, z)$满足条件\n$$\nK(x, z)=\\phi(x) \\cdot \\phi(z)\n$$\n则称$K(x, z)$为核函数，$\\phi(x)$为映射函数，式中$\\phi(x) \\cdot \\phi(z)$$为\\phi(x)$和$\\phi(z)$的内积 \n\n**线性核函数**\n$$\nK(x,z) = x\\cdot z\n$$\n主要用于线性可分的情况。可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的。\n\n**多项式核函数**（polynomial kernel function）\n$$\nK(x, z)=(x \\cdot z+1)^{p}\n$$\n对应的支持向量机是一个p次多项式分类器。分类决策函数为\n$$\nf(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i}\\left(x_{i} \\cdot x+1\\right)^{p}+b^{*}\\right)\n$$\n多项式核函数可以实现将低维的输入空间映射到高维的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。\n\n**高斯核函数**（Gaussian kernel function） \n$$\nK(x,z) = exp(-\\frac{1}{2} \\ ||x - z ||_2 ) = \\phi(x) \\cdot \\phi(z)\n$$\n对应的支持向量机是高斯径向基函数（radial basis function）分类器，分类决策函数为\n$$\nf(x)=\\operatorname{sign}\\left(\\sum_{i=1}^{N_{s}} a_{i}^{*} y_{i} \\exp \\left(-\\frac{\\|x-x_i\\|^{2}}{2 \\sigma^{2}}\\right)+b^{*}\\right)\n$$\n高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。\n\n**Sigmod核函数**\n$$\nK\\left(x, z\\right)=\\tanh \\left(\\eta \\ x \\cdot z +\\theta\\right)\n$$\n总结\n\n- 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；\n  - （特征维度高，往往线性可分，SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中）\n- 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM\n- 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；\n\n\n\n**参考资料**\n\n- [svm常用核函数及选择核函数的方法](https://blog.csdn.net/ningyanggege/article/details/84072842)\n- [SVM由浅入深的尝试（五）核函数的理解](https://www.jianshu.com/p/e07932472257?utm_campaign)\n\n### SVM 的对偶问题\n\n- [ ] TODO\n\n### SMO 算法原理\n\n- [ ] TODO\n\nSVM 为什么可以处理非线性问题？\n\n- [ ] TODO\n\n### SVM 中的优化技术有哪些？\n\n- [ ] TODO\n\n### SVM 的惩罚系数如何确定？\n\n- [ ] TODO\n\n### 正则化参数对支持向量数的影响\n\n- [ ] TODO\n\n### 如何解决线性不可分问题？\n\n- [ ] TODO\n\n### 软间隔和硬间隔\n\n- [ ] TODO\n\n### Hinge Loss\n\n- [ ] TODO\n\n## 梯度提升树（GBDT）\n\n### 基本原理\n\n下面关于GBDT的理解来自论文greedy function approximation: a gradient boosting machine\n\n1. 损失函数的数值优化可以看成是在函数空间，而不是在参数空间。\n2. 损失函数L(y,F)包含平方损失(y−F)2，绝对值损失|y−F|用于回归问题，负二项对数似然log(1+e−2yF),y∈{-1,1}用于分类。\n3. 关注点是预测函数的加性扩展。\n\n最关键的点在于损失函数的数值优化可以看成是在函数空间而不是参数空间。\n\nGBDT对分类问题基学习器是二叉分类树，对回归问题基学习器是二叉决策树。\n\n**参考资料**\n\n- [简单易学的机器学习算法——梯度提升决策树GBDT](https://blog.csdn.net/google19890102/article/details/51746402/)\n\n- [GBDT原理详解](https://www.cnblogs.com/ScorpioLu/p/8296994.html)\n\n## AdaBoost\n\n### 基本原理\n\nAdaboost算法基本原理就是将多个弱分类器（弱分类器一般选用单层决策树）进行合理的结合，使其成为一个强分类器。\n\nAdaboost采用迭代的思想，每次迭代只训练一个弱分类器，训练好的弱分类器将参与下一次迭代的使用。也就是说，在第N次迭代中，一共就有N个弱分类器，其中N-1个是以前训练好的，其各种参数都不再改变，本次训练第N个分类器。其中弱分类器的关系是第N个弱分类器更可能分对前N-1个弱分类器没分对的数据，最终分类输出要看这N个分类器的综合效果。\n\n**参考资料**\n\n- [Adaboost入门教程——最通俗易懂的原理介绍（图文实例）](https://blog.csdn.net/px_528/article/details/72963977)\n- [AdaBoost原理详解](https://www.cnblogs.com/ScorpioLu/p/8295990.html)\n- [数据挖掘领域十大经典算法之—AdaBoost算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79482487)\n- [聊聊Adaboost，从理念到硬核推导](https://zhuanlan.zhihu.com/p/62037189)\n\n### GBDT 和 AdaBoost 区别\n\n- [ ] TODO\n\n## XGBoost\n\n### 基本原理\n\n**XGBoost全名叫（eXtreme Gradient Boosting）极端梯度提升**，经常被用在一些比赛中，其效果显著。它是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包。下面我们将XGBoost的学习分为3步：\n\n① 集成思想 \n\n② 损失函数分析 \n\n③ 求解\n\n我们知道机器学习三要素：模型、策略、算法。对于集成思想的介绍，XGBoost算法本身就是以集成思想为基础的。所以理解清楚集成学习方法对XGBoost是必要的，它能让我们更好的理解其预测函数模型。在第二部分，我们将详细分析损失函数，这就是我们将要介绍策略。第三部分，对于目标损失函数求解，也就是算法了。\n\n**参考资料**\n\n- [XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/)\n- [通俗、有逻辑的写一篇说下Xgboost的原理，供讨论参考](https://blog.csdn.net/github_38414650/article/details/76061893)\n- [xgboost的原理没你想像的那么难](https://www.jianshu.com/p/7467e616f227)\n\n### XGBoost里处理缺失值的方法\n\n- [ ] TODO\n\n### XGBoost 和 GBDT 的区别\n\n- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。\n\n- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。\n\n- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。\n\n- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）\n\n- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。\n\n- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。\n\n- xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\n\n- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n\n### XGBoost 如何做到自定义损失函数？\n\n- [ ] TODO\n\n### XGBoost 如何防止过拟合？\n\n- [ ] TODO\n\n### XGBoost 为什么不用后剪枝？\n\n- [ ] TODO\n\n### XGBoost 是如何实现并行的？\n\n- [ ] TODO\n\n### XGBoost 有哪些参数，取指范围，各代表什么意思？\n\n- [ ] TODO\n\n**参考资料**\n\n- [XGBoost——机器学习（理论+图解+安装方法+python代码）](https://blog.csdn.net/huacha__/article/details/81029680)\n- [一文读懂机器学习大杀器 XGBoost 原理](http://blog.itpub.net/31542119/viewspace-2199549/)  \n\n### XGBoost 如何进行并行加速的？\n\n- [ ] TODO\n\n### 每次分裂叶子节点是怎么决定特征和分裂点的？\n\n- [ ] TODO\n\n### Adaboost、GBDT与XGBoost的区别\n\n- [ ] TODO\n\n**参考资料**\n\n- [Adaboost、GBDT与XGBoost的区别](https://blog.csdn.net/hellozhxy/article/details/82143554)\n\n## LightGBM\n\n### 基本原理\n\n- [ ] TODO\n\n### LightGBM 与 XGBoost 的区别\n\n- [ ] TODO\n\n### GBDT、LightGBM 和 XGBoost 区别\n\n- [ ] TODO\n\n### 基本原理\n\n1、KNN算法概述\n\n　　kNN算法的核心思想是如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。该方法在确定分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 \n　　\n2、KNN算法介绍\n\n 　　最简单最初级的分类器是将全部的训练数据所对应的类别都记录下来，当测试对象的属性和某个训练对象的属性完全匹配时，便可以对其进行分类。但是怎么可能所有测试对象都会找到与之完全匹配的训练对象呢，其次就是存在一个测试对象同时与多个训练对象匹配，导致一个训练对象被分到了多个类的问题，基于这些问题呢，就产生了KNN。\n\nKNN是通过测量不同特征值之间的距离进行分类。它的的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\n\n下面通过一个简单的例子说明一下：如下图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。\n\n![](https://images0.cnblogs.com/blog2015/771535/201508/041623504236939.jpg)\n　\n接下来对KNN算法的思想总结一下：就是在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的前K个数据，则该测试数据对应的类别就是K个数据中出现次数最多的那个分类，其算法的描述为：\n\n1）计算测试数据与各个训练数据之间的距离；\n\n2）按照距离的递增关系进行排序；\n\n3）选取距离最小的K个点；\n\n4）确定前K个点所在类别的出现频率；\n\n5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。　\n\n**参考资料**\n\n- [KNN算法原理及实现](https://www.cnblogs.com/sxron/p/5451923.html)\n- [第2章 k-近邻算法](https://www.cnblogs.com/apachecnxy/p/7462628.html)\n- [数据挖掘领域十大经典算法之—K-邻近算法/kNN（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458648)\n\n## K-Means\n\n### 基本原理\n\n算法思想：\n\n```\n选择K个点作为初始质心  \nrepeat  \n    将每个点指派到最近的质心，形成K个簇  \n    重新计算每个簇的质心  \nuntil 簇不发生变化或达到最大迭代次数  \n```\n\n这里的重新计算每个簇的质心，如何计算的是根据目标函数得来的，因此在开始时我们要考虑距离度量和目标函数。\n\n考虑欧几里得距离的数据，使用误差平方和（Sum of the Squared Error,SSE）作为聚类的目标函数，两次运行K均值产生的两个不同的簇集，我们更喜欢SSE最小的那个。\n\n**参考资料**\n\n- [深入理解K-Means聚类算法](https://blog.csdn.net/taoyanqi8932/article/details/53727841)\n- [数据挖掘领域十大经典算法之—K-Means算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458331)\n\n### 手撕 K-Means\n\n- [ ] TODO\n\n### K-Means 与 KNN 的区别\n\n- [ ] TODO\n\n**参考资料**\n\n- [Kmeans算法与KNN算法的区别](https://www.cnblogs.com/peizhe123/p/4619066.html)\n\n### K-Means 中的 K 怎么确定？\n\n- [ ] TODO\n\n### K-Means 的迭代循环停止条件\n\n- [ ] TODO\n\n### 评判聚类效果准则\n\n- [ ] TODO\n\n## Bagging\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [集成算法中的Bagging](https://blog.csdn.net/fontthrone/article/details/79074296)\n\n## Boosting\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习 —— Boosting算法](https://blog.csdn.net/starter_____/article/details/79328749)\n\n### Bagging 和 Boosting 的区别\n\n1）样本选择上：\n\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.\n\nBoosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.\n\n2）样例权重：\n\nBagging：使用均匀取样，每个样例的权重相等\n\nBoosting：根据错误率不断调整样例的权值，错误率越大则权重越大.\n\n3）预测函数：\n\nBagging：所有预测函数的权重相等.\n\nBoosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.\n\n4）并行计算：\n\nBagging：各个预测函数可以并行生成\n\nBoosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.\n\n**参考资料**\n\n- [Bagging和Boosting的区别（面试准备）](https://www.cnblogs.com/earendil/p/8872001.html)\n\n- [Bagging和Boosting 概念及区别](https://www.cnblogs.com/liuwu265/p/4690486.html)\n- [Bagging和Boosting的概念与区别](https://www.cnblogs.com/onemorepoint/p/9264782.html)\n\n\n## 朴素贝叶斯\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [第4章 基于概率论的分类方法：朴素贝叶斯](https://www.cnblogs.com/apachecnxy/p/7471634.html)\n- [数据挖掘领域十大经典算法之—朴素贝叶斯算法（超详细附代码）](https://blog.csdn.net/fuqiuai/article/details/79458943)\n\n### 为什么朴素贝叶斯被称为“朴素”？\n\n- [ ] TODO\n\n## EM 算法\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [数据挖掘领域十大经典算法之—EM算法](https://blog.csdn.net/fuqiuai/article/details/79484421)\n\n### E 步和 M 步的具体步骤\n\n- [ ] TODO\n\n### E 中的期望是什么？\n\n- [ ] TODO\n\n## 决策树\n\n### 基本原理\n\n- [ ] TODO\n\n### 决策树如何剪枝？\n\n- [ ] TODO\n\n### 决策树先剪枝还是后剪枝好？\n\n- [ ] TODO\n\n### 决策树能否有非数值型变量？\n\n- [ ] TODO\n\n### 决策树如何防止过拟合？\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习之-常见决策树算法(ID3、C4.5、CART)](https://shuwoom.com/?p=1452)\n- [机器学习实战（三）——决策树](https://blog.csdn.net/jiaoyangwm/article/details/79525237)\n- [决策树基本概念及算法优缺点](https://www.jianshu.com/p/655d8e555494)\n\n### 决策树的ID3和C4.5介绍一下\n\n- [ ] TODO\n\n## 随机森林（RF）\n\n### 基本原理\n\n随机森林属于集成学习（Ensemble Learning）中的bagging算法。在集成学习中，主要分为bagging算法和boosting算法。我们先看看这两种方法的特点和区别。\n\n**Bagging（套袋法）**\n\nbagging的算法过程如下：\n\n从原始样本集中使用Bootstraping方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。（k个训练集之间相互独立，元素可以有重复）\n对于k个训练集，我们训练k个模型（这k个模型可以根据具体问题而定，比如决策树，knn等）\n对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。（所有模型的重要性相同）\n\n**Boosting（提升法）**\n\nboosting的算法过程如下：\n\n对于训练集中的每个样本建立权值wi，表示对每个样本的关注度。当某个样本被误分类的概率很高时，需要加大对该样本的权值。\n\n进行迭代的过程中，每一步迭代都是一个弱分类器。我们需要用某种策略将其组合，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。误差越小的弱分类器，权值越大）\nBagging，Boosting的主要区别\n\n样本选择上：Bagging采用的是Bootstrap随机有放回抽样；而Boosting每一轮的训练集是不变的，改变的只是每一个样本的权重。\n\n样本权重：Bagging使用的是均匀取样，每个样本权重相等；Boosting根据错误率调整样本权重，错误率越大的样本权重越大。\n\n预测函数：Bagging所有的预测函数的权重相等；Boosting中误差越小的预测函数其权重越大。\n\n并行计算：Bagging各个预测函数可以并行生成；Boosting各个预测函数必须按顺序迭代生成。\n\n下面是将决策树与这些算法框架进行结合所得到的新的算法：\n\n1）Bagging + 决策树 = 随机森林\n\n2）AdaBoost + 决策树 = 提升树\n\n3）Gradient Boosting + 决策树 = GBDT\n\n**决策树**\n\n常用的决策树算法有ID3，C4.5，CART三种。3种算法的模型构建思想都十分类似，只是采用了不同的指标。决策树模型的构建过程大致如下：\n\n**ID3，C4.5决策树的生成**\n\n输入：训练集D，特征集A，阈值eps 输出：决策树T\n\n若D中所有样本属于同一类Ck，则T为单节点树，将类Ck作为该结点的类标记，返回T\n\n若A为空集，即没有特征作为划分依据，则T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T\n否则，计算A中各特征对D的信息增益(ID3)/信息增益比(C4.5)，选择信息增益最大的特征Ag\n\n若Ag的信息增益（比）小于阈值eps，则置T为单节点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T\n否则，依照特征Ag将D划分为若干非空子集Di，将Di中实例数最大的类作为标记，构建子节点，由结点及其子节点构成树T，返回T\n\n对第i个子节点，以Di为训练集，以A-{Ag}为特征集，递归地调用1~5，得到子树Ti，返回Ti\n\n**CART决策树的生成**\n\n这里只简单介绍下CART与ID3和C4.5的区别。\n\nCART树是二叉树，而ID3和C4.5可以是多叉树\nCART在生成子树时，是选择一个特征一个取值作为切分点，生成两个子树\n选择特征和切分点的依据是基尼指数，选择基尼指数最小的特征及切分点生成子树\n\n**随机森林**\n\n随机森林是一种重要的基于Bagging的集成学习方法，可以用来做分类、回归等问题。\n\n随机森林有许多优点：\n\n- 具有极高的准确率\n- 随机性的引入，使得随机森林不容易过拟合\n- 随机性的引入，使得随机森林有很好的抗噪声能力\n- 能处理很高维度的数据，并且不用做特征选择\n- 既能处理离散型数据，也能处理连续型数据，数据集无需规范化\n- 训练速度快，可以得到变量重要性排序\n- 容易实现并行化\n\n**随机森林的缺点：**\n\n当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大\n\n随机森林模型还有许多不好解释的地方，有点算个黑盒模型\n\n与上面介绍的Bagging过程相似，随机森林的构建过程大致如下：\n\n从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集\n对于n_tree个训练集，我们分别训练n_tree个决策树模型\n\n对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂\n\n每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝\n将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果\n\n**参考资料**\n\n- [随机森林算法学习(Random Forest)](https://blog.csdn.net/qq547276542/article/details/78304454)\n- [随机森林（Random Forest）算法原理](https://blog.csdn.net/edogawachia/article/details/79357844)\n- [随机森林（Random Forest）](https://www.cnblogs.com/maybe2030/p/4585705.html)\n- [机器学习算法之随机森林算法详解及工作原理图解](http://www.elecfans.com/d/647463.html)\n\n### 随机森林中的“随机”指什么？\n\n- [ ] TODO\n\n### 随机森林处理缺失值的方法\n\n- [ ] TODO\n\n### 随机森林和 GBDT 的区别\n\n- [ ] TODO\n\n### 随机森林与决策树关系\n\n- [ ] TODO\n\n## CART回归树是怎么实现的？\n\n- [ ] TODO\n\n## CART分类树和ID3以及C4.5有什么区别？\n\n- [ ] TODO\n\n## 机器学习中的分类、回归和聚类模型有哪些？  \n\n分类：LR、SVM、KNN、决策树、RandomForest、GBDT  \n\n回归：non-Linear regression、SVR（支持向量回归-->可用线性或高斯核（RBF））、随机森林  \n\n聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类\n\n## 高斯混合模型（GMM）\n\n- [ ] TODO\n\n**参考资料**\n\n- [深度理解高斯混合模型（GMM）](http://blog.sina.com.cn/s/blog_a36a563e0102y2ec.html)\n- [高斯混合模型（GMM）](https://blog.csdn.net/m_buddy/article/details/80432384)\n\n## 马尔科夫随机场（MRF）\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [马尔可夫随机场 MRF](https://blog.csdn.net/pipisorry/article/details/78396503)\n\n## 隐马尔科夫模型（HMM）\n\n### 基本原理\n\n- [ ] TODO\n\n**参考资料**\n\n- [一文搞懂HMM（隐马尔可夫模型）](https://www.cnblogs.com/skyme/p/4651331.html)\n- [机器学习中的隐马尔科夫模型（HMM）详解](https://blog.csdn.net/baimafujinji/article/details/51285082)\n\n### 发射概率和状态转移概率\n\n- [ ] TODO\n\n### 每层要记住所有路径吗？\n\n- [ ] TODO\n\n## 条件随机场（CRF）\n\n### 基本原理\n\n- [ ] TODO\n\n### CRF 的损失函数是什么？\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习之条件随机场（CRF）](https://blog.csdn.net/wangyangzhizhou/article/details/78489593)\n- [如何轻松愉快地理解条件随机场（CRF）？](https://www.jianshu.com/p/55755fc649b1)\n- [一文理解条件随机场CRF](https://zhuanlan.zhihu.com/p/70067113)\n\n### HMM、MEMM vs CRF 对比？\n\n1）HMM是有向图模型，是生成模型；HMM有两个假设：一阶马尔科夫假设和观测独立性假设；但对于序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。\n\n2）MEMM（最大熵马尔科夫模型）是有向图模型，是判别模型；MEMM打破了HMM的观测独立性假设，MEMM考虑到相邻状态之间依赖关系，且考虑整个观察序列，因此MEMM的表达能力更强；但MEMM会带来标注偏置问题：由于局部归一化问题，MEMM倾向于选择拥有更少转移的状态。这就是标记偏置问题。\n\n![img](https://pic3.zhimg.com/80/v2-7a5e998530e0d9c5f146d27603e6e496_hd.jpg)最大熵模型（MEMM）\n\n![img](https://pic3.zhimg.com/80/v2-610ca7a9b504936bfba136c464ebe81a_hd.jpg)\n\n3）CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应得也变复杂了。\n\nHMM、MEMM和CRF的优缺点比较：\n\na）与HMM比较。CRF没有HMM那样严格的独立性假设条件，因而可以容纳任意的上下文信息。特征设计灵活（与ME一样）\n\nb）与MEMM比较。由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔可夫模型标记偏置（Label-bias）的缺点。\n\nc）与ME比较。CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，定义下一个状态的状态分布.\n\n> 首先，CRF，HMM(隐马模型)，MEMM(最大熵隐马模型)都常用来做序列标注的建模，像分词、词性标注，以及命名实体标注\n> 隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择\n> 最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。\n> 条件随机场则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。\n\n**参考资料**\n\n- [HMM、MEMM vs CRF 对比？](https://zhuanlan.zhihu.com/p/57153934)\n\n## 主成分分析（PCA）\n\n### 基本原理\n\nPCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。\n\n当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。\n\n**参考资料**\n\n- [PCA的数学原理](https://www.cnblogs.com/mikewolf2002/p/3429711.html)\n- [PCA的数学原理(转)](https://zhuanlan.zhihu.com/p/21580949)\n- [第13章 利用 PCA 来简化数据](https://www.cnblogs.com/apachecnxy/p/7640976.html)\n- [PCA（主成分分析）原理推导](https://zhuanlan.zhihu.com/p/84946694)\n\n## 线性判别分析（LDA）\n\nTODO\n\n**参考资料**\n\n- [线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)\n- [LDA原理小结](https://blog.csdn.net/qq_16137569/article/details/82385050)\n\n## 奇异值分解（SVD）\n\n### 基本原理\n\n- [ ] TODO \n\n**参考资料**\n\n- [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n- [关于SVD(Singular Value Decomposition)的那些事儿](https://www.cnblogs.com/tgycoder/p/6266786.html)\n- [奇异值分解(SVD)原理与在降维中的应用](https://www.cnblogs.com/pinard/p/6251584.html)\n- [第14章 利用SVD简化数据](https://www.cnblogs.com/apachecnxy/p/7640987.html)\n\n### 手撕 SVD\n\n- [ ] TODO\n\n### 特征值和SVD的区别\n\n- [ ] TODO\n\n**参考资料**\n\n- [特征值和奇异值（svd）](https://blog.csdn.net/u012380663/article/details/36629951)\n\n## 凸优化\n\n### 基本原理\n\n- [ ] TODO \n\n**参考资料**\n\n- [关于凸优化的一些简单概念](https://www.cnblogs.com/harvey888/p/7100815.html)\n- [最优化理论与凸优化到底是干嘛的？](https://blog.csdn.net/qq_39422642/article/details/78816637)\n- [深度学习/机器学习入门基础数学知识整理（三）：凸优化，Hessian，牛顿法](https://blog.csdn.net/xbinworld/article/details/79113218)\n\n## 神经网络\n\n- [ ] TODO \n\n注：建议放在深度学习中介绍，这里可以简单介绍\n\n## Accuracy、Precision、Recall和 F1 Score\n\n在学习机器学习、深度学习，甚至做自己项目的时候，经过看到上述名词。然而因为名词容易搞混，所以经常会忘记相关的含义。\n\n这里做一次最全最清晰的介绍，若之后再次忘记相关知识点，本文可以帮助快速回顾。\n\n首先，列出一个清单：\n\n- TP（true positive，真正）: 预测为正，实际为正\n\n- FP（false positive，假正）: 预测为正，实际为负\n\n- TN（true negative，真负）：预测为负，实际为负\n\n- FN（false negative，假负）: 预测为负，实际为正\n\n- ACC（accuracy，准确率）：ACC = (TP+TN)/(TP+TN+FN+FP)\n\n- P（precision精确率、精准率、查准率P = TP/ (TP+FP)\n\n- R（recall，召回率、查全率）： R = TP/ (TP+FN)\n\n- TPR（true positive rate，，真正类率同召回率、查全率）：TPR = TP/ (TP+FN)\n\n  注：Recall = TPR\n\n- FPR（false positive rate，假正类率）：FPR =FP/ (FP+TN)\n\n- F-Score: F-Score = (1+β^2) x (PxR) / (β^2x(P+R)) = 2xTP/(2xTP + FP + FN)\n\n- 当β=1是，F1-score = 2xPxR/(P+R)\n\n- P-R曲线（precision-recall，查准率-查全率曲线）\n\n- ROC曲线（receiver operating characteristic，接收者操作特征曲线）\n\n- AUC（area under curve）值\n\n中文博大精深，为了不搞混，下面统一用英文全称或简称作为名词标识。\n\n\n\n正式介绍一下前四个名词：\n\n**True positives（TP，真正）** : 预测为正，实际为正\n\n**True negatives（TN，真负）**：预测为负，实际为负\n\n**False positives（FP，假正**）: 预测为正，实际为负 \n\n**False negatives（FN，假负）**: 预测为负，实际为正\n\n为了更好的理解，这里二元分类问题的例子：\n\n假设，我们要对某一封邮件做出一个判定，判定这封邮件是垃圾邮件、还是这封邮件不是垃圾邮件？\n\n如果判定是垃圾邮件，那就是做出（Positive）的判定； \n\n如果判定不是垃圾邮件，那就做出（Negative）的判定。\n\nTrue Positive（TP）意思表示做出Positive的判定，而且判定是正确的。\n\n因此，TP的数值表示正确的Positive判定的个数。 \n\n同理，False Positive（TP）数值表示错误的Positive判定的个数。 \n\n依此，True Negative（TN）数值表示正确的Negative判定个数。 \n\nFalse Negative（FN）数值表示错误的Negative判定个数。\n\n**TPR、FPR和TNR**\n\n**TPR（true positive rate，真正类率）**\n\nTPR = TP/(TP+FN)\n\n真正类率TPR代表分类器预测的正类中实际正实例占所有正实例的比例。\n\n\n\n**FPR（false positive rate，假正类率）**\n\nFPR = FP/(FP+TN)\n\n假正类率FPR代表分类器预测的正类中实际负实例占所有负实例的比例。\n\n**TNR（ture negative rate，真负类率）**\n\nTNR = TN/(FP+TN)\n\n真负类率TNR代表分类器预测的负类中实际负实例占所有负实例的比例。\n\n**Accuracy**\n\n准确率（accuracy，ACC）\n\nACC = (TP+TN)/(TP+TN+FN+FP)\n\n**Precision & Recall**\n\n[Precision精确率](https://en.wikipedia.org/wiki/Precision_and_recall)：\n\nP = TP/(TP+FP)\n\n表示当前划分到正样本类别中，被正确分类的比例（正确正样本所占比例）。\n\n[Recall召回率](https://en.wikipedia.org/wiki/Precision_and_recall)：\n\nR = TP/(TP+FN)\n\n表示当前划分到正样本类别中，真实正样本占所有正样本的比例。\n\n**F-Score**\n\nF-Score 是精确率Precision和召回率Recall的加权调和平均值。该值是为了综合衡量Precision和Recall而设定的。\n\nF-Score = (1+β^2) x (PxR) / (β^2x(P+R)) = 2xTP/(2xTP + FP + FN)\n\n当β=1时，F1-score = 2xPxR/(P+R)。这时，Precision和Recall都很重要，权重相同。\n\n当有些情况下，我们认为Precision更重要，那就调整β的值小于1；如果我们认为Recall更加重要，那就调整β的值大于1。\n\n一般来说，当F-Score或F1-score较高\n\n**P-R曲线**\n\n\n\n**ROC曲线**\n\n横轴：负正类率(false postive rate FPR)\n\n纵轴：真正类率(true postive rate TPR)\n\n\n\n![ROC Curve](https://upload-images.jianshu.io/upload_images/2394427-5f11fd1e6af07393?imageMogr2/auto-orient/strip%7CimageView2/2/w/700)\n\n**AUC值**\n\n上面都是理论，看起来很迷糊，这里举个真实应用的实例，加强理解。\n\n对于那些不熟悉的人，我将解释精确度和召回率，对于那些熟悉的人，我将在比较精确召回曲线时解释文献中的一些混淆。\n\n下面从图像分类的角度举个例子：\n\n假设现在有这样一个测试集，测试集中的图片只由大雁和飞机两种图片组成，如下图所示： \n\n![](https://sanchom.files.wordpress.com/2011/08/collection.png)\n\n假设你的分类系统最终的目的是：能取出测试集中所有飞机的图片，而不是大雁的图片。\n\n现在做如下的定义： \n\nTrue positives（TP，真正） : 飞机的图片被正确的识别成了飞机。 \n\nTrue negatives（TN，真负）: 大雁的图片没有被识别出来，系统正确地认为它们是大雁。 \n\nFalse positives（FP，假正）: 大雁的图片被错误地识别成了飞机。 \n\nFalse negatives（FN，假负）: 飞机的图片没有被识别出来，系统错误地认为它们是大雁。\n\n![Precision and recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/440px-Precisionrecall.svg.png)\n\n**实战**\n\n```python\n'''In binary classification settings'''\n\n######### Create simple data ##########\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\n\n\n######## Compute the average precision score ######## \n\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\n\t  \n\n######## Plot the Precision-Recall curve   ######\n\t  \nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\n\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='b')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n          average_precision))\nplt.show()\n```\n\n**参考资料**\n\n- [Accuracy, Precision, Recall & F1 Score: Interpretation of Performance Measures](http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)\n\n- [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n\n- [average precision](https://sanchom.wordpress.com/tag/average-precision/)\n\n- [Precision-Recall](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)\n\n- [【YOLO学习】召回率（Recall），精确率（Precision），平均正确率（Average_precision(AP) ），交除并（Intersection-over-Union（IoU））](https://blog.csdn.net/hysteric314/article/details/54093734)\n\n- [Precision，Recall，F1score，Accuracy的理解](https://blog.csdn.net/u014380165/article/details/77493978)\n\n- [ROC、Precision、Recall、TPR、FPR理解](https://www.jianshu.com/p/be2e037900a1)\n\n- [推荐系统评测指标—准确率(Precision)、召回率(Recall)、F值(F-Measure) ](http://bookshadow.com/weblog/2014/06/10/precision-recall-f-measure/)\n\n- [机器学习之分类器性能指标之ROC曲线、AUC值](http://www.cnblogs.com/dlml/p/4403482.html)\n\n## 正则化方法\n\n- L1 范数\n- L2 范数\n- 数据集增广\n\n- Dropout\n- Batch Normaliztion\n\n**参考资料**\n\n- [[Deep Learning\\] 正则化](https://www.cnblogs.com/maybe2030/p/9231231.html)\n\n### L1和L2正则化\n\n目的：降低损失函数\n\n机器学习中几乎都可以看到损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。\n\nL1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制。对于线性回归模型，使用L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）。下图是Python中Lasso回归的损失函数，式中加号后面一项α||w||1即为L1正则化项。\n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0003.png)\n\n下图是Python中Ridge回归的损失函数，式中加号后面一项即为L2正则化项。\n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0004.png)\n\n一般回归分析中回归w表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。L1正则化和L2正则化的说明如下：\n\n- L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为||w||1\n- L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2\n  一般都会在正则化项之前添加一个系数，Python中用α表示，一些文章也用λ表示。这个系数需要用户指定。\n\n那添加L1和L2正则化有什么用？下面是L1正则化和L2正则化的作用，这些表述可以在很多文章中找到。\n\n- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择\n- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\n**稀疏模型与特征选择**\n\n上面提到L1正则化有助于生成一个稀疏权值矩阵，进而可以用于特征选择。为什么要生成一个稀疏矩阵？\n\n稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。\n\n**L1和L2正则化的直观理解**\n\n这部分内容将解释为什么L1正则化可以产生稀疏模型（L1是怎么让系数等于零的），以及为什么L2正则化可以防止过拟合。\n\nL1正则化和特征选择\n假设有如下带L1正则化的损失函数：\n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0010.png)\n\n其中J0是原始的损失函数，加号后面的一项是L1正则化项，α是正则化系数。注意到L1正则化是权值的绝对值之和，J是带有绝对值符号的函数，因此J是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数J0后添加L1正则化项时，相当于对J0做了一个约束。令L=α∑w|w|，则J=J0+L，此时我们的任务变成在L约束下求出J0取最小值的解。考虑二维的情况，即只有两个权值w1和w2，此时L=|w1|+|w2|对于梯度下降法，求解J0的过程可以画出等值线，同时L1正则化的函数L也可以在w1w2的二维平面上画出来。如下图：\n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0005.png)\n图1 L1正则化\n\n图中等值线是J0的等值线，黑色方形是L函数的图形。在图中，当J0等值线与L图形首次相交的地方就是最优解。上图中J0与L在L的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(w1,w2)=(0,w)。可以直观想象，因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），J0与这些角接触的机率会远大于与L其它部位接触的机率，而在这些角上，会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。\n\n而正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大（上图中的黑色方框）；α越大，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(w1,w2)=(0,w)中的w可以取到很小的值。\n\n类似，假设有如下带L2正则化的损失函数： \n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0011.png)\n\n同样可以画出它们在二维平面上的图形，如下：\n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0006.png)\n图2 L2正则化\n\n二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。\n\n注：以二维平面举例，借助可视化L1和L2，可知L1正则化具有稀疏性。\n\n**L2正则化和过拟合**\n\n拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。\n\n那为什么L2正则化可以获得值很小的参数？\n\n以线性回归中的梯度下降法为例。假设要求的参数为θ，hθ(x)是我们的假设函数，那么线性回归的代价函数如下： \n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0007.png)\n\n那么在梯度下降法中，最终用于迭代计算参数θ的迭代式为： \n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0008.png)\n\n其中α是learning rate. 上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面的样子： \n\n![Logistic Regression.png](F:/Projects/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E5%AE%9D%E5%85%B8%E8%B5%84%E6%96%99%E5%A4%A7%E5%85%A8/Deep-Learning-Interview-Book/docs/面试/DLIB-0009.png)\n\n其中λ就是正则化参数。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。\n最开始也提到L1正则化一定程度上也可以防止过拟合。之前做了解释，当L1的正则化系数很小时，得到的最优解会很小，可以达到和L2正则化类似的效果。\n\nL2正则化参数\n\n从上述公式可以看到，λ越大，θj衰减得越快。另一个理解可以参考图2，λ越大，L2圆的半径越小，最后求得代价函数最值时各参数也会变得很小。\n\n### L1 和 L2 正则化的区别\n\n- [ ] TODO\n\n### 机器学习中常常提到的正则化到底是什么意思？\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)\n\n### 为什么 L2 正则化可以防止过拟合？\n\n- [ ] TODO\n\n## 过拟合和欠拟合\n\n### 基本原理\n\n**过拟合（Over-Fitting）**\n\n高方差\n\n在训练集上误差小，但在测试集上误差大，我们将这种情况称为高方差（high variance），也叫过拟合。\n\n**欠拟合（Under-Fitting）**\n\n在训练集上训练效果不好（测试集上也不好），准确率不高，我们将这种情况称为高偏差（high bias），也叫欠拟合。\n\n![](https://testerhome.com/uploads/photo/2017/ba5ebeb8-1af7-4dfa-aeba-6bb36c056aff.png!large)\n\n\n\n### 如何防止过拟合？\n\n- 数据增广（Data Augmentation）\n- 正则化（L0正则、L1正则和L2正则），也叫限制权值Weight-decay\n- Dropout\n- Early Stopping\n- 简化模型\n- 增加噪声\n- Bagging\n- 贝叶斯方法\n- 决策树剪枝\n- 集成方法，随机森林\n- Batch Normalization\n\n### 如何防止欠拟合？\n\n- 添加新特征\n- 添加多项式特征\n- 减少正则化参数\n- 增加网络复杂度\n- 使用集成学习方法，如Bagging\n\n## 精确率（Precision）和召回率（Recall）\n\n对于二分类问题常用的评价指标是精确率和召回率。通常以关注的类为正类，其他类为负类，分类器在数据集上的预测或者正确或者不正确，我们有4中情况，在混淆矩阵中表示如下：\n\n![img](C:\\Users\\morni\\AppData\\Local\\YNote\\data\\lixin4009@163.com\\531a7b4df00e4f89acad101c07d030ab\\9-1294986450.png)\n\n- [ ] 精确率$Precision = \\frac{TP}{TP+FP} $, 那么有误拦率为$ \\frac{FP}{TP+FP} $\n- [ ] 召回率$ Recall = TP / (TP + FN)$ \n\n精确率表示我现在有了这么的预测为正的样本，那么这些样本中有多少是真的为正呢？\n\n召回率表示我现在预测为正的这些值中，占了所有的正的为正的样本的多大比例呢？\n\n**参考资料**\n\n- [如何解释召回率与精确率？](https://www.zhihu.com/question/19645541)\n- [分类--精确率和召回率](https://www.cnblogs.com/taro/p/8643335.html)\n\n## AUC 和 ROC\n\n### 基本原理\n\n- [ ] TODO\n\n### 如何绘制ROC曲线？\n\n- [ ] TODO\n\n### ROC曲线下面积表示什么？\n\n- [ ] TODO\n\n### 手撕 AUC 曲面面积代码\n\n- [ ] TODO\n\n## 梯度弥散和梯度爆炸\n\n- [ ] TODO\n\n## 什么是参数范数惩罚？\n\n- [ ] TODO\n\n## 为什么要进行归一化？优点？\n\n- [ ] TODO\n\n## CCA和PCA的区别\n\n- [ ] TODO\n\n## Softmax\n\n### 基本原理\n\n- [ ] TODO\n\n### Softmax是和什么loss function配合使用？\n\n- [ ] TODO\n\n###  Softmax代码实现\n\n- [ ] TODO\n\n## 交叉熵损失函数\n\n- [ ] TODO\n\n## 通俗解释一下信息熵\n\n- [ ] TODO\n\n## 如何加快梯度下降收敛速度？\n\n- [ ] TODO\n\n## 如何解决正负样本数量不均衡？\n\n- [ ] TODO\n\n## 如何解决异常值问题？\n\n- [ ] TODO\n\n## 机器学习中使用正则化来防止过拟合是什么原理？\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习中使用正则化来防止过拟合是什么原理？](https://www.zhihu.com/question/20700829)\n\n## 机器学习中常常提到的正则化到底是什么意思？\n\n- [ ] TODO\n\n**参考资料**\n\n- [机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)\n\n## 梯度下降陷入局部最优有什么解决办法\n\n- [ ] TODO\n\n## 聚类算法中的距离度量有哪些？\n\n- [ ] TODO\n\n常见的距离度量：欧氏距离、曼哈顿距离、夹角余弦、切比雪夫距离、汉明距离\n\n## KL 散度和交叉熵的区别\n\n- [ ] TODO\n\n## 降维的方法都有哪些？\n\n- [ ] TODO\n\n## TODO\n\n"},{"title":"FM","url":"/2021/03/03/FM/","content":"#### 1  $\\ FM\\ $算法介绍\n\n在传统的线性模型中，各个特征之间都是独立考虑的，并没有涉及到特征与特征之间的交互关系，但实际上大量的特征之间是相互关联的。如何寻找相互关联的特征，基于上述思想$\\ FM\\ $算法应运而生。传统的线性模型为\n$$\ny=w_0+\\sum\\limits_{i=1}^nw_ix_i\n$$\n在传统的线性模型的基础上中引入特征交叉项可得\n$$\ny=w_0+\\sum\\limits_{i=1}^nw_ix_i+\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=i+1}^nw_{ij}x_ix_j\n$$\n在数据非常稀疏的情况下很难满足$\\ x_i、x_j\\ $都不为$\\ 0\\ $，这样将会导致$\\ w_{ij}\\ $不能够通过训练得到，因此无法进行相应的参数估计。可以发现参数矩阵$\\ w \\ $是一个实对称矩阵，$\\ w_{ij}\\ $可以使用矩阵分解的方法求解，通过引入辅助向量$\\  V\\ $\n$$\n\\begin{aligned}\nV=\n\\begin{bmatrix}\nv_{11} & v_{12} & v_{13} & \\cdots & v_{1k} \\\\\nv_{21} & v_{22} & v_{23} & \\cdots & v_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_{n1} & v_{n2} & v_{n3} & \\cdots & v_{nk}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{v}_1\\\\\n\\mathbf{v}_2\\\\\n\\vdots\\\\\n\\mathbf{v}_n\n\\end{bmatrix}\n\\end{aligned}\n$$\n然后用$\\ w_{ij}=\\mathbf{v}_i\\mathbf{v}_j^T \\ $对$\\ w\\  $进行分解\n$$\nw=VV^T=\n\\begin{bmatrix}\n\\mathbf{v}_1\\\\\n\\mathbf{v}_2\\\\\n\\vdots\\\\\n\\mathbf{v}_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{v}_1^T &\n\\mathbf{v}_2^T & \n\\cdots & \n\\mathbf{v}_n^T\n\\end{bmatrix}\n$$\n综上可以发现原始模型的二项式参数为$\\ \\frac{n(n-1)}{2}\\ $个，现在减少为$\\ kn(k\\ll n)\\ $个。引入辅助向量$\\ V\\ $最为重要的一点是使得$\\ x_tx_i\\ $和$\\ x_ix_j\\ $的参数不再相互独立，这样就能够在样本数据稀疏的情况下合理的估计模型交叉项的参数\n$$\n\\begin{aligned}\n\\langle\\mathbf{v}_t,\\mathbf{v}_i\\rangle&=\\sum\\limits_{f=1}^k\\mathbf{v}_{tf}\\cdot\\mathbf{v}_{if}\\\\\n\\langle\\mathbf{v}_i,\\mathbf{v}_j\\rangle&=\\sum\\limits_{f=1}^k\\mathbf{v}_{if}\\cdot\\mathbf{v}_{jf}\n\\end{aligned}\n$$\n$\\ x_tx_i\\ $和$\\ x_ix_j\\  $的参数分别为$\\ \\langle\\mathbf{v}_{t},\\mathbf{v}_i \\rangle \\ $和$\\ \\langle\\mathbf{v}_{i},\\mathbf{v}_j \\rangle \\ $，它们之间拥有共同项$\\ \\mathbf{v}_i\\ $，即所有包含$\\ \\mathbf{v}_i\\ $的非零组合特征的样本都可以用来学习隐向量$\\  \\mathbf{v}_i\\ $，而原始模型中$\\ w_{ti}\\ $和$\\ w_{ij}\\ $却是相互独立的，这在很大程度上避免了数据稀疏造成的参数估计不准确的影响。因此原始模型可以改写为最终的$\\ FM\\ $算法\n$$\ny=w_0+\\sum\\limits_{i=1}^nw_ix_i+\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=i+1}^n \\langle\\mathbf{v}_i,\\mathbf{v}_j \\rangle x_ix_j\n$$\n由于求解上述式子的时间复杂度为$\\ \\mathcal{O}(n^2)\\ $，可以看出主要是最后一项计算比较复杂，因此从数学上对该式最后一项进行一些改写可以把时间复杂度降为$\\ \\mathcal{O}(kn)\\ $\n$$\n\\begin{equation}\n\\begin{aligned} & \\sum_{i=1}^{n} \\sum_{j=i+1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{j}\\right\\rangle x_{i} x_{j} \\\\=& \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{j}\\right\\rangle x_{i} x_{j}-\\frac{1}{2} \\sum_{i=1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{i}\\right\\rangle x_{i} x_{i} \\\\=& \\frac{1}{2}\\left(\\sum_{i=1}^{n} \\sum_{j=1}^{n} \\sum_{f=1}^{k} \\mathbf{v}_{if} \\mathbf{v}_{jf} x_{i} x_{j}-\\sum_{i=1}^{n} \\sum_{f=1}^{k} \\mathbf{v}_{if} \\mathbf{v}_{if} x_{i} x_{i}\\right) \\\\=& \\frac{1}{2} \\sum_{f=1}^{k}\\left(\\left(\\sum_{i=1}^{n} \\mathbf{v}_{if} x_{i}\\right)\\left(\\sum_{j=1}^{n} \\mathbf{v}_{jf} x_{j}\\right)-\\sum_{i=1}^{n} \\mathbf{v}_{if}^{2} x_{i}^{2}\\right) \\\\=& \\frac{1}{2} \\sum_{f=1}^{k}\\left(\\left(\\sum_{i=1}^{n} \\mathbf{v}_{if} x_{i}\\right)^{2}-\\sum_{i=1}^{n} \\mathbf{v}_{if}^{2} x_{i}^{2}\\right) \\end{aligned}\n\\end{equation}\n$$\n第一步：首先我们知道$V$是一个对角阵，假设$A=\\sum_{i=1}^{n} \\sum_{j=i+1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{j}\\right\\rangle x_{i} x_{j}$代表上三角元素之和（这里$A$不包括对角线元素），$B=\\frac{1}{2} \\sum_{i=1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{i}\\right\\rangle x_{i} x_{i}$表示对角线元素之和，同时整个矩阵之和可表示为$C=\\sum_{i=1}^{n} \\sum_{j=1}^{n}\\left\\langle\\mathbf{v}_{i}, \\mathbf{v}_{j}\\right\\rangle x_{i} x_{j}$. 根据对角线的性质可以得到$C= 2（A+B)$，那么 $A= C/2 - B/2$。\n\n在训练 FM 时，也可以和 LR 算法一样可以利用 SGD（随机梯度下降）来求解参数，各个参数的梯度如下：\n$$\n\\dfrac{d}{d\\theta} \\hat{y}(x) =\n\\begin{cases}\n1  &  \\theta :w_0 \\\\\nx_i & \\theta :w_i \\\\\nx_i\\sum_{j=1}^{n}v_{j,f}x_j-v_{i,f}x_i^2 & \\theta :v_{i,f}\n\\end{cases}\n$$\n模型中，$\\sum_{j=1}^{n}v_{j,f}x_j$只和$f$有关，在每次迭代中，只需要计算一次就可以得到所有的梯度。原本FM的复杂度为$O(kn^2)$,通过上面等式的变换将其二次项简化为$v_{i,f}$有关等式，模型复杂度降为$O(kn).$\n\n#### 2 算法优缺点\n\n- $\\ FM\\ $算法降低了因数据稀疏，导致特征交叉项参数学习不充分的影响；\n- $\\ FM\\ $算法提升了参数学习效率和模型预估的能力。\n\n#### 3 代码实现：\n\nscala代码实现：\n\nhttps://github.com/StringsLi/ml_scratch_scala/tree/master/src/main/scala/com/strings/model/ctr/fm"},{"title":"svm","url":"/2021/03/01/svm/","content":"​      **支持向量机**，英文全名为**Support Vector Machine**(SVM)，和感知机算法一样它通常用于二分类样本数据的分类，不同之处在于感知机算法要求样本数据线性可分否则算法不收敛，而支持向量机算法通过引入核函数巧妙的解决了这个问题。支持向量机的基本思想是在样本数据中训练出一个间隔最大化的线性分类器，其学习策略为间隔最大化，最终转化为求解一个凸二次规划问题\n\n#### 1. 支持向量机的目标函数\n\n在样本空间中，划分超平面可以通过如下线性方程描述：\n$$\nw^Tx + b = 0 \\label{1}\n$$\n其中，$w = (w_1;w_2;...;w_d)$为法向量，决定了超平面的方向；b为位移项，决定了超平面与原点之间的距离。显然，划分超平面可以被法向量$w$和$b$确定，下面我们将其记为$(w,b)$. 样本空间中任意点$x$到超平面$(w,b)$的距离可写为：\n$$\nr=\\dfrac{\\vert w^Tx+b\\vert}{\\Vert w\\Vert} \\label{2}\n$$\n假设超平面$(w,b)$能将训练样本正确分类，即对于$(x_i,y_i) \\in D$,若$y_i=+1$,则有$w^Tx_i + b > 0$;若$y_i=-1$,则有$w^Tx_i + b < 0$,令\n$$\n\\begin{aligned}\n&w^Tx_{i}+b\\geqslant 1,  & \\forall y_{i}=1\\\\\n& w^Tx_{i}+b\\leqslant -1,  & \\forall y_{i}=-1 \n\\end{aligned}\n$$\n如下图所示，距离超平面最近的这几个训练样本点使得上式等号成立，他们被称为“支撑向量（Support Vector)”,两个异类支撑向量到超平面的距离之和为：\n$$\nr=\\frac{2}{\\Vert w\\Vert}\n$$\n![](./svm/svm_间隔.png)\n\n欲找到具有“最大间隔（maximum margin）”的划分超平面，也就是要找到满足2式约束的参数$w$和$b$,故上述模型可以写成\n$$\n\\begin{aligned}\n&\\mathop{\\max}_{w,b}\\ \\dfrac{2}{\\Vert w\\Vert}\\\\\n&s.t\\ \\ \\ \\ \\ y_{i}(w ^Tx_{i}+b)\\geqslant 1\n\\end{aligned}\n$$\n即\n$$\n\\begin{aligned}\n&\\mathop{\\min}_{w,b}\\ \\dfrac{1}{2}\\Vert w\\Vert^2\\\\\n&s.t\\ \\ \\ \\ \\ y_{i}(w ^Tx_{i}+b)\\geqslant 1\n\\end{aligned}\n$$\n这个就是**支撑向量机的基本型。**\n\n#### 2. 支持向量机算法的对偶问题\n\n​\t回到支持向量机的优化模型\n$$\n\\begin{aligned}\n&\\mathop{\\min}_{w,b}\\ \\dfrac{1}{2}\\Vert w\\Vert^2\\\\\n&s.t\\ \\ \\ \\ \\ y_{i}(w ^Tx_{i}+b)\\geqslant 1\n\\end{aligned}\n$$\n由于该模型满足$\\ KKT \\ $条件，因此可以通过拉格朗日乘数法将有约束的优化目标转化为无约束的优化函数\n$$\n\\begin{aligned}\n\\mathcal{L}(\\omega,b,\\alpha)&=\\dfrac{1}{2}\\Vert w\\Vert^2-\\sum\\limits_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1]\\\\\ns.t.\\ \\ &\\alpha_i \\geqslant0\n\\end{aligned}\n$$\n因此优化目标转化为\n$$\n\\begin{aligned}\n\\mathop{\\min}_{w,b} \\mathop{\\max}_{\\alpha_i\\geqslant0} \\mathcal{L}(w,b,\\alpha)\n\\end{aligned}\n$$\n该问题的对偶问题为\n$$\n\\begin{aligned}\n\\mathop{\\max}_{\\alpha_i\\geqslant0} \\mathop{\\min}_{w,b}  \\mathcal{L}(w,b,\\alpha)\n\\end{aligned}\n$$\n从上式中可以观察到先求优化函数对于$\\ \\omega,b\\ $的极小值，然后再求拉格朗日乘子$\\ \\alpha_i\\ $的极大值。首先为了求$\\ \\begin{aligned}\\mathop{\\min}_{\\omega,b}  \\mathcal{L}(\\omega,b,\\alpha)\\end{aligned}\\ $,可以通过对$\\ \\omega,b\\ $分别求偏导数令其为$\\ 0\\ $得到\n$$\n\\begin{aligned}\n&\\nabla_w\\mathcal{L}(w,b,\\alpha)=0\\Rightarrow w=\\sum\\limits_{i=1}^m\\alpha_iy^{(i)}x^{(i)}\\\\\n&\\nabla_b\\mathcal{L}(w,b,\\alpha)=0\\Rightarrow \\sum\\limits_{i=1}^m\\alpha_iy^{(i)}=0\n\n\\end{aligned}\n$$\n上面已经求出来的$\\ \\omega,\\alpha\\ $的关系，可以带入$\\ \\mathcal{L}(\\omega,b,\\alpha) \\ $消去$\\ \\omega\\ $，令$\\ \\phi(\\alpha)=\\begin{aligned}\\mathop{\\min}_{\\omega,b}  \\mathcal{L}(\\omega,b,\\alpha)\\end{aligned}\\ $则\n$$\n\\begin{aligned}\n\\phi(\\alpha)&=\\dfrac{1}{2}\\Vert w\\Vert_2^2-\\sum\\limits_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1]\\\\\n\n&=\\sum\\limits_{i=1}^m\\alpha_i-\\dfrac{1}{2}\\sum\\limits_{i=1,j=1}^m\\alpha_i\\alpha_jy^{(i)}y^{(j)}(x^{(i)})^Tx^{(j)}\\\\\n\n\\end{aligned}\n$$\n对$\\ \\phi(\\alpha)\\ $极大化的数学表达式如下：\n$$\n\\begin{aligned}\n&\\mathop{\\max}_{\\alpha}\\ \\sum\\limits_{i=1}^m\\alpha_i-\\dfrac{1}{2}\\sum\\limits_{i=1,j=1}^m\\alpha_i\\alpha_jy^{(i)}y^{(j)}(x^{(i)},x^{(j)})\\\\\n&s.t.\n\\begin{cases} \n\\sum\\limits_{i=1}^m\\alpha_iy^{(i)}=0\\\\\n\\alpha_i\\geqslant0,i=1,2,\\cdots,m\n\\end{cases}\n\\end{aligned}\n$$\n等价于求解如下极小化问题\n$$\n\\begin{aligned}\n&\\mathop{\\min}_{\\alpha}\\ \\dfrac{1}{2}\\sum\\limits_{i=1,j=1}^m\\alpha_i\\alpha_jy^{(i)}y^{(j)}(x^{(i)},x^{(j)})-\\sum\\limits_{i=1}^m\\alpha_i\\\\\n&s.t.\n\\begin{cases} \n\\sum\\limits_{i=1}^m\\alpha_iy^{(i)}=0\\\\\n\\alpha_i\\geqslant0,i=1,2,\\cdots,m\n\\end{cases}\n\\end{aligned}\n$$\n\n#### 3. SMO算法\n\n​\t假设输入空间是$\\ X\\in\\R^{n} \\ $,$\\ Y\\in\\{+1,-1\\}\\ $，不妨假设含有$m$个样本数据($x^{(1)}$,$y^{(1)}$)、($x^{(2)}$,$y^{(2)}$)、$\\cdots$、($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\\in X、y^{(i)}\\in Y \\ $，精度为$\\ \\varepsilon\\ $，输出近似解为$\\ \\widehat{\\alpha} \\ $；\n\n1. 取初值$\\ \\alpha^0=0\\ $，令$\\ k=0\\ $\n\n2. 选取待优化变量$\\ \\alpha_1^k、\\alpha_2^k\\ $，计算出新的$\\ \\alpha_2^{new,unr}\\ $\n   $$\n   \\alpha_2^{new,unr}=\\alpha_2^{old}+\\dfrac{y^{(2)}(E^{(1)}-E^{(2)})}{k_{1,1}-2k_{1,2}+k_{2,2}}\n   $$\n\n3. 更新$\\ \\alpha_2^{k+1}\\ $\n   $$\n   \\begin{aligned}\n   \\alpha_2^{k+1}\n   =\n   \\begin{cases}\n   H, & \\alpha_2^{new,unr}\\gt H\\\\\n   \\alpha_2^{new,unr}, & L\\leqslant\\alpha_2^{new,unr}\\leqslant H\\\\\n   L,& \\alpha_2^{new,unr}<L\n   \\end{cases}\n   \\end{aligned}\n   $$\n\n4. 利用$\\ \\alpha_1^{k+1}\\ $和$\\ \\alpha_2^{k+1}\\ $的关系求出$\\ \\alpha_1^{k+1}\\ $\n   $$\n   \\alpha_1^{k+1}=\\alpha_1^{k}+y^{(1)}y^{(2)}(\\alpha_2^{k}-\\alpha_2^{k+1})\n   $$\n\n5. 计算$\\ b^{k+1}\\ $和$\\ E_i\\ $\n\n6. 在精度$\\ \\varepsilon\\ $范围内检查是否满足如下的终止条件，其中$\\ f(x^{(i)})=\\sum\\limits_{j=1}^m\\alpha_jy^{(j)}k_{j,i}+b \\ $ \n   $$\n   \\begin{aligned}\n   &\\begin{cases}\n   \\sum\\limits_{i=1}^m\\alpha_iy^{(i)}=0,\\\\\n   0\\leqslant\\alpha_i\\leqslant C,i=1,2,\\cdots,m\n   \\end{cases}\\\\\n   \\\\\n   &\\begin{cases}\n   \\alpha_i^{k+1}=0\\Longleftrightarrow y^{(i)}f(x^{(i)})\\geqslant1,\\\\\n   \\alpha_i^{k+1}=C\\Longleftrightarrow y^{(i)}f(x^{(i)})\\leqslant1,\\\\\n   0\\lt \\alpha_i^{k+1}\\lt C\\Longleftrightarrow y^{(i)}f(x^{(i)})=1,\n   \n   \\end{cases}\n   \\end{aligned}\n   $$\n   若满足则转$\\ 7\\ $；否则令$\\ k=k+1 \\ $转到步骤$\\ 2\\ $；\n\n7. 取$\\ \\widehat{\\alpha}=\\alpha^{k+1} \\ $。\n\n\n\n#### 4. 面试相关问题\n\n##### 4.1 SVM为什么可以分类非线性问题？\n\n原输入空间是一个非线性可分问题，能用一个超曲面将正负例正确分开；\n\n通过核技巧的非线性映射，将输入空间的超曲面转化为特征空间的超平面，原空间的非线性可分问题就变成了新空间的的线性可分问题。低维映射到高维。\n\n在核函数 $K(x,z)$ 给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，在学习和预测中只定义核函数 $K(x,z)$，而不需要显式地定义特征空间和映射函数$\\phi$，这样的技巧成为核技巧。通常直接计算$K(x,z)$比较容易，而通过$\\phi(x)$和$\\phi(z)$计算$K(x,z)$并不容易。\n\n对于给定核 $K(x,z)$，特征空间和映射函数的取法并不唯一。\n\n##### 4.2 多分类问题\n\n1. 某些算法原生的支持多分类，如：决策树、最近邻算法等。但是有些算法只能求解二分类问题，如：支持向量机。\n2. 对于只能求解二分类问题的算法，一旦遇到问题是多类别的，那么可以将多分类问题拆解成二分类任务求解。\n   - 先对原问题进行拆分，然后为拆出的每个二分类任务训练一个分类器。\n   - 测试时，对这些二分类器的预测结果进行集成，从而获得最终的多分类结果。\n3. 多分类问题有三种拆解方式：\n   - 一对其余(`One-vs-rest:OvR`) :为每一对类别训练一个分类器。\n   - 一对一(`one-vs-one:OvO`) :训练k(k-1)个分类器\n   - 多对多(`many-vs-many:MvM`) 。\n\n##### 4.3  当用支持向量机进行分类时，支持向量越多越好还是越少越好?\n\n结论：在$n$维特征空间中，线性SVM一般会产生$n+1$个支持向量（不考虑退化情况） \n\n通常的SVM的使用会伴随着核技巧（kernel），这用于将低维空间映射到一个更高维的空间，使得原本不线性可分的数据点变得在高维空间中线性可分。虽然这种映射是隐式的，我们通常并不知道映射到的空间是什么样子。但是根据之前的结论，我们可以认为如果训练出来的SVM有d+1个支持向量，这个kernel在这个任务里就讲原来的数据映射到了一个d维的空间中，并使得其线性可分。\n\n更高的维度通常意味着更高的模型复杂度，所以支持向量越多，表示着训练得到的模型越复杂。根据泛化理论，这意味着更有过拟合的风险。\n\n 如果在性能一致的情况下，更少的支持向量可能是更好的。但是这一点其实不绝对，因为泛化理论仅仅是误差的上界，实际的泛化情况的决定因素比较复杂，也可能取决于kernel的性质。所以还是自己做cross validation比较好。\n\n\n\n##### 4.4 **为什么要将求解SVM的原始问题转换为其对偶问题？**\n\n1. 对偶问题往往更易求解（当我们寻找约束存在时的最优点的时候，约束的存在虽然减小了需要搜寻的范围，但是却使问题变得更加复杂。为了使问题变得易于处理，我们的方法是把目标函数和约束全部融入一个新的函数，即拉格朗日函数，再通过这个函数来寻找最优点。）\n2. 自然引入核函数，进而推广到非线性分类问题....\n\n\n\n### 5. SVM损失函数\n\n#### 5.1 Hinge Loss Function\n\n$$\nc(x, y, f(x)) = (1 - y * f(x))_+\n$$\n\n其中$c$ 是损失函数, $x$ 是样本, $y$ 是实际目标列, $f(x)$ 是预测目标列。\n\n上式也可以写成:\n$$\nc(x, y, f(x))= \n\\begin{cases}\n    0,& \\text{if } y*f(x)\\geq 1\\\\\n    1-y*f(x),              & \\text{else}\n\\end{cases}\n$$\n\n#### 5.2 目标函数 \n\nSvm的目标函数如下:\n$$\n\\underset{w}{min}\\ \\lambda\\parallel w\\parallel^2 + \\ \\sum_{i=1}^n\\big(1-y_i \\langle x_i,w \\rangle\\big)_+\n$$\n其中第一项是正则化项, 第二项是损失。\n\n#### 5.3 目标函数的导数\n\n最小化目标函数，我们需要对目标函数求导：\n\n我们分别对目标函数的两项求导如下：\n$$\n\\frac{\\delta}{\\delta w_k} \\lambda\\parallel w\\parallel^2 \\ = 2 \\lambda w_k\n$$\n\n$$\n\\frac{\\delta}{\\delta w_k} \\big(1-y_i \\langle x_i,w \\rangle\\big)_+ \\ = \\begin{cases}\n    0,& \\text{if } y_i \\langle x_i,w \\rangle\\geq 1\\\\\n    -y_ix_{ik},              & \\text{else}\n\\end{cases}\n$$\n\nif $y_i⟨x_i,w⟩ < 1$:\n$$\nw = w + \\eta (y_ix_i - 2\\lambda w)\n$$\nelse:\n$$\nw = w + \\eta (-2\\lambda w)\n$$\n\n### \n\n"},{"title":"Kmeans算法及其衍生算法综述","url":"/2021/01/12/Kmeans算法及其衍生算法综述/","content":"本文主要介绍Kmeans相关算法。\n\n\n\n#### 1 Kmeans聚类\n\nK-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛：\n\n假设输入空间 $\\cal X \\in \\R^n \\ $ 为$\\ n\\ $维向量的集合，$\\ \\cal{X}=\\{x^{(1)} ,x^{(2)},\\cdots,x^{(m)} \\} \\ $，$ \\ \\mathcal  C\\ $为输入空间$\\ \\cal X\\ $的一个划分，不妨令$\\ \\mathcal C=\\{ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\} \\ $，因此可以定义$\\ k\\text{-}means\\ $算法的损失函数为\n$$\nJ(\\mathcal C)=\\sum\\limits_{k=1}^K\\sum\\limits_{x^{(i)}\\in \\mathbb C_k}\\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2  \\tag{1}\n$$\n其中$\\ \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)}  \\ $是簇$\\ \\mathbb C_k\\ $的聚类中心。\n\n事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数$J(\\mathcal C)$\n\n##### 1.1 算法流程\n\n1. 首先随机初始化$\\ K\\ $个聚类中心，$\\ \\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(K)} \\ $；\n\n2. 然后根据这$\\ K\\ $个聚类中心给出输入空间$\\ \\mathcal X \\ $的一个划分，$\\ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\ $；\n\n   - 样本离哪个簇的聚类中心最近，则该样本就划归到那个簇\n     $$\n     \\mathop{\\arg\\min}_{k}\\ \\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2 \\tag{2}\n     $$\n\n3. 再根据这个划分来更新这$\\ K\\ $个聚类中心\n   $$\n   \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)} \\tag{3}\n   $$\n\n4. 重复2、3步骤直至收敛\n\n   - 即$\\ K\\ $个聚类中心不再变化\n\nK-means代码地址如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala\n\n#### 2 K-Means++聚类算法\n\nK-Means聚类算法原理中可知，K-means聚类算法首先需要初始化k个聚类中心。但是K-means算法存在一个巨大的缺陷-收敛情况严重依赖聚类中心的初始化情况。当不小心把所有的k个聚类中心初始化到一个类中，K-means算法很难收敛，情况变得很糟糕。所以在改进K-means算法初始化聚类中心方法，引入一种K-means++聚类算法，中心思想是：**逐个选取$k$个聚类中心，且离其他聚类中心越远的样本点越有可能被选为下一个聚类中心。**\n\n算法流程：\n\n> 1. 在数据集中，随机选取一点，作为第一个聚类中心;\n>\n> 2. 迭代数据中的所有点，计算所有点到最近的聚类中心的距离$D(x)$(具体实现分别计算所有点到每类聚类中心的距离，选择最短的距离作为所有点到最近聚类中心的距离)；\n>    $$\n>    P(x) = \\frac{D(x)^2}{\\sum_{x \\in X}D(x) ^ 2} \\label{4}\n>    $$\n>    \n>\n> 3. 选取距离较大的点作为新的聚类中心；\n>\n> 4. 重复2和3直到选择出K个聚类中心点;\n>\n> 5. 用这k个聚类中心作为初始化质心去运行标准的K-Means算法;\n\n详细scala实现代码如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/KmeansPlus.scala\n\n#### 3. BisectingKMeans-二分Kmeans聚类算法\n\n为了克服k-均值算法收敛于局部最小值的问题，有人提出了另一个称为二分K-均值的算法。\n\n二分k-均值算法思想：\n\n> 1. 首先将所有点作为一个簇，\n>\n> 2. 将该簇一分为二;\n> 3. 之后选择其中一个簇继续进行划分，选择哪个簇继续划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到用户指定的簇数目为止\n\n详细scala代码如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/BisectingKMeans.scala\n\n\n\n#### 4. FuzzyCMeans 模糊均值聚类算法\n\n 模糊c均值聚类较于k-means的硬聚类，模糊c提供了更加灵活的聚类结果。因为大部分情况下，数据集中的对象不能划分成为明显分离的簇，指派一个对象到一个特定的簇有些生硬，也可能会出错。所以对每个对象和每个簇赋予一个权值，指明对象属于该簇的程度。当然，基于概率的方法也可以给出这样的权值，但是有时候我们很难确定一个合适的统计模型，因此使用具有自然地、非概率特性的模糊c均值就是一个比较好的选择。\n\n算法的主要思想是：**给每个样本赋予属于每个簇的隶属度函数。通过隶属度值大小来将样本归类。**\n\n**算法的步骤：**\n\n1. 初始化隶属度$U=[u_{ij}]$矩阵，其中$u_{ij}$表示样本$x_i$属于$j$类的隶属度，对单个样本$x_i$，它对于每个簇的隶属度之和为1。\n\n2. 第$k$步，基于隶属度矩阵$U$计算聚类中心$C^k = [c_j]$\n   $$\n   c_j = \\frac{\\sum_{i=1}^{N}u_{ij}^m x_i}{\\sum_{i=1}^{N}u_{ij}^m} \\label{5}\n   $$\n\n3. 更新$U^k$ 和 $U^{k+1}$\n   $$\n   u_{ij} = \\frac{1}{\\sum_{k=1}^C(\\frac{||x_i-c_j||}{||x_i-c_k||})^{\\frac{2}{m-1}}}\n   $$\n\n4. 如果$||U^{k+1} - U^{k}|| < \\epsilon $,Stop，否则返回步骤2.\n\n   注：迭代的终止条件为：$max_{ij}\\{|u_{ij}^{k+1} - u_{ij}^{k}|\\} < \\epsilon$\n\nscala 代码实现如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/FuzzyCMeans.scala\n\n\n\n#### 5. KMedian 聚类算法\n\n算法流程：\n\n1. 首先随机初始化$\\ K\\ $个聚类中心，$\\ \\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(K)} \\ $；\n\n2. 然后根据这$\\ K\\ $个聚类中心给出输入空间$\\ \\mathcal X \\ $的一个划分，$\\ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\ $；\n\n   - 样本离哪个簇的聚类中心最近，则该样本就划归到那个簇（**计算样本和中心点之间的距离使用的是曼哈顿距离**，**而K-Means聚类算法使用的欧式距离**。\n     $$\n     \\mathop{\\arg\\min}_{k}\\ \\Vert x^{(i)}-\\mu^{(k)} \\Vert \\tag{2}\n     $$\n\n3. 再根据这个划分来更新这$\\ K\\ $个聚类中心\n   $$\n   \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)} \\tag{3}\n   $$\n\n4. 重复2、3步骤直至收敛\n\n   - 即$\\ K\\ $个聚类中心不再变化\n\nscala 代码地址为：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/KMedian.scala\n\n\n\n#### 6. KMedoid 聚类算法\n\n与K-means算法类似，区别在于中心点的选取，K-means中选取的中心点为当前类中所有点的重心，而K-medoids法选取的中心点为当前cluster中存在的一点，准则函数是当前cluster中所有其他点到该中心点的距离之和最小，这就在一定程度上削弱了异常值的影响，但缺点是计算较为复杂，耗费的计算机时间比K-means多。\n\n算法流程如下：\n\n>1.在总体$n$个样本点中任意选取k个点作为medoids\n>\n>2.按照与medoids最近的原则，将剩余的$n-k$个点分配到当前最佳的medoids代表的类中\n>\n>3.对于第$i$个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，准则函数的值，遍历所有可能，选取准则函数最小时对应的点作为新的medoids\n>\n>4.重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数\n>\n>5.产出最终确定的$k$个类;\n\n代码地址：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/KMedoid.scala\n\n\n\n#### 7 SpectralClustering 谱聚类算法\n\n​      谱聚类是从图论中演化出来的算法，主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的子图间边权重和尽可能的低，而子图内的边权重和尽可能的高，从而达到聚类的目的。\n\n算法步骤如下：\n\n最常用的相似矩阵的生成方式是基于高斯核距离的全连接方式\n\n>1) 根据输入的相似矩阵的生成方式构建样本的相似矩阵$S$;\n>\n>2) 根据相似矩阵S构建邻接矩阵$W$，构建度矩阵$D$;\n>\n>3) 计算出标准化的拉普拉斯矩阵$L = I- D^{-1}W$ ;\n>\n>4) 计算矩阵$L$的$k$个最小特征值对应的$n$维特征向量$v_1,...,v_k$，通过下式求解特征向量：\n>$$\n>Lv = \\lambda Dv\n>$$\n>5) $k$个$n$维特征向量$v_1,...,v_k$组成$n×k$维的矩阵$M$；\n>\n>6）每一行表示一个样本，对该$n$个样本进行$k$均值聚类算法，得到聚类结果$C_1,...,C_k$;\n\n代码地址如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/SpectralClustering.scala\n\n"},{"title":"MySQL和Oracle 表注释和字段注释相关总结","url":"/2020/11/25/MySQL和Oracle 表注释和字段注释相关总结/","content":"#### 1. Oracle 添加/查看表注释和字段注释\n\n##### 1.1 Oracle添加表注释和字段注释\n\nTODO\n\n##### 1.2 Oracle查看全部表注释和字段注释\n\n```sql\n--获取表注释。获得 table_name,table_type,comments。\nselect * from user_tab_comments; \n--获取字段注释。获得 table_name,column_name,comments。\nselect * from user_col_comments; \n```\n\n#### 2. Mysql 添加/查看表注释和字段注释\n\n##### 2.1 Mysql添加表注释和字段注释\n\n```sql\nCREATE TABLE `t_cluster`(\n    `id`             int(11)    NOT NULL COMMENT '样本的ID',\n    `cluster_id`     int(11)    NOT NULL COMMENT '聚类的编号',\n    PRIMARY KEY (`id`))\n ENGINE = InnoDB  DEFAULT CHARSET = utf8 COMMENT ='聚类结果表';\n```\n\n##### 2.2 Mysql 查看全部表注释和字段注释\n\n```sql\n-- 表注释\nselect * from information_schema.TABLES where TABLE_SCHEMA='数据库名' and TABLE_NAME='表名' \n-- 字段注释\nshow  full  columns  from  test1; \n--在元数据的表里面看 \nselect * from information_schema.COLUMNS where TABLE_SCHEMA='数据库名' and TABLE_NAME='表名' ;\n```\n\n##### 2.3  修改表的注释\n\n```sql\nalter table table_name comment '注释';\n```\n\n##### 2.4  修改字段的注释\n\n```sql\nalter table table_name modify column column_name int comment '注释'; \n```\n\n#### 3. 分享一段由Oracle迁移到Mysql表注释和字段注释python代码\n\n最后将更改表注释的语句批量执行。\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Nov 25 09:02:55 2020\n\n@author: lixin\n\"\"\"\n\nimport mysql.connector as mysql\nimport pandas as pd\n \ndb = mysql.connect(\n    host = \"127.0.0.1\",\n    user = \"root\",\n    passwd = \"121457\",\n    database = \"tpc2\"\n)\n\ndata = pd.read_sql(\"SELECT TABLE_NAME,TABLE_COMMENT FROM information_schema.TABLES\",db)\n\n# 从oracle下载来的表的注释，用于迁移到mysql\ndict_comments_tab = pd.read_excel(\"comments/user_tab_comments.xlsx\")\n\ndef prc(row, df):\n    dff = df[(df['TABLE_NAME'] == row.upper())]\n    res = [i for i in dff['COMMENTS']]\n    return res[0] if len(res) > 0 else None\n\n\ndata['注释'] = data.apply(lambda x: prc(x['TABLE_NAME'],dictjj),axis = 1)\ndata = data[pd.notnull(data['注释'])]\ndata2 = data[data['TABLE_COMMENT'] == '']\ndata2.to_csv('tab_commentsII.csv',index=0)\n\ncursor = db.cursor()\nres = []\nfor index, row in data2.iterrows():\n    val = row['注释']\n    table = row['TABLE_NAME']\n    sql = ''' alter table {} comment '{}';'''.format(table,val)\n    res.append(sql)\n\nfile_name = 'write_tab_comment.txt'\nwith open(file_name,'w') as file_obj:\n    for i in res:\n        file_obj.write(i+'\\n')\n```\n\n\n\n#### 参考：\n\n1. https://www.cnblogs.com/chaos-li/p/11118696.html"},{"title":"风控算法工程师面试指南","url":"/2020/11/23/风控算法工程师面试指南/","content":"# 风控模型师面试指南\n\n## **一. 算法**\n\n## **1.逻辑回归**\n\n**1.1  逻辑回归的优缺点，在金融领域相比其他算法有什么优势，局限性在哪？**\n\n1）优点：\n\n- 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。\n- 模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。\n- 模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。\n- 特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。\n- 模型的结果可以很方便的转化为策略规则，且线上部署简单。\n\n2）缺点和局限性:\n\n- 容易欠拟合，相比集成模型，准确度不是很高。\n- 对数据的要求比较高，逻辑回归对缺失值，异常值，共线性都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。\n- 在金融领域对场景的适应能力有局限性，例如数据不平衡问题，高维特征，大量多类特征，逻辑回归在这方面不如决策树适应能力强。\n\n**1.2 : 逻辑回归是线性模型吗？逻辑回归和线性回归的区别？**\n\n- 逻辑回归是一种广义线性模型，它引入了$Sigmod$函数，是非线性模型，但本质上还是一个线性回归模型，因为除去$Sigmod$函数映射关系，其他的算法原理，步骤都是线性回归的。\n- 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个$Sigmod$函数，使样本映射到$[0,1]$之间的数值，从而来处理分类问题。另外逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。\n\n**1.3：逻辑回归做分类的样本应该满足什么分布？**\n\n应该满足伯努利分布，逻辑回归的分类标签是基于样本特征通过伯努利分布产生的，分类器要做的就是估计这个分布。\n\n**1.4：逻辑回归解决过拟合的方法有哪些？**\n\n- 减少特征数量，在实际使用中会用很多方法进行特征筛选，例如基于$IV$值的大小，变量的稳定性，变量之间的相关性等。\n- 正则化，常用的有$L1$正则化和$L2$正则化。\n\n**1.5：什么是特征的离散化和特征交叉？逻辑回归为什么要对特征进行离散化？**\n\n- 特征离散化是将数值型特征（一般是连续型的）转变为离散特征，例如评分卡中的$woe$转化，就是将特征进行分箱，再将每个分箱映射到$woe$值上，就转换为了离散特征。特征交叉也叫作特征组合，是将单独的特征进行组合，使用相乘/相除/笛卡尔积等形成合成特征，有助于表示非线性关系。比如使用$One-Hot$向量的方式进行特征交叉。这种方式一般适用于离散的情况，我们可以把它看做基于业务理解的逻辑和操作，例如经度和纬度的交叉，年龄和性别的交叉等。\n- 实际工作中很少直接将连续型变量带入逻辑回归模型中，而是将特征进行离散化后再加入模型，例如评分卡的分箱和$woe$转化。这样做的优势有以下几个：1）特征离散化之后，起到了简化模型的作用，使模型变得更稳定，降低了模型过拟合的风险。2）离散化之后的特征对异常数据有很强的鲁棒性，实际工作中的哪些很难解释的异常数据一般不会做删除处理，如果特征不做离散化，这个异常数据带入模型，会给模型带来很大的干扰。3）离散特征的增加和减少都很容易，且稀疏向量的内积乘法运算速度快，易于模型的快速迭代。4）逻辑回归属于广义线性模型，表达能力有限，特征离散化之后，每个离散变量都有单独的权重，相当于给模型引入了非线性，能够提高模型的表达能力。5）离散化后的特征可进行特征交叉，进一步引入非线性，提高模型的表达能力。\n\n**1.6：在逻辑回归中，为什么要常常做特征组合（特征交叉）？**\n\n逻辑回归模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸，过拟合，所以基本特征+特征组合兼顾了全局和个性化。\n\n**1.7 ：做评分卡中为什么要进行WOE化？**\n\n- 更好的解释性，变量离散化之后可将每个箱体映射到woe值，而不是通常做one-hot转换。\n- woe化之后可以计算每个变量的IV值，可用来筛选变量。\n- 对离散型变量，woe可以观察各个level间的跳转对odds的提升是否呈线性。\n- 对连续型变量，woe和IV值为分箱的合理性提供了一定的依据，也可分析变量在业务上的可解释性。\n- 用woe编码可以处理缺失值问题。\n\n**1.8：高度相关的特征带入逻辑回归到底有什么影响？为什么逻辑回归要将高度相关特征剔除？**\n\n- 在损失函数最终收敛的情况下，就算有很多相关度很高的特征，也不会影响模型的效果。假设一个特征将它重复100次，生成100个高度相关的特征。那么模型训练完之后，这100个特征和原来那一个特征扮演的效果一样，每一个特征的权重都是原来特征的1/100，只是可能中间很多特征的系数正负相互抵消了，比如做评分卡，如果引入了高度相关的特征，那么最后逻辑回归的系数符号可能就会不一致。\n- 虽然高度相关特征对模型结果没什么大的影响，但还是要剔除相关性高的特征，原因是一个可以减少特征数量，提高模型的训练速度，减少过拟合的风险。二是去掉高相关特征可以让模型的可解释性更好。尤其在做评分卡时，为了使最后每个特征的系数符号一致，必须做特征相关性筛选。\n\n**1.9：逻辑回归的特征系数的绝对值可以认为是特征的重要性吗？**\n\n首先特征系数的绝对值越大，对分类效果的影响越显著，但不能表示系数更大的特征重要性更高。因为改变变量的尺度就会改变系数的绝对值，而且如果特征是线性相关的，则系数可以从一个特征转移到另一个特征，特征间相关性越高，用系数解释变量的重要性就越不可靠。\n\n**1.10：逻辑回归为什么要用极大似然函数作为损失函数？**\n\n- 如果用最小二乘法，目标函数就是 $E_{w,b}=\\sum_{i=1}^{m}\\left ( y_{i}-\\frac{1}{1+e^{-\\left ( w^{T}x_{i}+b \\right )}}\\right )^2$ ,是非凸的，不容易求解，会得到局部最优。\n- 如果用最大似然估计，目标函数就是对数似然函数： $l_{w,b}=\\sum_{i=1}^{m}\\left ( -y_{i}\\left ( w^{T}x_{i}+b \\right )+ln\\left ( 1+e^{w^{T}x_{i}+b} \\right ) \\right )$,是关于 $(w,b)$ 的高阶连续可导凸函数，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。  \n\n##### 1.11：L1和L2正则化有什么区别？\n\n​    L0正则化的值是模型参数中非零参数的个数。\n\n​    L1正则化表示各个参数绝对值之和。\n\n​    L2正则化标识各个参数的平方的和的开方值。\n\n​    先讨论几个问题：\n\n1）实现参数的稀疏有什么好处吗？\n\n​    一个好处是可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对 训练数据可以预测的很好，但是对**测试**数据就只能呵呵了。另一个好处是参数变少可以使整个模型获得更好的可解释性。\n\n2）参数值越小代表模型越简单吗？\n\n​    是的。为什么参数越小，说明模型越简单呢，这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。\n\n根据上面的讨论，稀疏的参数可以防止过拟合，因此用L0范数（非零参数的个数）来做正则化项是可以防止过拟合的。\n\n​    从直观上看，利用非零参数的个数，可以很好的来选择特征，实现特征稀疏的效果，具体操作时选择参数非零的特征即可。但因为L0正则化很难求解，是个NP难问题，因此一般采用L1正则化。L1正则化是L0正则化的最优凸近似，比L0容易求解，并且也可以实现稀疏的效果。\n\n​    L1正则化在实际中往往替代L0正则化，来防止过拟合。在江湖中也人称Lasso。\n\n​    L1正则化之所以可以防止过拟合，是因为L1范数就是各个参数的绝对值相加得到的，我们前面讨论了，参数值大小和模型复杂度是成正比的。因此复杂的模型，其L1范数就大，最终导致损失函数就大，说明这个模型就不够好。\n\n​    L2正则化可以防止过拟合的原因和L1正则化一样，只是形式不太一样。\n\n​    L2范数是各参数的平方和再求平方根，我们让L2范数的正则项  $||w||^{2}$最小，可以使W的每个元素都很小，都接近于0。但与L1范数不一样的是，它不会是每个元素为0，而只是接近于0。越小的参数说明模型越简单，越简单的模型越不容易产生过拟合现象。\n\nL1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，因此可以用于特征选择\n\nL2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合\n\n## **2.决策树**\n\n**2.1：决策树模型的优缺点及适用性？**\n\n优点：\n\n- 易于理解，决策树可以生成IF..TEHN逻辑表达的树结构，可解释性很好。\n- 相比逻辑回归对数据的处理较简单，不太需要做例如数据离散化，归一化等操作。\n- 决策树是目前已知的对于处理非线性交互的最好的算法。\n- 模型的效果比较好，例如随机森林，$xgboost$都是基于决策树构建的。\n\n缺点：\n\n- 很容易在训练过程中生成过于复杂的树结构，造成过拟合。\n- 不适合处理高维数据，当属性数量过大时，部分决策树就不适用了。\n- 泛化能力能力比较差，对于没有出现过的值几乎没有办法。\n\n**2.2：简述一下决策树的原理以及树的构建过程。**\n\n决策树时基于树的结构进行决策的，学习过程包括特征选择，决策树的生成和剪枝过程。决策树的学习过程通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，选择最优特征，该特征有几种值就划分为多少子集，每个子集递归调用此方法，返回结点，返回的结点就是上一层的子节点，直到所有特征都已经用完，或者数据集只有一维特征为止。\n\n**2.3：简述一下ID3，C4.5，CART三类决策树的原理和异同点。**\n\n- ID3选择最佳分割点是基于信息增益的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有泛化能力，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用信息增益率来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。\n- C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以CART生成的是一颗二叉树，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类是使用的是GINI系数作为划分标准，在做回归时使用的是均方误差。\n\n**2.4：分类树和回归树的区别在哪里？**\n\n- 分类树以C4.5为例，在对一个特征进行划分时，是穷举这个特征的每一个阈值，找到使得特征<=阈值和特征>阈值分成的两个分支的熵的最大值，按照该标准分支得到两个新的节点，用同样的方法继续分支，直到得到种类唯一的叶子节点，或者达到预设的终止条件为止。\n- 回归树的流程是类似分类树的，区别在于划分时的标准不再是最大熵，而是最小化均差，如果节点的预测值错的越离谱，均方差越大，通过最小化均差能够找到最可靠的分支依据。\n\n**2.5：决策树对缺失值是如何处理的？**\n\n决策树处理缺失要考虑以下三个问题：\n\n1. 当开始选择哪个属性来划分数据集时，样本在某几个属性上有缺失怎么处理：\n\n- 忽略这些缺失的样本。\n- 填充缺失值，例如给属性A填充一个均值或者用其他方法将缺失值补全。\n- 计算信息增益率时根据缺失率的大小对信息增益率进行打折，例如计算属性A的信息增益率，若属性A的缺失率为0.9，则将信息增益率乘以0.9作为最终的信息增益率。\n\n2. 一个属性已经被选择，那么在决定分割点时，有些样本在这个属性上有缺失怎么处理？\n\n- 忽略这些缺失的样本。\n- 填充缺失值，例如填充一个均值或者用其他方法将缺失值补全。\n- 把缺失的样本，按照无缺失的样本被划分的子集样本个数的相对比率，分配到各个子集上去，至于那些缺失样本分到子集1，哪些样本分配到子集2，这个没有一定准则，可以随机而动。\n- 把缺失的样本分配给所有的子集，也就是每个子集都有缺失的样本。\n- 单独将缺失的样本归为一个分支。\n\n3.决策树模型构建好后，测试集上的某些属性是缺失的，这些属性该怎么处理？\n\n- 如果有单独的缺失值分支，依据此分支。\n- 把待分类的样本的属性A分配一个最常出现的值，然后进行分支预测。\n- 待分类的样本在到达属性A结点时就终止分类，然后根据此时A结点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。\n\n**2.6：为什么决策树不需要对数据做归一化等预处理？**\n\n决策树是一种概率模型，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。\n\n**2.7：如何解决决策树的过拟合问题？**\n\n- 预剪枝的方法：通过提前停止树的构建而对树剪枝，是目前解决过拟合的主要方法。常用的剪枝条件包括限制树的深度，限制叶节点最小样本数，限制叶节点的最小样本权重，限制叶节点的信息增益值的阈值等。\n- 后剪枝的方法：首先构造完整的决策树，允许树过度拟合数据，然后应单个结点代替子树，节点的分类采用子树的主要分类。剪枝方法有错误率降低剪枝，悲观错误剪枝，代价复杂度剪枝\n\n\n\n## **3.集成学习**\n\n**3.1：什么是集成学习？集成学习有哪些框架？简单介绍各个框架的常用算法。**\n\n- 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，$SVM$，$kNN$等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有$bagging$，$boosting$，$stacking$三种：\n- $bagging$：对训练集进行随机子抽样，对每个子训练集构建基模型，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用多数投票法确定最终类别，如果是回归算法，则将各个回归结果做算术平均作为最终的预测值。常用的$bagging$算法：随机森林\n- $boosting$：训练过程为阶梯状，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型预测错误的样本上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。\n- $stacking$：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用LR进行回归组合），从而得到完整的$stacking$模型。要得到$stacking$模型，关键在于如何构造第二层的特征，构造第二层特征的原则是尽可能的避免信息泄露，因此对原始训练集常常采用类似于K折交叉验证的划分方法。各个基模型要采用相同的$Kfold$，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。\n\n**3.2: 简单描述一下模型的偏差和方差？bagging和boosting主要关注哪个？**\n\n- 偏差描述的是预测值与真实值的差距，偏差越大，越偏离真实数据。\n- 方差描述的是预测值的变化范围，离散程度，方差越大，数据分布越分散。\n- Bagging算法是对训练样本进行采样，产生出若干不同的子集，再从每个数据子集中训练出一个分类器，取这些分类器的平均，所以是降低模型的方差（variance）。Bagging算法和Random Forest这种并行算法都有这个效果。Boosting则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，所以模型的偏差（bias）会不断降低。\n\n\n**3.3：简述一下随机森林的原理，随机森林的构造过程。**\n\n随机森林是$bagging$算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合，利用这种组合来降低单棵决策树的可能带来的片面性和判断不准确性。对于普通的决策树，是在所有样本特征中找一个最优特征来做决策树的左右子树划分，而随机森林会先通过自助采样的方法（bootstrap）得到$N$个训练集，然后在单个训练集上会随机选择一部分特征，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按多数投票的准则确定最终结果，对于回归问题，由多棵决策树的预测值的平均数作为最终结果。随机森林的随机性体现在两方面，一个是选取样本的随机性，一个是选取特征的随机性，这样进一步增强了模型的泛化能力。\n\n**3.4：随机森林的优缺点？**\n\n优点：\n\n- 训练可以高度并行化，训练速度快，效率高。\n- 两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。\n- 由于每次不再考虑全部的特征属性，二是特征的一个子集，所以相对于bagging计算开销更小，效率更高。\n- 对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。\n- 可以输出变量的重要程度，被认为是一种不错的降维方法。\n\n缺点：\n\n- 在某些噪声较大的分类问题和或回归问题上容易过拟合。\n- 模型的可解释性比较差，无法控制模型内部的运行。\n- 对于小数据或者低维数据，效果可能会不太好。\n\n**3.5：随机森林为什么不容易过拟合？**\n\n随机森林由很多棵树组合在一起，单看每一棵树可以是过拟合的，但是既然是过拟合，就会拟合到非常小的细节，随机森林通过引入随机性，让每一棵树过拟合的细节不同，再将这些树组合在一起，过拟合的部分就会抵消掉，不过随机森林还是可能会出现过拟合的现象，只是出现的概率相对较低。\n\n**3.6：随机森林输出特征重要性的原理？**\n\n- 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。\n- 基于基尼系数：如果特征X出现在决策树J中的结点M，则计算节点M分枝前后的Gini指数变化量，假设随机森林由N棵树，则计算N次的Gini系数，最后将所有的Gini系数做一个归一化处理就得到了该特征的重要性。\n- 基于袋外数据错误率：袋外数据指的是每次随机抽取未被抽取达到的数据，假设袋外的样本数为O，将这O个数据作为测试集，代入已生成好的随机森林分类器，得到预测的分类结果，其中预测错误的样本数为X，则袋外数据误差为X/O，这个袋外数据误差记为errOOB1，下一步对袋外数据的特征A加入噪声干扰，再次计算袋外误差errOOB2，假设随机森林由N个分类器，则特征A的重要性为：sum(errOOB2-errOOB1)/N,其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征A不重要。\n\n**3.7：简单描述一下Adaboost的算法原理和流程。**\n\n- Adaboost基于分类器的错误率分配不同的权重系数，最后得到累加加权的的预测结果。\n\n算法流程：\n\n- 给数据中每一个样本一个权重，若有N个样本，则每个样本的权重为1/N.\n- 训练数据的每一个样本，得到第一个分类器。\n- 计算该分类器的错误率，根据错误率计算给分类器分配的权重。\n- 将第一个分类器分错的样本权重增加，分对的样本权重减少，然后再用新的样本权重训练数据，得到新的分类器。\n- 迭代这个训练步骤直到分类器错误为0或达到迭代次数。\n- 将所有的弱分类器加权求和，得到分类结果（分类器权重），错误率低的分类器获得更高的决定系数，从而在数据进行预测起关键作用。\n\n**3.8：Adaboost的优点和缺点？**\n\n优点：\n\n- 分类精度高，构造简单，结果可理解。\n- 可以使用各种回归分类模型来构建弱学习器，非常灵活。\n- 不容易过拟合。\n\n缺点：\n\n- 训练时会过于偏向分类困难的数据，导致Adaboost容易受噪声数据干扰。\n- 依赖于弱分类器，训练时间可能比较长。\n\n**3.9：简单说一下GBDT的原理。**\n\n- **GBDT是boosting的一种方法，主要思想是每一次建立单个分类器时，是在之前建立的模型的损失函数的梯度下降方向。损失函数越大，说明模型越容易出错，如果我们的模型能让损失函数持续的下降，则说明我们的模型在持续不断的改进，而最好的方式就是让损失函数在其梯度的方向上下降。**\n- GBDT的核心在于每一棵树学的是之前所有树结论和的残差，残差就是真实值与预测值的差值，所以为了得到残差，GBDT中的树全部是回归树，之所以不用分类树，是因为分类的结果相减是没有意义的。\n- Shrinkage（缩减）是 GBDT 的一个重要演进分支，Shrinkage的思想在于每次走一小步来逼近真实的结果，要比直接迈一大步的方式更好，这样做可以有效减少过拟合的风险。它认为每棵树只学到了一小部分，累加的时候只累加这一小部分，通过多学习几棵树来弥补不足。这累加的一小部分（步长*残差）来逐步逼近目标，所以各个树的残差是渐变的而不是陡变的。\n- GBDT可以用于回归问题（线性和非线性），也可用于分类问题。\n\n**3.10：为什么对于高维稀疏特征不太适合用GBDT？**\n\n- GBDT在每一次分割时需要比较大量的特征，特征太多，模型训练很耗费时间。\n- 树的分割往往只考虑了少部分特征，大部分的特征都用不到，所有的高维稀疏的特征会造成大量的特征浪费。\n\n**3.11：GBDT和随机森林的异同点？**\n\n相同点：\n\n- 都是由多棵树构成，最终的结果也是由多棵树决定。\n\n不同点：\n\n- 随机森林可以由分类树和回归树组成，GBDT只能由回归树组成。\n- 随机森林的树可以并行生成，而GBDT只能串行生成，所以随机森林的训练速度相对较快。\n- 随机森林关注减小模型的方差，GBDT关注减小模型的偏差。\n- 随机森林对异常值不敏感，GBDT对异常值非常敏感。\n- 随机森林最终的结果是多数投票或简单平均，而GBDT是加权累计起来。\n\n**3.12：GBDT的优缺点？**\n\n优点：\n\n- GBDT每一次的残差计算都增大了分错样本的权重，而分对的权重都趋近于0，因此泛化性能比较好。\n- 可以灵活的处理各种类型的数据。\n\n缺点：\n\n- 对异常值比较敏感。\n- 由于分类器之间存在依赖关系，所以很难进行并行计算。\n\n**3.13：XGBOOST和GBDT的区别在哪里？**\n\n- 传统的GBDT是以CART树作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题），线性分类器的速度是比较快的，这时候xgboost的速度优势就体现了出来。\n- 传统的GBDT在优化时只使用一阶导数，而xgboost对损失函数做了二阶泰勒展开，同时用到了一阶和二阶导数，并且xgboost支持使用自定义损失函数，只要损失函数可一阶，二阶求导。\n- xgboost在损失函数里加入了正则项，用来减小模型的方差，防止过拟合，正则项里包含了树的叶节点的个数， 每个叶子节点上输出的score的L2模的平方和。\n- xgboost里有一个参数叫学习速率（learning_rate）， xgboost在进行完一次迭代后，会将叶子节点的权重乘上学习速率，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把learing_rate设置得小一点，然后迭代次数(n_estimators)设置得大一点。\n- xgboost借鉴了随机森林的原理，支持行抽样(subsample)和列抽样(colsample_bytree,colsample_bylevel)， 行抽样指的是随机森林里对数据集进行有放回抽样，列抽样指的是对特征进行随机选择，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。\n\n**3.14：为什么XGBOOST要用泰勒展开，优势在哪里？**\n\nxgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法的优化分开来了，这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可以用于分类，也可以用于回归。\n\n**3.15：XGBOOST是如何寻找最优特征的？**\n\nxgboost在训练过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据，从而记忆了每个特征在模型训练时的重要性，从根到叶子中间节点涉及某特征的次数作为该特征重要性排序。\n\n**3.16：XGBOOST是如何处理缺失值的？**\n\nxgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中选择能够最小化训练误差的方向作为默认的分裂方向，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。\n\n**3.17：XGBOOST的并行化是如何实现的？**\n\n- xgboost的并行不是在tree粒度上的并行，xgboost也是一次迭代完才能进行下一次迭代（第t次迭代的损失函数包含了第t-1次迭代的预测值），它的并行处理是在特征粒度上的，在决策树的学习中首先要对特征的值进行排序，然后找出最佳的分割点，xgboost在训练之前，就预先对数据做了排序， 然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\n- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n\n**3.18：XGBOOST采样时有放回的还是无放回的？**\n\nxgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost支持子采样，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还支持列采样，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。\n\n**3.19：XGBOOST的调参步骤是怎样的？**\n\nPS：这里使用Gridsearch cv来穷举检索最佳的参数，如果时间允许，可以通过设置步数先粗调，再细调。\n\n- 保持learning rate和其他booster相关的参数不变，调节和estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。\n- 调节max_depth 和 min_child_weight参数，首先，我们先大范围地粗调参数，然后再小范围地微调。\n- gamma参数调优\n- subsample和colsample_bytree 调优\n- 正则化参数调优，选择L1正则化或者L2正则化\n- 缩小learning rate，得到最佳的learning rate值\n\n**3.20：XGBOOST特征重要性的输出原理？**\n\nxgboost是用get_score方法输出特征重要性的，其中importance_type*参数*支持三种特征重要性的计算方法：\n\n- importance_type*=*weight（默认值），使用特征在所有树中作为划分属性的次数。\n- importance_type*=*gain，使用特征在作为划分属性时loss平均的降低量。\n- importance_type*=*cover，使用特征在作为划分属性时对样本的覆盖度。\n\n**3.21：LightGBM相比XGBOOST在原理和性能上的差异？**\n\n1.速度和内存上的优化：\n\n- xgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。\n- LightGBM用的是直方图（Histogram）的决策树算法，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。\n\n2.准确率上的优化：\n\n- xgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n- LightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。\n\n3.对类别型特征的处理**：**\n\n- xgboost不支持直接导入类别型变量，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致哑变量处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。\n- LightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它没有对类别型特征进行独热编码，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是对类别按照与目标标签的相关性进行重排序，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。\n\n\n\n## **二. 特征工程**\n\n**4.1：什么是特征工程？为什么特征工程对机器学习很重要？**\n\n- 特征工程指的是使用专业知识和技巧来处理数据，使得特征在机器学习算法上发挥更好的作用的过程。这个过程包含了数据预处理，特征构建，特征筛选等。特征工程的目的就是筛选出好的特征，得到更好的训练数据，使模型达到更好的效果。\n- 从数据中提取出来的特征好坏会直接影响到模型的效果，有的时候，如果特征工程做得好，仅使用一些简单的机器学习算法，也能达到很好的效果。由此可见特征工程在实际的机器学习中的重要性。\n\n\n\n**4.2：特征工程的一般步骤是什么？什么是特征工程的迭代？**\n\n特征工程常规步骤：\n\n- 数据获取，数据的可用性评估（覆盖率，准确率，获取难度）\n- 探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。\n- 特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。\n- 特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，四则运算，基于业务理解进行头脑风暴构建特征等。\n- 特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有过滤法，包装法，嵌入法。\n\n特征工程的迭代:\n\n- 选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。\n- 设计特征：可以自动进行特征提取工作，也可以手工进行特征的构建。\n- 选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。\n- 计算模型：计算模型在该特征上所提升的准确率。\n- 上线测试：通过在线测试的效果来评估特征是否有效。\n\n\n\n**4.3：常用的特征工程方法有哪些？**\n\n- 特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。\n- 特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。\n- 特征筛选：过滤法，封装法，嵌入法。\n\n\n\n**4.4：在实际的风控建模中怎么做好特征工程？**\n\n本人工作中的一些经验总结：\n\n- 因为做风控模型大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。\n- 基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。\n- 如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上多下功夫，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。\n\n\n\n**4.5：实际项目中原始数据通常有哪些问题？你是如何解决的？**\n\n- 一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。\n- 数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。\n- 有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。\n- 大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成维度灾难，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。\n\n\n\n**4.6：在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？**\n\n- 特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。\n- 特征与目标变量是高度相关的，衡量的指标主要是IV。\n- 特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。\n- 特征的覆盖率，一般来说覆盖率要达到70%以上。\n- 特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。\n- 特征的及时性，最好是能代表用户最近的信用风险情况。\n\n\n\n**4.7：为什么探索性数据分析（EDA）在机器学习中非常重要？**\n\n- EDA不单是看看数据的分布，而是对数据整体有一个大概的了解。通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律。从中发现关键性的价值信息，这些信息对于后续建模及对模型的正确理解有很重要的意义。\n- 通过EDA可以发现数据的异常，可以分析每个特征与目标变量之间的关系，特征与特征之间的关系，为特征构建和特征筛选提供有价值的信息。\n- EDA分析可以验证数据是不是你认为的那样，实际情况中由于数据和特征量比较大，往往忽视这些数据是如何生成的，数据突出的问题或模型的实施中的错误会被长时间忽视，这可能会导致基于错误信息做出决策。\n\n\n\n**4.8：缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？**\n\n- 首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。\n- 风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。\n\n\n\n**4.9：如何发现数据中的异常值？对异常值是怎么处理的？**\n\n- 一种是基于统计的异常点检测算法例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有欧式距离，绝对距离。\n- 对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。\n\n\n\n**4.10：对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？**\n\n- 时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。\n- 连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。\n- 离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。\n\n\n\n**4.11：如何处理样本不平衡的问题？**\n\n- 在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种抽样方法，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。\n- 确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。\n- 对数据集进行抽样，一种是进行欠采样，通过减少大类的数据样本来降低数据的不平衡，另一种是进行过采样，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。\n- 尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。\n- 尝试从新的角度来理解问题，可以把那些小类样本当做异常点，因此该分类问题转化为异常检测问题或变化趋势检测问题，这种方法笔者很少用到，就不详细说明了。\n\n\n\n**4.12：特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？**\n\n常规的特征衍生方法：\n\n- 基于对业务的深入理解，进行头脑风暴，构造特征。\n- 特征交叉，例如对类别特征进行交叉相乘。\n- 分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。\n- 重构数值量（单位转换，整数小数拆分，构造阶段性特征）\n- 特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。\n\n平时工作特征衍生的做法：\n\n- 因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个\"在敏感时间段（深夜）的通话次数占比\"，如果占比较高，用户的风险也较大。\n- 平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。\n- 对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。\n\n\n\n**4.13：特征筛选的作用和目的？筛选的特征需要满足什么要求？**\n\n作用和目的：\n\n- 简化模型，增加模型的可解释性， 降低模型过拟合的风险。\n- 缩短模型的训练时间。\n- 避免维度灾难。\n\n筛选特征满足的要求：\n\n- 具有良好的区分能力。\n- 可解释性好，与目标变量的关系在业务上能解释的通。\n- 在时间序列上有比较好的稳定性。\n- 特征的用户覆盖率符合要求。\n\n\n\n**4.14：特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？**\n\nFilter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n\n- 相关系数，方差（适用于连续型变量），卡方检验（适用于类别型变量），信息熵，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。\n- 优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。\n- 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。\n\nWrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。\n\n- 方法有完全搜索（递归消除法），启发式搜索（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。\n- 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。\n- 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。\n\nEmbedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n\n- 一种是基于惩罚项，例如岭回归，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。\n- 优点：效果最好速度最快，模式单调，快速并且效果明显。\n- 缺点：如何参数设置， 需要对模型的算法原理有较好的理解。\n\n\n\n\n\n## **三.模型评估和优化**\n\n**5.1：简单介绍一下风控模型常用的评估指标。**\n\n- 混淆矩阵指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。\n- ROC曲线和AUC值，ROC曲线是一种对于查全率和假正率的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n- KS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。KS值仅能代表模型的区隔能力，KS不是越高越好，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。\n- 基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。基尼系数越大，表示模型对于好坏用户的区分能力越好。\n\n\n\n**5.2：为什么ROC适合不平衡数据的评价？**\n\n- ROC曲线的纵轴是TPR= ![\\frac{TP}{TP+FN}](https://www.zhihu.com/equation?tex=%5Cfrac%7BTP%7D%7BTP%2BFN%7D) ，横轴是FPR= ![\\frac{FP}{FP+TN}](https://www.zhihu.com/equation?tex=%5Cfrac%7BFP%7D%7BFP%2BTN%7D) ，TPR聚焦于正例，FPR聚焦于与负例，所以ROC兼顾了正样本和负样本的权衡，使其成为一个比较均衡的评估方法。\n- 因为TPR用到的TP和FN都是正样本，FPR用到的FP和TN都是负样本，所以说正样本或负样本发生了改变，TPR和FPR也不会相互影响，因此即使类别分布发生了改变，数据变得不平衡了，ROC曲线也不会产生大的变化。ROC曲线的优点，即具有鲁棒性，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。\n\n\n\n**5.3：AUC和KS的关系是什么？**\n\n![img](https://pic3.zhimg.com/v2-51c5764548effcfc1348c1390fb47a52_b.jpg) ![img](https://pic3.zhimg.com/80/v2-51c5764548effcfc1348c1390fb47a52_hd.jpg)\n\n- 左图是KS曲线，红色的是TPR曲线（累计正样本占比），蓝色的是FPR曲线（累计负样本占比）。由于按照正样本预测概率降序排列，所以排在前面的样本为正的概率更大，但为正的概率是递减的；相反排在前面的样本为负的概率更小，但为负的概率递增。所以KS图中，TPR曲线在FPR曲线上方，并且TPR曲线的导数递减，FPR曲线的导数递增，而KS曲线先上升到达峰值P点（导数为0）后下降，P点对应的C值就是KS值。ROC图中，ROC曲线的导数是递减的，且刚开始导数大于1，逐渐递减到导数为1的T点（T点对应P点），然后导数继续降低。另外，A值对应X值，B值对应Y值，且C=B-A=Y-X\n- 在用KS评估模型时，除了看P点对应的KS值C，还要看P点的横坐标F值的大小，F值表示的是将分数从低到高排序后的累计样本占比，F值越小，说明模型对正样本的预测越精确，也就是说在识别出正样本的同时也能保证对负样本更小的误杀率。\n- 假设F值不变，C值增大，即P点沿着垂直方向向上移动，那么A值应该减小，B值应该增大；对应地，X值减小，Y值增大，T点会向左上角移动；所以ROC曲线下方的面积会增大，也就是AUC值增大。\n- 假设C值不变，F值减小，即P点沿着水平方向向左移动，因为C=B-A，所以A和B减小相同的幅度，也是就说X和Y减小相同的幅度，即T点沿着斜率为1的切线方向向下移动，此时ROC曲线下方的面积也会增大，即AUC值增大。\n- 所以P点的位置决定了T点的位置，C值和F值均会影响AUC值。AUC值看上去更像一个综合评估指标，但缺乏对模型细节的评估。而KS值结合F值，可以评估每一段评分的效果，还可以找出评分切分的阈值等。\n\n\n\n**5.4：什么是模型的欠拟合和过拟合？**\n\n- 欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。\n- 过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。\n\n\n\n**5.5：如何判断模型是否存在过拟合或欠拟合？对应的解决方法有哪些？**\n\n- 判断模型是否存在过拟合/欠拟合主要用学习曲线，学习曲线指的是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高（过拟合）或偏差过高（欠拟合）。当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。\n- 解决欠拟合的方法：增加效果好的特征，添加多项式特征，减小正则化参数等。\n- 解决过拟合的方法：使用更多的数据，选择更加合适的模型，加入正则项等。\n\n\n\n**5.6：什么是正则化？什么是L1正则化和L2正则化？**\n\n- 正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。\n- L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。\n- L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。\n\n\n\n**5.7：正则化为什么可以防止过拟合？**\n\n最简单的解释是正则化对模型参数添加了先验，在数据少的时候，先验知识可以防止过拟合。\n\n\n\n**5.8：什么是交叉验证？交叉验证的目的是什么？有哪些优点？**\n\n交叉验证概念：\n\n交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓\"交叉\"。　\n\n交叉验证的目的：\n\n评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。\n\n交叉验证的优点：\n\n- 如果只是对数据随机划分为训练集和测试集，假如很幸运地将难以分类的样本划分进训练集中，则在测试集会得出一个很高的分数，但如果不够幸运地将难以分类的样本划分进测试集中，则会得到一个很低的分数。所以得出的结果随机性太大，不够具有代表性。而交叉验证中每个样本都会出现在训练集和测试集中各一次，因此，模型需要对所有样本的泛化能力都很好，才能使其最后交叉验证得分，及其平均值都很高，这样的结果更加稳定，全面，具有说服力。\n- 对数据集多次划分后，还可以通过每个样本的得分比较，来反映模型对于训练集选择的敏感性信息。\n- 对数据的使用更加高效，可以得到更为精确的模型。\n\n\n\n**5.8：交叉验证常用的方法有哪些？**\n\n- 标准K折交叉验证：K是自定义的数字，通常取5或10，如果设为5折，则会训练5个模型，得到5个精度值。\n- 分层K折交叉验证：如果一个数据集经过标准K折划分后，在测试集上只有一种类别，则无法给出分类器整体性能的信息，这种情况用标准K折是不合理的。而在分层K折交叉验证中，每个折中的类别比例与整个数据集类别比例相同，这样能对泛化性能做出更可靠的估计。\n- 留一法交叉验证：每次划分时，把单个数据点作为测试集，如果数据量小，能得到更好的估计结果，数据量很大时则不适用。\n- 打乱划分交叉验证：每次划分数据时为训练集取样train_size个点，为测试集取样test_size个点，将这一划分划分方法重复n_splits次。这种方法还允许每次迭代中使用部分数据，可通过设置train_size和test_size之和不为0来实现，用这种方法对数据进行二次采样可能对大型数据上的试验很用用。另外也有分层划分的形式（ StratifiedShuffleSplit），为分类任务提供更可靠的结果。\n- 分组交叉验证：适用于数据中的分组高度相关时，以group数组作为参数，group数组表示数据中的分组，在创建训练集和测试集的时候不应该将其分开，也不应该与类别标签弄混。\n\n\n\n## 参考文献\n\n1.   https://zhuanlan.zhihu.com/p/56175215 "},{"title":"神经网络公式推导","url":"/2020/06/29/神经网络公式推导/","content":"#### 神经网络的反向传播公式\n\n##### 1. 逻辑回归反向传播公式的推导\n\n逻辑回归是最简单的神经网络，先入手逻辑回归，有助于后面的理解。\n\n![](./神经网络公式推导/逻辑回归反向传播.jpg)\n\n上图是一个逻辑回归正向传播的示意图。具体细节不再描述。\n\n损失函数为：\n$$\nL(a,y) = -yloga -(1-y)log(1-a)\n$$\n反向传播的目的为了求$dw和db$，从而采用梯度下降法进行迭代优化，那么反向传播就是从后向前一步步的求微分，从而得到$dw,db$,具体过程如下：\n\n1.  $da = \\frac{dL(a,y)}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n2.  $dz = \\frac{dL(a,y)}{dz} = \\frac{dL}{da} \\frac{da}{dz}= da.g^{'}(z) = a - y$，其中sigmoid函数导数计算公式$g^{'}(z) = g(z)(1-g(z))$\n3. $dw = dz.x$\n4. $db = dz$\n\n这样就完成了逻辑回归的反向传播\n\n\n\n##### 2. 单隐层神经网络的反向传播公式推导\n\n神经网络计算中，与逻辑回归十分相似，但中间会有多层计算。下图是一个双层神经网络，有一个输入层，一个隐藏层和一个输出层。 \n\n![](./神经网络公式推导/二层神经网络反向传播.jpg)\n\n前向传播如图所示。其中$L(a^{[2]},y)$为交叉熵损失函数，假设有两个输出则：\n$$\nL(a^{[2]},y) = -yloga^{[2]} -(1-y)log(1-a^{[2]})\n$$\n反向传播公式如下：\n\n$da^{[2]} = \\frac{dL(a^{[2]},y)}{da} = -\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}$\n\n1.  $dz^{[2]} = \\frac{dL(a,y)}{dz^{[2]}} = \\frac{dL}{da^{[2]}} \\frac{da^{[2]}}{dz^{[2]}}= da^{[2]}.g^{'}(z^{[2]}) = a^{[2]} - y$，其中sigmoid函数导数计算公式$g^{'}(z) = g(z)(1-g(z))$\n2. $dw ^{[2]}= dz^{[2]}.(a^{[1]})^T$\n3. $db^{[2]} = dz^{[2]}$\n4. $dz^{[1]} = \\frac{dL}{da^{[1]}}\\frac{da^{[1]}}{dz^{[1]}}  = (w^{[2]})^Tdz^{[2]}*(g^{[1]})^{'}(z^{[1]})$\n5. $dw^{[1]} = dz^{[1]}.x^T$\n6. $db^{[1]} = dz^{[1]}$\n\n##### 3. 深层神经网络的前向和反向传播\n\n下面是一个四层的神经网络：\n\n![](./神经网络公式推导/深层神经网络反向传播.jpg)\n\n前向传播过程：\n$$\n\\begin{aligned}\nz^{[l]} &= w^{[l]}a^{[l-1]} + b^{[l]} \\\\\na^{[l]} &= g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n与单隐层神经网络反向传播类似，我们可以直接写出深层神经网络的反向传播递推公式：\n$$\n\\begin{aligned}\n\\mathrm{d}z^{[l]} &= \\mathrm{d}a^{[l]} \\cdot g^{[l]'}(z^{[l]}) \\\\\n\\mathrm{d}w^{[l]} &= \\mathrm{d}z^{[l]} \\cdot a^{[l-1]} \\\\\n\\mathrm{d}b^{[l]} &= \\mathrm{d}z^{[l]} \\\\\n\\mathrm{d}a^{[l-1]} &= w^{[l]^T} \\cdot \\mathrm{d}z^{[l]}\n\\end{aligned}\n$$\n\n\n**NOTE:上面由于前向传播使用的$WX$,导致使用正常样本时，需要将$X$取转置设置，很是不爽，于是在下面给出$XW$的情形**。\n\n这里只关注单隐层的情形：\n\n输入层，隐藏层，输出层。输入层的节点数目由数据集的维度决定（我们的数据集是2：x和y），同样的，输出层的节点数目也取决于数据集中的类别(同样是2：0和1)。值得注意的是：2个类别可以用1个节点来表示，但考虑到网络的扩展性，我们将输出节点的数目定为2。网络的输入是(x,y)坐标，输出是0或者1。用图片表示的话网络结构如下：\n\n![](./神经网络公式推导/nn-from-scratch-3-layer-network.png)\n\n隐藏层的维度（节点数）是可以设置的，节点越多，能够匹配的函数模型越复杂。但这也会耗费更多的计算资源，也会增加过拟合的可能性。隐藏层大小的设定更像是一门艺术，它需要根据问题的具体情况进行分析。稍后我们将分析隐藏层节点的个数是如何对我们的输出进行影响的。\n\n除此之外，我们还需要为隐藏层选择一个合适的激活函数。激活函数用来将该层的输入转化为输出。非线性的激活函数能够让我们做一些非线性的假设。常用的非线性激活函数有：$tanh/sigmoid/ReLU$等。本文中使用的激活函数是tanh（我个人建议用ReLU），这个激活函数的有效性也经过了很多方案的验证。一个好的激活函数具有以下性质：保证数据输入与输出也是可微的。比如说$dtanh(x)/dx = 1-（tanh(x)*tanh(x)）$。这样可以保证我们只计算一次tanh(x)的值就可以用在之后的计算导数过程中。\n\n为了使网络的输出是一个概率，所以输出层的激活函数就只能是softmax（逻辑回归只能输出二分类而softmax可以输出多个分类），这里用softmax的原因是它可以将分数转化为概率。\n\n\n**神经网络如何预测**\n\n我们设计的神经网络通过前向传播来进行预测，可以简单的将前向传播理解为一系列的矩阵乘法运算和使用激活函数的结果。如果输入$x$代表一个二维向量，那么我们计算输出$\\hat{y}$（同样也是二维）的方法如下：\n$$\n\\begin{aligned}\nz_1 &= xW_1 + b_1 \\\\\na_1 &= tanh(z_1) \\\\\nz_2 &= a_1W_2 + b_2 \\\\\na_2 &= \\hat{y} = softmax(z_2) \\\\\n\\end{aligned}\n$$\n$z_i$ 代表第$i$层的输入，$a_i$ 代表第$i$层应用激活函数后的输出 。$W_1,b_1,W_2,b_2$ 是网络的一些参数，具体的值需要通过训练数据来得到，你可以把他们看作层与层之间的矩阵变化数据。 矩阵的维度可以通过上述的矩阵乘法得到。举例说明：当我们使用的隐藏层有100个节点时，那么就有$W_1 \\in \\mathbb{R}^{2 \\times 100},b_1 \\in \\mathbb{R}^{100},W_2 \\in \\mathbb{R}^{100*2},b_2 \\in \\mathbb{R}^2$,这也可以解释为什么当节点数量增加时计算量也会随之增加。\n\n**参数的学习过程**\n\n参数学习是指神经网络寻找能使训练数据误差最小的$W_1, b_1, W_2, b_2$值的过程。我们把衡量误差的函数叫做损失函数。当输出通过$softmax$得到时，常用的损失函数分类交叉熵损失（也叫负对数似然函数）。如果有$N$组训练值和$C$个分类，我们的预测结果$\\hat{y}$与真实值$y$之间的损失定义为：\n$$\nL(y,\\hat{y}) = -\\frac{1}{N}\\sum_{n \\in N}\\sum_{i \\in C}y_{n,i}log\\hat{y}_{n,i}\n$$\n上面的式子并没有看起来那么复杂，它代表我们训练数据和预测错误时损失的累加和。$y 与 \\hat{y}$之间的差距越大，网络训练的损失也就越大。在参数学习过程中，训练损失越来越小，与训练数据的似然度也不断提高。\n\n在寻找最小值的过程中，我们可以使用梯度下降的方法。本文中实现了一种最普通的梯度下降方法，也叫固定学习率的批梯度下降方法。在实际应用中，SGD（随机梯度下降）或minibatch梯度下降法（还有Adam）可能会有更好的表现。如果你需要进一步的学习和研究\n\n\n\n梯度下降方法的输入是损失函数对于各项参数的梯度（向量的差分）：$\\frac{\\partial{L}}{\\partial{W_1}},\\frac{\\partial{L}}{\\partial{b_1}},\\frac{\\partial{L}}{\\partial{W_2}},\\frac{\\partial{L}}{\\partial{b_2}}$为了得到上述值，我们采用了著名的后向传播算法，这是一种根据输出计算梯度的有效算法。\n\n根据后向传播算法，我们可以得出以下结论：\n\n\n\n二分类的交叉熵损失函数为$L(a_2,y)$：\n$$\nL(a_2,y) = -yloga_2 -(1-y)log(1-a_2)\n$$\n反向传播公式如下：\n\n$da_2 = \\frac{dL(a_2,y)}{da} = -\\frac{y}{a_2} + \\frac{1-y}{1-a_2}$\n\n1.  $dz_2 = \\frac{dL}{da_2}\\frac{da_2}{dz_2} = a_2 - y$， 这里对应$z$ 对应$\\delta $,由于$softmax$的导数$a-(1-a_2)$\n\n2. $dW_2=  \\frac{\\partial L}{\\partial W_2}= a_1^Tdz_2$\n\n3. $db_2 =  \\frac{\\partial L}{\\partial b_2}=dz_2$\n\n4. $dz_1 = \\frac{dL}{da_1}\\frac{da_1}{dz_1}  =(g_1)^{'}(z_1) *dz_2W_2^T = (1- tanh^2z_1) * dz_2 W_2^T$\n\n5. $dW_1 = \\frac{\\partial L}{\\partial W_1} = x^Tdz_1$\n\n6. $db_1 = dz_1$\n\n   \n\n参考文献：\n\n1. 吴恩达 神经网络和深度学习\n2. https://blog.csdn.net/lst227405/article/details/56495625"},{"title":"leetcode算法总结系列1-堆","url":"/2020/06/29/leetcode算法总结系列1-堆/","content":"\n\n### Leetcode算法总结系列1-堆\n\n[TOC]\n\n##### 1.0 堆的介绍\n\n堆（Heap）是一个可以被看成近似完全二叉树的数组。树上的每一个结点对应数组的一个元素。除了最底层外，该树是完全充满的，而且是从左到右填充。—— 来自：《算法导论》\n\n堆包括最大堆和最小堆：最大堆的每一个节点（除了根结点）的值不大于其父节点；最小堆的每一个节点（除了根结点）的值不小于其父节点。\n\n堆常见的操作：\n\nHEAPIFY 建堆：把一个乱序的数组变成堆结构的数组，时间复杂度为 $O(n)$。\nHEAPPUSH：把一个数值放进已经是堆结构的数组中，并保持堆结构，时间复杂度为 $O(log\\ n)$。\nHEAPPOP：从最大堆中取出最大值或从最小堆中取出最小值，并将剩余的数组保持堆结构，时间复杂度为 $O(log\\ n)$。\nHEAPSORT：借由 HEAPFY 建堆和 HEAPPOP 堆数组进行排序，时间复杂度为 $O(n\\ log\\ n)$，空间复杂度为 $O(1)$。\n堆结构的一个常见应用是建立优先队列（Priority Queue）。\n\n##### 1.1 前K个高频单词\n\nid：692\n\nurl：https://leetcode-cn.com/problems/top-k-frequent-words/\n\n描述：\n\n给一非空的单词列表，返回前 k 个出现次数最多的单词。\n\n返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。\n\n示例 1：\n\n输入: [\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"], k = 2\n输出: [\"i\", \"love\"]\n解析: \"i\" 和 \"love\" 为出现次数最多的两个单词，均为2次。\n    注意，按字母顺序 \"i\" 在 \"love\" 之前。\n\n\n\n算法：\n\n1. 计算每个单词的频率，然后将其添加到存储到大小为 k 的小根堆中。它将频率最小的候选项放在堆的顶部。最后，我们从堆中弹出最多 k 次，并反转结果，就可以得到前 k 个高频单词。\n2. 在 Python 中，我们使用 heapq\\heapify，它可以在线性时间内将列表转换为堆，从而简化了我们的工作。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_692_topKFrequent {\n    public static List<String> topKFrequent(String[] words, int k){\n        Map<String,Integer> countMap = new HashMap<String, Integer>();\n        for(String word:words){\n            countMap.put(word,countMap.getOrDefault(word,0)+1);\n        }\n\n        PriorityQueue<String> queue = new PriorityQueue<String>(\n                (o1, o2) -> countMap.get(o1) == countMap.get(o2)?\n                        o2.compareTo(o1):countMap.get(o1) - countMap.get(o2)\n        );\n\n        for(String word:countMap.keySet()){\n            queue.offer(word);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<String> ans = new ArrayList<>();\n        while (!queue.isEmpty()){\n            ans.add(queue.poll());\n        }\n        Collections.reverse(ans);\n        return ans;\n\n    }\n\n    public static void main(String[] args) {\n        String[] words = {\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"};\n        topKFrequent(words,2);\n    }\n}\n\n```\n\n复杂度分析\n\n时间复杂度： $O(N \\log{k})$。其中 $N$ 是 words 的长度。我们用 $O(N)$的时间计算每个单词的频率，然后将 $N$ 个单词添加到堆中，添加每个单词的时间为 $O(\\log {k})$ 。最后，我们从堆中弹出最多 $k$ 次。因为 $k \\leq N$ 的值，总共是 $O(N \\log{k})$\n空间复杂度：$O(N)$,用于存储我们计数的空间\n\n\n\n```python\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, words: List[str], k: int) -> List[str]:\n        count = Counter(words)\n        return heapq.nsmallest(k,count,lambda i: (-count[i], i))\n```\n\n\n\n##### 1.2 [前 K 个高频元素](https://leetcode-cn.com/problems/top-k-frequent-elements/)\n\nid:347\n\n给定一个非空的整数数组，返回其中出现频率前 **k** 高的元素。\n\n**示例 1:**\n\n```text\n输入: nums = [1,1,1,2,2,3], k = 2\n输出: [1,2]\n```\n\n- 你可以假设给定的 *k* 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。\n- 你的算法的时间复杂度**必须**优于 O(*n* log *n*) , *n* 是数组的大小。\n\n\n\n```java\nclass Solution {\n    public List<Integer> topKFrequent(int[] nums, int k) {\n        Map<Integer,Integer> count = new HashMap<>();\n        for(int n:nums){\n            count.put(n,count.getOrDefault(n,0) + 1);\n        }\n\n        PriorityQueue<Integer> queue = new PriorityQueue<>((o1,o2) -> count.get(o1) - count.get(o2));\n\n        for(Integer i: count.keySet()){\n            queue.add(i);\n            while (queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<Integer> res = new ArrayList<>();\n        while (!queue.isEmpty()){\n            res.add(queue.poll());\n        }\n        Collections.reverse(res);\n        return res;\n    }\n}\n```\n\n```python\nfrom typing import List\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        count = Counter(nums)\n        return heapq.nlargest(k, count.keys(), count.get)\n```\n\n##### 1.3 [ 数组中的第K个最大元素](https://leetcode-cn.com/problems/kth-largest-element-in-an-array/)\n\nid:215\n\n在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。\n\n示例 1:\n\n输入: [3,2,1,5,6,4] 和 k = 2\n输出: 5\n\n```java\nclass Solution {\n    public int findKthLargest(int[] nums, int k) {\n        PriorityQueue<Integer> queue = new PriorityQueue<>(Comparator.comparingInt(n -> n));\n\n        for(int n:nums){\n            queue.add(n);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n        return queue.poll();\n    }\n}\n```\n\n\n\n```python\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        return heapq.nlargest(k,nums)[-1]\n```\n\n##### 1.4  [面试题40. 最小的k个数](https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/)\n\n输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。\n\n \n\n示例 1：\n\n输入：arr = [3,2,1], k = 2\n输出：[1,2] 或者 [2,1]\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_40_mian_getLeastNumbers {\n\n    public static int[]  getLeastNumbers(int[] arr, int k) {\n        if(k == 0) return new int[0];\n        PriorityQueue<Integer> priorityQueue = new PriorityQueue<>((a, b) -> (b -a));\n        for(int i: arr){\n            if(priorityQueue.size() < k){\n                priorityQueue.add(i);\n            }else{\n                if(priorityQueue.peek() > i){\n                    priorityQueue.remove();\n                    priorityQueue.add(i);\n                }\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (priorityQueue.size() > 0){\n            res[count++] = priorityQueue.remove();\n        }\n        return res;\n    }\n\n    public static int[] getLeastNumbers2(int[] arr, int k) {\n        if(k == 0 || arr.length == 0){\n            return new int[0];\n        }\n        PriorityQueue<Integer> heap = new PriorityQueue<>((o1, o2) -> o2 - o1);\n\n        for(int i:arr){\n            heap.offer(i);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (!heap.isEmpty()){\n            res[count++] = heap.poll();\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {3,2,1};\n        System.out.println(Arrays.toString(getLeastNumbers2(arr,2)));\n        System.out.println(Arrays.toString(getLeastNumbers(arr,2)));\n    }\n}\n\n```\n\n```scala\n    def getLeastNumbers(arr: Array[Int], k: Int): Array[Int] = {\n        val heap = scala.collection.mutable.PriorityQueue.empty[Int].reverse\n        arr.foreach(x => heap.enqueue(x))\n        (0 until k).map(_ => heap.dequeue()).toArray\n    }\n```\n\n##### 1.5 [最接近原点的 K 个点](https://leetcode-cn.com/problems/k-closest-points-to-origin/)\n\nid：973\n\n我们有一个由平面上的点组成的列表 `points`。需要从中找出 `K` 个距离原点 `(0, 0)` 最近的点。\n\n（这里，平面上两点之间的距离是欧几里德距离。）\n\n你可以按任何顺序返回答案。除了点坐标的顺序之外，答案确保是唯一的。\n\n \n\n**示例 1：**\n\n```\n输入：points = [[1,3],[-2,2]], K = 1\n输出：[[-2,2]]\n解释： \n(1, 3) 和原点之间的距离为 sqrt(10)，\n(-2, 2) 和原点之间的距离为 sqrt(8)，\n由于 sqrt(8) < sqrt(10)，(-2, 2) 离原点更近。\n我们只需要距离原点最近的 K = 1 个点，所以答案就是 [[-2,2]]。\n```\n\n二、大根堆(前 K 小) / 小根堆（前 K 大),Java中有现成的 PriorityQueue，实现起来最简单：$O(NlogK)$\n本题是求前 K 小，因此用一个容量为 K 的大根堆，每次 poll 出最大的数，那堆中保留的就是前 K 小啦（注意不是小根堆！小根堆的话需要把全部的元素都入堆，那是 O(NlogN)O(NlogN)😂，就不是 O(NlogK)O(NlogK)啦～～）\n这个方法比快排慢，但是因为 Java 中提供了现成的 PriorityQueue（默认小根堆），所以实现起来最简单，没几行代码\n\n```java\n\npackage com.strings.leetcode.heap;\n\nimport scala.actors.threadpool.Arrays;\n\nimport java.util.PriorityQueue;\n\npublic class Problem_973j_kClosest {\n    public static int[][] kClosest(int[][] points, int K) {\n        if(K == 0 || points.length == 0){\n            return new int[0][0];\n        }\n        PriorityQueue<int[]> heap = new PriorityQueue<>((o1, o2) ->\n                o2[0]*o2[0] + o2[1]*o2[1] - o1[0]*o1[0] - o1[1]*o1[1]\n        );\n\n        for(int[] p:points){\n            heap.offer(p);\n            if(heap.size() > K){\n                heap.poll();\n            }\n        }\n        int[][] res = new int[K][2];\n        int count = 0;\n        for(int[] i:heap){\n            res[count++] = i;\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[][] points = {{1,3},{-2,2}};\n        int K = 1;\n        System.out.println(Arrays.deepToString(kClosest(points,1)));\n    }\n}\n```\n\n```python\nclass Solution:\n    def kClosest(self, points: List[List[int]], K: int) -> List[List[int]]:\n        return heapq.nsmallest(K, points, key = lambda point: point[0] ** 2 + point[1] ** 2 )\n```\n\n```scala\nobject Solution {\n  def diff(arr: Array[Int]) = -arr(0)*arr(0) - arr(1)*arr(1)\n  def kClosest(points: Array[Array[Int]], K: Int): Array[Array[Int]] = {\n    val heap = scala.collection.mutable.PriorityQueue[Array[Int]]()(Ordering.by(diff))\n    points.foreach(x => heap.enqueue(x))\n    (0 until K).map(_ => heap.dequeue()).toArray\n  }\n}\n```\n\n\n\n##### 1.6 [查找和最小的K对数字](https://leetcode-cn.com/problems/find-k-pairs-with-smallest-sums/)\n\nid：373.\n\n给定两个以升序排列的整形数组 **nums1** 和 **nums2**, 以及一个整数 **k**。\n\n定义一对值 **(u,v)**，其中第一个元素来自 **nums1**，第二个元素来自 **nums2**。\n\n找到和最小的 k 对数字 **(u1,v1), (u2,v2) ... (uk,vk)**。\n\n**示例 1:**\n\n```\n输入: nums1 = [1,7,11], nums2 = [2,4,6], k = 3\n输出: [1,2],[1,4],[1,6]\n解释: 返回序列中的前 3 对数：\n     [1,2],[1,4],[1,6],[7,2],[7,4],[11,2],[7,6],[11,4],[11,6]\n```\n\n```scala\nclass Solution {\n    public List<List<Integer>> kSmallestPairs(int[] nums1, int[] nums2, int k) {\n      List<List<Integer>> pair = new ArrayList<>();\n        for (int i = 0; i < nums1.length ; i++) {\n            for (int j = 0; j < nums2.length; j++) {\n                List<Integer> res = new ArrayList<>();\n                res.add(nums1[i]);\n                res.add(nums2[j]);\n                pair.add(res);\n            }\n        }\n\n        PriorityQueue<List<Integer>> heap = new PriorityQueue<>((o1, o2) ->\n                o2.get(0) + o2.get(1) - o1.get(0) - o1.get(1)\n                );\n\n        for(List<Integer> lst:pair){\n            heap.offer(lst);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n        List<List<Integer>> ans = new ArrayList<>();\n        if(k==0 || nums1.length==0 || nums2.length == 0){\n            return ans;\n        }\n        for(List<Integer> lst:heap){\n            ans.add(lst);\n        }\n        return ans;\n\n    }\n}\n```\n\n```scala\nimport scala.collection.mutable.ArrayBuffer\n\nobject Solution {\n    def diff(num: List[Int]) = {-num(0) - num(1)}\n    def kSmallestPairs(nums1: Array[Int], nums2: Array[Int], k: Int): List[List[Int]] = {\n        if(k==0||nums1.length== 0){ List()}\n        else{\n        val pair:ArrayBuffer[List[Int]] = new ArrayBuffer[List[Int]]()\n        for(i <- 0 until nums1.length){\n            for(j <- 0 until nums2.length){\n                pair.append(List(nums1(i),nums2(j)))\n            }\n        }\n        val heap = scala.collection.mutable.PriorityQueue[List[Int]]()(Ordering.by(diff))\n        pair.toArray.foreach(x => heap.enqueue(x))\n        var tmpk= k\n        if(heap.size < k){\n            tmpk = heap.size\n        }\n        (0 until tmpk).map(_ => heap.dequeue()).toList\n        }\n\n    }\n}\n```\n\n##### 1.7  [数据流中的中位数](https://leetcode-cn.com/problems/shu-ju-liu-zhong-de-zhong-wei-shu-lcof/)\n\n###### [数据流的中位数](https://leetcode-cn.com/problems/find-median-from-data-stream/)\n\n###### [连续中值](https://leetcode-cn.com/problems/continuous-median-lcci/)\n\n这三个题都是一样的。\n\n将输入的数分成两部分：较小的一部分和较大的一部分\n\n1. lowPart ：定义为较小的一部分，用最大堆\n   允许lowPart的大小比highPart多1\n2. highPart ： 定义为较大的一部分，用最小堆\n   如果size是奇数，那么中位数就是lowPart的最大值，也就是堆顶\n\n否则，最大值是lowPart和highPart的堆顶平均值\n\n维护\n\n每进入一个数，先加入lowPart，然后将lowPart的最大值（堆顶）移出到highPart\n\n如果这时size是奇数，此时highPart将最小值移出到lowPart\n\n\n\n```java\nclass MedianFinder {\n\n        private PriorityQueue<Integer> lowPart;\n    private PriorityQueue<Integer> highPart;\n    int size;\n    /** initialize your data structure here. */\n    public MedianFinder() {\n        lowPart = new PriorityQueue<Integer>((x, y) -> y - x);  //最大堆\n        highPart = new PriorityQueue<Integer>();\n        size = 0;\n    }\n\n    public void addNum(int num) {\n        size++;\n        lowPart.offer(num);\n        highPart.offer(lowPart.poll());\n        if((size & 1) == 1){\n            lowPart.offer(highPart.poll());\n        }\n    }\n\n    public double findMedian() {\n        if((size & 1) == 1){\n            return (double) lowPart.peek();\n        }else{\n            return (double) (lowPart.peek() + highPart.peek()) / 2;\n        }\n    }\n}\n\n/**\n * Your MedianFinder object will be instantiated and called as such:\n * MedianFinder obj = new MedianFinder();\n * obj.addNum(num);\n * double param_2 = obj.findMedian();\n */\n```\n\n```python\nclass MedianFinder:\n\n    def __init__(self):\n        # 当前大顶堆和小顶堆的元素个数之和\n        self.count = 0\n        self.max_heap = []\n        self.min_heap = []\n\n    def addNum(self, num: int) -> None:\n        self.count += 1\n        # 因为 Python 中的堆默认是小顶堆，所以要传入一个 tuple，用于比较的元素需是相反数，\n        # 才能模拟出大顶堆的效果\n        heapq.heappush(self.max_heap, (-num, num))\n        _, max_heap_top = heapq.heappop(self.max_heap)\n        heapq.heappush(self.min_heap, max_heap_top)\n        if self.count & 1:\n            min_heap_top = heapq.heappop(self.min_heap)\n            heapq.heappush(self.max_heap, (-min_heap_top, min_heap_top))\n\n    def findMedian(self) -> float:\n        if self.count & 1:\n            # 如果两个堆合起来的元素个数是奇数，数据流的中位数大顶堆的堆顶元素\n            return self.max_heap[0][1]\n        else:\n            # 如果两个堆合起来的元素个数是偶数，数据流的中位数就是各自堆顶元素的平均值\n            return (self.min_heap[0] + self.max_heap[0][1]) / 2\n\n        \n\n\n# Your MedianFinder object will be instantiated and called as such:\n# obj = MedianFinder()\n# obj.addNum(num)\n# param_2 = obj.findMedian()\n```\n\n##### 1.8 [距离相等的条形码](https://leetcode-cn.com/problems/distant-barcodes/)\n\nid：1054\n\n在一个仓库里，有一排条形码，其中第 `i` 个条形码为 `barcodes[i]`。\n\n请你重新排列这些条形码，使其中两个相邻的条形码 **不能** 相等。 你可以返回任何满足该要求的答案，此题保证存在答案。\n\n \n\n**示例 1：**\n\n```\n输入：[1,1,1,2,2,2]\n输出：[2,1,2,1,2,1]\n```\n\n**示例 2：**\n\n```\n输入：[1,1,1,1,2,2,3,3]\n输出：[1,3,1,3,2,1,2,1]\n```\n\n贪心堆\n\n```java\npackage com.strings.leetcode.heap;\n\n\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\nclass Problem_1054j_rearrangeBarcodes {\n    public static int[] rearrangeBarcodes2(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2){\n            return barcodes;\n        }\n        Map<Integer,Integer> countMap = new HashMap<>();\n        for(int b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(int i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1){\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = countMap.get(a);\n            int freqB = countMap.get(b);\n            if(freqA > 1){\n                countMap.put(a,freqA-1);\n                maxHeap.offer(a);\n            }\n            if(freqB > 1){\n                countMap.put(b,freqB-1);\n                maxHeap.add(b);\n            }\n        }\n        if(maxHeap.size() > 0){\n            res[idx] = maxHeap.poll();\n        }\n\n        return res;\n    }\n\n    public static int[] rearrangeBarcodes(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2) return barcodes;\n        Map<Integer, Integer> map = new HashMap<>();\n        for(int i : barcodes) {\n            map.put(i, map.getOrDefault(i, 0) + 1);\n        }\n        //大顶堆\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> map.get(b) - map.get(a));\n        for(int i : map.keySet()) {\n            maxHeap.offer(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1) {\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = map.get(a);\n            int freqB = map.get(b);\n            if(freqA > 1) {\n                map.put(a, freqA - 1);\n                maxHeap.add(a);\n            }\n            if(freqB > 1) {\n                map.put(b, freqB - 1);\n                maxHeap.add(b);\n            }\n        }\n        //收尾\n        if(maxHeap.size() > 0) res[idx] = maxHeap.poll();\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] bor = {1,1,2};\n        System.out.println(Arrays.toString(rearrangeBarcodes(bor)));\n        System.out.println(Arrays.toString(rearrangeBarcodes2(bor)));\n    }\n}\n\n```\n\n##### 1.9 [ 重构字符串](https://leetcode-cn.com/problems/reorganize-string/)\n\n给定一个字符串S，检查是否能重新排布其中的字母，使得两相邻的字符不同。\n\n若可行，输出任意可行的结果。若不可行，返回空字符串。\n\n示例 1:\n\n输入: S = \"aab\"\n输出: \"aba\"\n示例 2:\n\n输入: S = \"aaab\"\n输出: \"\"\n\n\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\npublic class Problem_767_reorganizeString {\n\n    public static String reorganizeString(String S) {\n        if(S == null || S.length() < 2){\n            return S;\n        }\n        char[] barcodes = S.toCharArray();\n            Map<Character,Integer> countMap = new HashMap<>();\n            for(char b:barcodes){\n                countMap.put(b,countMap.getOrDefault(b,0)+1);\n            }\n            if(Collections.max(countMap.values()) > (S.length()+1)/2){\n                return \"\";\n            }\n            PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n            for(char i:countMap.keySet()){\n                maxHeap.add(i);\n            }\n            char[] res = new char[barcodes.length];\n            int idx = 0;\n            while(maxHeap.size() > 1){\n                char a = maxHeap.poll();\n                char b = maxHeap.poll();\n                res[idx++] = a;\n                res[idx++] = b;\n                int freqA = countMap.get(a);\n                int freqB = countMap.get(b);\n                if(freqA > 1){\n                    countMap.put(a,freqA-1);\n                    maxHeap.offer(a);\n                }\n                if(freqB > 1){\n                    countMap.put(b,freqB-1);\n                    maxHeap.add(b);\n                }\n            }\n            if(maxHeap.size() > 0){\n                res[idx] = maxHeap.poll();\n            }\n            return String.valueOf(res);\n    }\n\n    public static void main(String[] args) {\n        String S1 = \"aab\";\n        String S2 = \"aaab\";\n\n        System.out.println(reorganizeString(S1));\n        System.out.println(reorganizeString(S2));\n    }\n}\n\n```\n\n##### 1.10 [有序矩阵中第K小的元素](https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/)\n\nid:378\n\n给定一个 n x n 矩阵，其中每行和每列元素均按升序排序，找到矩阵中第k小的元素。\n请注意，它是排序后的第 k 小元素，而不是第 k 个不同的元素。\n\n \n\n示例:\n\nmatrix = [\n   [ 1,  5,  9],\n   [10, 11, 13],\n   [12, 13, 15]\n],\nk = 8,\n\n返回 13。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Comparator;\nimport java.util.PriorityQueue;\n\npublic class Problem_378_kthSmallest {\n    public static int kthSmallest(int[][] matrix, int k) {\n        PriorityQueue<Integer> heap = new PriorityQueue<>(Comparator.reverseOrder());\n        for(int[] arr:matrix){\n            for(int i: arr){\n                heap.offer(i);\n                if(heap.size() > k){\n                    heap.poll();\n                }\n            }\n        }\n        return heap.peek();\n    }\n\n    public static void main(String[] args) {\n        int[][] matrix = {{1,  5,  9},{10, 11, 13},{12, 13, 15}};\n        System.out.println(kthSmallest(matrix,8));\n    }\n\n}\n```\n\n##### 1.11 [根据字符出现频率排序](https://leetcode-cn.com/problems/sort-characters-by-frequency/)\n\nid:451\n\n给定一个字符串，请将字符串里的字符按照出现的频率降序排列。\n\n示例 1:\n\n输入:\n\"tree\"\n\n输出:\n\"eert\"\n\n解释:\n'e'出现两次，'r'和't'都只出现一次。\n因此'e'必须出现在'r'和't'之前。此外，\"eetr\"也是一个有效的答案。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_451j_frequencySort {\n\n    public static String frequencySort(String s){\n        if(s == null || s.length() < 2){\n            return s;\n        }\n        char[] barcodes = s.toCharArray();\n        Map<Character,Integer> countMap = new HashMap<>();\n        for(char b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n\n        PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(char i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n\n        StringBuilder sb = new StringBuilder();\n        while (!maxHeap.isEmpty()){  // 注意不能使用for(char c:maxHeap)\n            char a = maxHeap.poll();\n            int freq = countMap.get(a);\n            for (int i = 0; i < freq; i++) {\n                sb.append(a);\n            }\n        }\n\n        return sb.toString();\n    }\n\n    public static void main(String[] args) {\n        String s  = \"Aabb\";\n        System.out.println(frequencySort(s));\n    }\n}\n\n```\n\n"},{"title":"聚类算法总结","url":"/2020/05/19/聚类算法总结/","content":"### 聚类算法总结\n\n聚类是一种经典的无监督学习方法，无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。\n\n聚类直观上来说是将相似的样本聚在一起，从而形成一个类簇（cluster）。那首先的问题是如何来度量相似性（similarity measure）呢？这便是距离度量，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是性能度量，性能度量为评价聚类结果的好坏提供了一系列有效性指标。\n\n#### 1 Kmeans聚类\n\nK-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛：\n\n假设输入空间 $\\cal X \\in \\R^n \\ $ 为$\\ n\\ $维向量的集合，$\\ \\cal{X}=\\{x^{(1)} ,x^{(2)},\\cdots,x^{(m)} \\} \\ $，$ \\ \\mathcal  C\\ $为输入空间$\\ \\cal X\\ $的一个划分，不妨令$\\ \\mathcal C=\\{ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\} \\ $，因此可以定义$\\ k\\text{-}means\\ $算法的损失函数为\n$$\nJ(\\mathcal C)=\\sum\\limits_{k=1}^K\\sum\\limits_{x^{(i)}\\in \\mathbb C_k}\\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2  \\tag{1}\n$$\n其中$\\ \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)}  \\ $是簇$\\ \\mathbb C_k\\ $的聚类中心。\n\n事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数$J(\\mathcal C)$\n\n##### 1.1 算法流程\n\n1. 首先随机初始化$\\ K\\ $个聚类中心，$\\ \\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(K)} \\ $；\n\n2. 然后根据这$\\ K\\ $个聚类中心给出输入空间$\\ \\mathcal X \\ $的一个划分，$\\ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\ $；\n\n   - 样本离哪个簇的聚类中心最近，则该样本就划归到那个簇\n     $$\n     \\mathop{\\arg\\min}_{k}\\ \\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2 \\tag{2}\n     $$\n\n3. 再根据这个划分来更新这$\\ K\\ $个聚类中心\n   $$\n   \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)} \\tag{3}\n   $$\n\n4. 重复2、3步骤直至收敛\n\n   - 即$\\ K\\ $个聚类中心不再变化\n\n##### 1.2  算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.model.Model\nimport com.strings.data.Data\n\nimport scala.collection.mutable.ListBuffer\nimport scala.util.Random\n\nclass Kmeans(val k:Int = 3,\n             val max_iter:Int = 100,\n             val seed:Long = 1234L,\n             val tolerance: Double = 1e-4) extends Model{\n\n  private var centroids = List[DenseVector[Double]]()\n  private var cluster = ListBuffer[(Int,DenseVector[Double])]()\n\n  var iterations:Int = 0\n\n  def _init_random_centroids(data : List[DenseVector[Double]]):List[DenseVector[Double]] = {\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closest_centroid2(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n        var close_i = 0\n        var closest_dist = -1.0\n        centroids.zipWithIndex.foreach(centroid => {\n          val distance = squaredDistance(centroid._1,row)\n          if(closest_dist>distance || closest_dist == -1.0){\n            closest_dist = distance\n            close_i = centroid._2\n          }\n        })\n    (close_i,row)\n  }\n\n  def _closest_centroid(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n      val distWithIndex =  centroids.zipWithIndex.map(x =>\n                          (squaredDistance(x._1,row),x._2)\n                          ).minBy(_._1)\n      (distWithIndex._2,row)\n  }\n\n  def train(data:List[DenseVector[Double]]):Unit = {\n    centroids = _init_random_centroids(data)\n    var flag = true\n    for(_ <- Range(0,max_iter) if flag){\n      iterations += 1\n      data.foreach{d =>\n        val b = _closest_centroid(centroids, d)\n        cluster.append(b)\n      }\n      val prev_centroid = centroids\n      centroids = _calculate_centroids(cluster)\n      cluster = ListBuffer[(Int,DenseVector[Double])]()\n      val diff = prev_centroid.zip(centroids).map(x => squaredDistance(x._2,x._1))\n      if( diff.sum < tolerance){\n        flag = false\n      }\n    }\n\n  }\n\n  def predit(data:List[DenseVector[Double]]):List[(Int,DenseVector[Double])]= {\n    data.map(x => _closest_centroid(centroids,x))\n  }\n\n  def _calculate_centroids(cluster:ListBuffer[(Int,DenseVector[Double])]):List[DenseVector[Double]]= {\n    cluster.groupBy(_._1).map { x =>\n      val temp = x._2.map(_._2)\n      temp.reduce((a, b) => a :+ b).map(_ / temp.length)\n    }.toList\n  }\n\n  /**\n   * @param x input matrix\n   * @return predict vector value\n   */\n  override def predict(x: DenseMatrix[Double]): DenseVector[Double] = {\n    DenseVector[Double]()\n  }\n}\n\nobject Kmeans{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).map(DenseVector(_)).toList\n    val kmeans = new Kmeans(max_iter = 100)\n    kmeans.train(data)\n    println(kmeans.centroids)\n\n  }\n}\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala \n\n#### 2 GMM 聚类\n\nGMM聚类又称高斯混合聚类，即采用高斯分布来描述原型。现假设每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成。\n\n##### 2.1 算法过程\n\n高斯分布的定义，对于$n$维样本空间$\\mathcal X$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为\n$$\np(x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}  \\tag{4}\n$$\n其中$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵,由(4)可以看出，高斯分布完全由均值向量和协方差矩阵$\\Sigma$这两个参数确定。为了明确高斯分布与相应的参数的依赖关系，将概率密度函数记为$p(x|\\mu,\\Sigma)$.\n\n我们可以定义高斯混合分布\n$$\np_{\\mathcal M}(x) = \\sum_{i=1}{k}\\alpha_i.p(x|\\mu_i,\\Sigma_i) \\tag{5}\n$$\n$α_i$称为混合系数,满足$\\sum_{i=1}^{k}\\alpha_i=1$，假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,...,\\alpha_k$定义的先验分布选择高斯混合成分，其中$α_i$为选择第$i$混合成分的概率，然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。\n\n若训练集$D={x_1,x_2,...,x_m}$由上述过程生成，令随机变量$z_j \\in{1,2,...,k}$表示生成样本的高斯混合成分，根据贝叶斯定理，$z_j$的后验分布对应于\n$$\np_{\\mathcal M}(z_j = i | x_j) = \\frac{P(z_j=i).p_{\\mathcal M}(x_j|z_j=i)}{p_{\\mathcal M}(x_j)} \\\\\n=\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}   \\tag{6}\n$$\n当高斯混合分布(5)已知时，高斯混合聚类将样本集$D$划分为$k$个簇$\\mathcal C = {C_1,C_2,...,C_k}$,将每个样本$x_j$的簇标记为$\\lambda_j$如下确定：\n$$\n\\lambda_j = \\arg \\max _{i\\in{1,2...,k}}\\gamma_{ji} \\tag{7}\n$$\n对于(5)式，模型参数$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$如何求解？显然给定样本集$D$，可以采用极大似然估计，即最大化对数似然：\n$$\nLL(D) = \\ln(\\prod_{j}^mp_{\\mathcal M}(x_j)) \\\\\n=\\sum_{j=1}^{m}ln(\\sum_{i=1}^{k}\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)) \\tag{8}\n$$\n采用EM算法进行迭代优化求解，下面做个简单地推导：\n\n若参数$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$能使(8)式最大化，则由$\\frac{\\partial LL(D)}{\\partial \\mu_i} = 0$有：\n$$\n\\sum_{j=1}^{m}\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}(x_j - \\mu_i) \\tag{9}\n$$\n由(6)式以及$\\gamma_{ji} = p_{\\mathcal M}(z_j = i |x_j)$有\n$$\n\\mu_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{10}\n$$\n即混合成分的均值可以通过样本的加权平均来估计，样本权重是每个样本属于该成分的后验概率，类似的，由$\\frac{\\partial LL(D)}{\\partial \\Sigma_i} = 0$可以得到：\n$$\n\\Sigma_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}(x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{11}\n$$\n对于混合系数，使用拉格朗日法可以得到：\n$$\n\\alpha_i = \\frac{1}{m}\\sum_{j=1}^{m}\\gamma_{ji} \\tag{12}\n$$\n即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定。\n\n由上述推导可得高斯混合模型的EM算法：在每步迭代中，先根据当前参数来计算每个样本属于高斯成分的后验概率$\\gamma_{ji}$(**E步**), 再根据式(10)-(12)更新参数模型$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$（**M步**）.\n\n##### 2.2  算法流程\n\nGMM算法步骤如下：\n\n![](./聚类算法总结/gmm_算法步骤.png)\n\n##### 2.3 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{*, Axis, DenseMatrix, DenseVector, argmax, det, max, norm, pinv, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport com.strings.utils.MatrixUtils\nimport org.slf4j.LoggerFactory\n\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass GMM(k:Int = 3,\n          max_iterations:Int = 2000,\n          tolerance:Double = 1e-8) {\n\n  private val logger = LoggerFactory.getLogger(classOf[GMM])\n\n  var means:Array[DenseVector[Double]] = new Array[DenseVector[Double]](k)\n  var vars:Array[DenseMatrix[Double]] = new Array[DenseMatrix[Double]](k)\n  var sample_assignments:DenseVector[Int] = _\n  var priors:DenseVector[Double] = _\n  var responsibility:DenseMatrix[Double] = _\n  var responsibilities:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n\n  def _initialize(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val X_lst = (0 until n_samples).map(X.t(::,_))\n    val rng =  new scala.util.Random()\n    priors = DenseVector.ones[Double](k) :/ k.toDouble\n    means = rng.shuffle(X_lst).take(k).toArray\n//    means = X_lst.take(k).toArray\n    for(i <- 0 until k){\n      vars(i) = MatrixUtils.calculate_covariance_matrix(X)\n    }\n  }\n\n  def multivariate_gaussian(X:DenseMatrix[Double],i:Int):DenseVector[Double]={\n    val n_features = X.cols\n    val mean = means(i)\n    val covar = vars(i)\n    val determinant = det(covar)\n    val likelihoods = DenseVector.zeros[Double](X.rows)\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n\n    for((sample,index) <- X_arr.zipWithIndex){\n      val d = n_features\n      val coeff = 1.0 / (math.pow(2 * Math.PI,d/2) * math.sqrt(determinant))\n      val gram = (sample :- mean).t * pinv(covar) * (sample :- mean)\n      val exponent = math.exp(-0.5 * gram)\n      likelihoods(index) = coeff * exponent\n    }\n    likelihoods\n  }\n\n  def _get_likelihoods(X:DenseMatrix[Double]):DenseMatrix[Double] = {\n    val n_samples = X.rows\n    val likelihoods = DenseMatrix.zeros[Double](n_samples,k)\n    for(i <- 0 until k){\n      likelihoods(::,i) := multivariate_gaussian(X,i)\n    }\n    likelihoods\n  }\n\n  def _expectation(X:DenseMatrix[Double]): Unit ={\n    val weighted_likelihoods = _get_likelihoods(X)(*,::).map(x => x :* priors)\n    val sum_likelihoods = sum(weighted_likelihoods,Axis._1)\n    responsibility = weighted_likelihoods(::,*).map(x => x :/ sum_likelihoods) // 列除\n    sample_assignments = argmax(responsibility,Axis._1)\n    responsibilities.append(max(responsibility,Axis._1))\n  }\n\n  def _maximization(X:DenseMatrix[Double]): Unit ={\n    for(i <- 0 until k){\n      val resp = responsibility(::,i)\n      val mean = sum(X(::,*).map(f => resp :* f),Axis._0) :/ sum(resp)\n      means(i) = mean.t\n      val diff = X(*,::).map(f => f :- mean.t)\n      val covariance = diff.t * diff(::,*).map(f => f :* resp) :/sum(resp) // 注意diff(::,*)是取列运算\n      vars(i) = covariance\n    }\n    val n_samples = X.rows\n    priors = sum(responsibility,Axis._0).t :/ n_samples.toDouble\n  }\n\n  def predict(X:DenseMatrix[Double]): DenseVector[Double] = {\n    _initialize(X)\n    var iter = 0\n    var flag = true\n    for (_ <- 0 until max_iterations if flag) {\n      iter += 1\n      _expectation(X)\n      _maximization(X)\n      breakable {\n        if (responsibilities.length < 2) {\n          break()\n        }else{\n          val n = responsibilities.length\n          val diff = norm(responsibilities(n-1) - responsibilities(n-2), 2)\n          println(diff)\n          if (diff <= tolerance) flag = false\n        }\n    }\n  }\n    logger.info(s\"$iter 之后收敛\")\n    _expectation(X)\n    sample_assignments.map(_.toDouble)\n  }\n\n}\n\nobject GMM{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val gmm = new GMM(max_iterations = 100)\n    gmm._initialize(DenseMatrix(data:_*))\n\n    val pred = gmm.predict(DenseMatrix(data:_*))\n    println(pred)\n\n  }\n}\n```\n\n详细请参考:https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/GMM.scala\n\n#### 3 DBSCAN聚类\n\n密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。\n\nDBSCAN是一种著名的密度聚类算法，它是一组“邻域”（neighbhood）参数($\\epsilon $,MinPts)来刻画样本分布的紧密程度.给定数据集$D = \\{x_1,x_2,...,x_m\\}$,定义下面几个概念：\n\n1. $\\epsilon$-邻域：对于$x_j \\in D$,其$\\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j) = {x_i \\in D | dist(x_i,x_j) \\leq \\epsilon}$;\n2. 核心对象(core object):若$x_j$的$\\epsilon$-邻域至少包含MinPts个样本，即$|N_{\\epsilon}(x_j)| \\geq MinPts$,则$x_j$是一个核心对象；\n3. 密度直达(directly density-reachable):若$x_j$位于$x_i$的$\\epsilon$-邻域中，且$x_i$是核心对象，则称$x_j$是由$x_i$密度直达；\n4. 密度可达(density-reachable)：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,...,p_n$,其中$p_1 = x_i,p_n = x_j$,且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；\n5. 密度相连(density-connected):对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连；\n\n下图给出了上述概念的直观显示：\n\n![](./聚类算法总结/dbscan_密度直达.png)\n\n##### 3.1 算法步骤\n\nDBSCAN的算法的思想是找出一个核心对象所有密度可达的样本集合形成簇。首先从数据集中任选一个核心对象$A$，找出所有$A$密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：\n\n![](./聚类算法总结/dbscan_算法步骤.png)\n\n##### 3.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass DBSCAN(eps:Double = 1.0,\n             min_samples:Int = 5) {\n\n  var visited_samples:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n  var neighbors: Map[Int, Array[Int]] = Map()\n  var X:DenseMatrix[Double] = _\n  var clusters:ArrayBuffer[Array[Int]] = new ArrayBuffer[Array[Int]]()\n\n  def euclidean_distance(x1:DenseVector[Double],x2:DenseVector[Double]): Double ={\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def _get_neighbors(sample_i:Int): Array[Int] ={\n    val neighbors:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n    val X_arr2 =  X_arr.indices.filter(i => i != sample_i).map(X_arr(_))\n    for((_sample,inx) <- X_arr2.zipWithIndex){\n      val dist = euclidean_distance(_sample,X_arr(sample_i))\n      if(dist < eps){\n        neighbors.append(inx)\n      }\n    }\n    neighbors.toArray\n  }\n\n  def _expand_cluster(sample_i:Int, neighbor:Array[Int]): Array[Int] ={\n    val cluster:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    cluster.append(sample_i)\n    for(neighbor_i <- neighbor){\n      if(!visited_samples.contains(neighbor_i)){\n        visited_samples.append(neighbor_i)\n        neighbors += (neighbor_i -> _get_neighbors(neighbor_i) )\n        if (neighbors(neighbor_i).length >= min_samples){   //        neighbors.get(neighbor_i).size 结果是1\n          val expanded_cluster = _expand_cluster(neighbor_i,neighbors(neighbor_i))\n          cluster.append(expanded_cluster:_*)\n        }else{\n          cluster.append(neighbor_i)\n        }\n      }\n    }\n    cluster.toArray\n  }\n\n  def _get_cluster_labels(): Array[Int] ={\n    val labels = Array.fill(X.rows)(clusters.length)\n    for((cluster,cluster_i) <- clusters.zipWithIndex){\n      for(sample_i <- cluster){\n        labels(sample_i) = cluster_i\n      }\n    }\n    labels\n  }\n\n  def predict(XX:DenseMatrix[Double]): Array[Int] ={\n    X = XX\n    visited_samples = new ArrayBuffer[Int]()\n    neighbors = Map()\n    val n_samples = X.rows\n    for(sample_i <- 0 until n_samples){\n      breakable {\n        if (visited_samples.contains(sample_i)) {\n          break()\n        }else{\n          neighbors += (sample_i -> _get_neighbors(sample_i))\n          if(neighbors.get(sample_i).size >= min_samples){\n            visited_samples.append(sample_i)\n          }\n          val new_cluster = _expand_cluster(sample_i,neighbors(sample_i))\n          clusters.append(new_cluster)\n        }\n      }\n    }\n    _get_cluster_labels()\n  }\n\n}\n\nobject DBSCAN{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val dbscan = new DBSCAN(eps = .7,min_samples = 5)\n\n    val pred = dbscan.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n  }\n}\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/DBSCAN.scala\n\n#### 4 PAM聚类算法\n\nK-means是每次选**簇的**均值**作为新的中心，迭代直到簇中对象分布不再变化。其缺点是对于离群点是敏感的，因为一个具有很大极端值的对象会扭曲数据分布。那么我们可以考虑新的簇中心不选择均值而是选择**簇内的某个对象**，只要使总的代价降低就可以。\n\nPAM（partitioning around medoid，围绕中心点的划分）是具有代表性的k-medoids算法。\n\n它最初随机选择k个对象作为中心点，该算法反复的用非代表对象（非中心点）代替代表对象，试图找出更好的中心点，以改进聚类的质量。 K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此， PAM可以容纳混合数据类型，并且不仅限于连续变量。\n\n##### 4.1  算法步骤\n\n​       PAM算法如下：\n​       (1) 随机选择K个观测值（每个都称为中心点）；\n​       (2) 计算观测值到各个中心的距离/相异性；\n​       (3) 把每个观测值分配到最近的中心点；\n​       (4) 计算每个中心点到每个观测值的距离的总和（总成本）；\n​       (5) 选择一个该类中不是中心的点，并和中心点互换；\n​       (6) 重新把每个点分配到距它最近的中心点；\n​       (7) 再次计算总成本；\n​       (8) 如果总成本比步骤(4)计算的总成本少，把新的点作为中心点；\n​       (9) 重复步骤(5)～(8)直到中心点不再改变。\n\n##### 4.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\n\n/**\n *  Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.\n *\n * @param k nums of cluster\n */\n\n\nclass PAM(k:Int = 2,seed:Long = 1234L) {\n\n  def _init_random_medoids(X: DenseMatrix[Double]): IndexedSeq[DenseVector[Double]] ={\n    val n_samples = X.rows\n    val data = (0 until n_samples).map(X.t(::,_))\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closet_medoid(sample:DenseVector[Double],medoids:IndexedSeq[DenseVector[Double]]): Int ={\n    val distWithIndex =  medoids.zipWithIndex.map(x =>\n      (squaredDistance(x._1,sample),x._2)\n    ).minBy(_._1)\n    distWithIndex._2\n  }\n\n  def _create_clusters(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[Array[Int]] ={\n    val clusterss = new Array[Int](X.rows)\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((sample,inx) <- data.zipWithIndex){\n      val medoid_i = _closet_medoid(sample,medoids)\n      clusterss(inx) = medoid_i\n    }\n    clusterss.zipWithIndex.groupBy(_._1).toArray.sortBy(_._1).map(_._2.map(_._2))\n  }\n\n  def _calculate_cost(X:DenseMatrix[Double],clusters:Array[Array[Int]],medoids:IndexedSeq[DenseVector[Double]]):Double={\n    var cost = 0.0\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((cluster,i) <- clusters.zipWithIndex){\n      val medoid = medoids(i)\n      for(sample_i <- cluster){\n        cost += squaredDistance(data(sample_i),medoid)\n      }\n    }\n    cost\n  }\n\n  def _get_cluster_labels(clusters:Array[Array[Int]],X:DenseMatrix[Double]): Array[Int] ={\n    val y_pred = Array.fill(X.rows)(0)\n    for(cluster_i <- 0 until clusters.length){\n      val cluster = clusters(cluster_i)\n      for(sample_i <- cluster){\n        y_pred(sample_i) = cluster_i\n      }\n    }\n    y_pred\n  }\n  def _get_no_medoids(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[DenseVector[Double]] ={\n    val non_medoids:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n    val data = (0 until X.rows).map(X.t(::,_))\n\n    for(sample <- data){\n      if(!medoids.contains(sample)) non_medoids.append(sample)\n    }\n    non_medoids.toArray\n  }\n\n  def predict(X:DenseMatrix[Double]): Array[Int] ={\n    var medoids = _init_random_medoids(X)\n    val clusters = _create_clusters(X,medoids)\n    var cost = _calculate_cost(X, clusters, medoids)\n    breakable {\n      while (true) {\n        var best_medoids = medoids\n        var lowest_cost = cost\n        for (medoid <- medoids) {\n          val non_medoids = _get_no_medoids(X, medoids)\n          for (sample <- non_medoids) {\n            val new_medoids = new Array[DenseVector[Double]](medoids.length)\n            for (i <- 0 until medoids.length) {\n              new_medoids(i) = medoids(i)\n            }\n            val inx: IndexedSeq[Int] = medoids.indices.filter(i => medoids(i) == medoid)\n            inx.foreach(i => new_medoids(i) = sample)\n\n            val new_clusters = _create_clusters(X, new_medoids)\n            val new_cost = _calculate_cost(X, new_clusters, new_medoids)\n\n            if (new_cost < lowest_cost) {\n              lowest_cost = new_cost\n              best_medoids = new_medoids\n            }\n          }\n        }\n        if (lowest_cost < cost) {\n          cost = lowest_cost\n          medoids = best_medoids\n        } else {\n          break()\n        }\n      }\n    }\n    val finaly_clusters = _create_clusters(X,medoids)\n    _get_cluster_labels(finaly_clusters,X)\n  }\n\n}\n\nobject PAM{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val pam = new PAM(k = 3)\n\n    val pred = pam.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n    val acc =  Metric.accuracy(pred.map(_.toDouble),target) * 100\n    println(f\"准确率为: $acc%-5.2f%%\")\n  }\n}\n\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/PAM.scala\n\n#### 5. LVQ聚类\n\nLVQ又称“学习向量量化”(Learning Vector Quantization)也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ假设样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。\n\n给定样本集$\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$,每个样本$x_j$是由$n$个属性描述的特征向量$(x_{j1};x_{j2};...;x_{jn})$,$y_j \\in \\mathcal Y$是样本$x_j$的类别标记. LVQ的目标是学得一组$n$维原型向量$\\{p_1,p_2,...,p_q\\}$,每个原型向量代表一个聚类簇，簇标记为$t_i \\in \\mathcal Y$.\n\n##### 5.1 算法步骤\n\nLVQ的算法步骤如下：\n\n![](./聚类算法总结/lvq_算法步骤.png)\n\n##### 5.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, argmin, sum}\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\n\nclass LVQ(t:Array[Int],\n          lr:Double = 0.1,\n          nums_iters:Int = 400) {\n\n  val c = t.distinct.length\n  val q = t.length\n  var C: Map[Int, ArrayBuffer[Int]] = Map()\n  var p: DenseMatrix[Double] = _\n  var labels: DenseVector[Int] = _\n\n  def euclidean_distance(x1: DenseVector[Double], x2: DenseVector[Double]): Double = {\n    require(x1.length == x2.length)\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def fit(X: DenseMatrix[Double], y: DenseVector[Int]) = {\n    p = DenseMatrix.zeros[Double](q, X.cols)\n    for (i <- 0 until q) {\n      C += (i -> ArrayBuffer[Int]())\n      val candidate_indices = y.toArray.indices.filter(f => y(f) == t(i))\n      val target_indice = Random.shuffle(candidate_indices.toList).take(1).apply(0)\n      p(i, ::) := X(target_indice, ::)\n    }\n\n\n    var p_arr = (0 until p.rows).map(p.t(::, _))\n    for (_ <- 0 until nums_iters) {\n      val j = Random.shuffle(Range(0, y.length).toList).take(1).apply(0)\n      val x_j = X(j, ::).t\n      val d = p_arr.map(f => euclidean_distance(f, x_j))\n      val idx: Int = argmin(d.toArray)\n      if (y(j) == t(idx)) {\n        p(idx, ::) := p(idx, ::) :+ ((X(j, ::) :- p(idx, ::)) :* lr)  // :+ 和 :* 运算优先级一致\n      } else {\n        p(idx, ::) := p(idx, ::) :- ((X(j, ::) :- p(idx, ::)) :* lr)\n      }\n\n    }\n    p_arr = (0 until p.rows).map(p.t(::, _))\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      C(idx).append(j)\n    }\n\n    labels = DenseVector.zeros[Int](X.rows)\n    for (i <- 0 until q) {\n      for (j <- C(i)) {\n        labels(j) = i\n      }\n    }\n  }\n\n  def predict(X: DenseMatrix[Double]): DenseVector[Int] = {\n    val p_arr = (0 until p.rows).map(p.t(::, _))\n    val preds_y: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      preds_y.append(t(idx))\n    }\n    DenseVector(preds_y.toArray)\n  }\n}\n\nobject LVQ{\n  def main(args: Array[String]): Unit = {\n\n    val X = Array(Array(0.697,0.460),Array(0.774,0.376),Array(0.634,0.264),Array(0.608,0.318),Array(0.556,0.215),\n                  Array(0.403,0.237),Array(0.481,0.149),Array(0.437,0.211),Array(0.666,0.091),Array(0.243,0.267),\n                  Array(0.245,0.057),Array(0.343,0.099),Array(0.639,0.161),Array(0.657,0.198),Array(0.360,0.370),\n                  Array(0.593,0.042),Array(0.719,0.103),Array(0.359,0.188),Array(0.339,0.241),Array(0.282,0.257),\n                  Array(0.748,0.232),Array(0.714,0.346),Array(0.483,0.312),Array(0.478,0.437),Array(0.525,0.369),\n                  Array(0.751,0.489),Array(0.532,0.472),Array(0.473,0.376),Array(0.725,0.445),Array(0.446,0.459))\n\n   val XX = DenseMatrix(X:_*)\n   val y = DenseVector.zeros[Int](XX.rows)\n\n    for(i <- 9 until 21){\n      y(i) = 1\n    }\n\n    val t = Array(0,1,1,0,0)\n    println(y)\n    val lvq = new LVQ(t)\n    lvq.fit(XX,y)\n\n    println(lvq.C)\n    println(lvq.labels)\n    println(lvq.predict(XX))\n\n  }\n}\n```\n\n详细代码请参考：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/LVQ.scala\n\n代码实现参考了python代码：\n\nhttps://github.com/fengyang95/tiny_ml/blob/master/tinyml/cluster/LVQ.py\n\n#### 6 层次聚类\n\n层次聚类(hierarchical clustering)是一种基于树形结构的聚类方法，常用的是**自底向上**的结合策略（**AGNES算法**），它将数据集中的每个样本看作一个初始聚类簇，然后再算法运行的每一步中找到距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。这里的关键是如何计算聚类簇之间的距离。实际上，每个簇是一个样本集合，只需要采用集合的某种距离即可。例如，给定聚类簇$C_i$与$C_j$,可以通过下面的式子来计算距离：\n\n最小距离：\n$$\nd_{\\min}(C_i,C_j) = \\min _{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{15}\n$$\n最大距离：\n$$\nd_{\\max}(C_i,C_j) = \\max_{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{16}\n$$\n平均距离：\n$$\nd_{avg}(C_i,C_j) = \\frac{1}{|C_i||C_j|}dist(x,z)  \\tag{17}\n$$\n显然，最小距离由两个簇的最近的样本决定；最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本决定。\n\n##### 6.1  算法步骤\n\nAGNES 算法步骤如下,在1-9行，算法先对仅含一个样本的初始聚类簇和相应的距离进行初始化，然后在11-23行，AGNES不断合并距离最近的聚类簇，并对合并得到的聚类簇的距离矩阵进行更新；上述过程不断重复，直至达到预设的聚类簇数。\n\n![](./聚类算法总结/hc_算法步骤.png)\n\n##### 6.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector}\nimport com.strings.data.Data\nimport com.strings.utils.MatrixUtils\n\nimport scala.collection.mutable.ArrayBuffer\n\n\ncase class ClusterNode(vector:DenseVector[Double],\n                       id:Int,\n                       left:ClusterNode = null,\n                       right:ClusterNode = null,\n                       distance:Double = -1.0,\n                       count:Int = 1)\n\nclass HierarchicalCluster(k:Int) {\n\n  var labels:DenseVector[Int] = _\n\n  def fit(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val n_features = X.cols\n    val X_arr = (0 until n_samples).map(X.t(::,_))\n    val nodes:ArrayBuffer[ClusterNode] = new ArrayBuffer[ClusterNode]()\n    for((sample,inx) <- X_arr.zipWithIndex){\n      nodes.append(ClusterNode(sample,inx))\n    }\n    labels = DenseVector.ones[Int](n_samples) :* (-1)\n    var distances: Map[(Int, Int), Double] = Map()\n    var curret_cluster_id = -1\n    while (nodes.length > k){\n      var min_dist = Double.MaxValue\n      val nodes_len = nodes.length\n      var closest_part:(Int, Int) = 0 -> 0\n      for(i <- 0 until nodes_len - 1){\n        for(j <- i+1 until nodes_len){\n          val d_key = nodes(i).id -> nodes(j).id\n          if(!distances.contains(d_key)){\n            distances += (d_key -> MatrixUtils.euclidean_distance(nodes(i).vector,nodes(j).vector))\n          }\n          val d = distances(d_key)\n          if(d < min_dist){\n            min_dist = d\n            closest_part = i -> j\n          }\n        }\n      }\n\n      val part1 = closest_part._1\n      val part2 = closest_part._2\n      val node1 = nodes(part1)\n      val node2 = nodes(part2)\n      val new_vec = DenseVector.ones[Double](n_features)\n      for(i <- 0 until n_features){\n        new_vec(i) = (node1.vector(i) * node1.count + node2.vector(i) * node2.count)/\n          (node1.count + node2.count)\n      }\n      val new_count = node1.count + node2.count\n      val new_node = ClusterNode(new_vec,curret_cluster_id,node1,node2, min_dist,new_count)\n\n      curret_cluster_id -= 1\n      nodes.remove(part2)\n      nodes.remove(part1)\n      nodes.append(new_node)\n    }\n    calc_label(nodes)\n\n  }\n\n  def calc_label(nodes:ArrayBuffer[ClusterNode]): Unit ={\n    for((node,inx) <- nodes.zipWithIndex){\n      leaf_traveral(node,inx)\n    }\n  }\n\n  def leaf_traveral(node:ClusterNode,label:Int): Unit ={\n    if(node.left == null && node.right == null){\n      labels(node.id) = label\n    }\n    if(node.left != null){\n      leaf_traveral(node.left,label)\n    }\n    if(node.right != null){\n      leaf_traveral(node.right,label)\n    }\n  }\n}\n\nobject HierarchicalCluster{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4))\n    val dd = DenseMatrix(data:_*)\n    val hc = new HierarchicalCluster(k=3)\n    hc.fit(dd)\n    println(hc.labels)\n  }\n}\n\n```\n\n算法实现参考了：\n\nhttps://zhuanlan.zhihu.com/p/32438294\n\n#### 7 MeanShift 均值漂移聚类算法\n\n​        Mean Shift（均值漂移）是基于密度的非参数聚类算法，其算法思想是假设不同簇类的数据集符合不同的概率密度分布，找到任一样本点密度增大的最快方向，样本密度高的区域对应于该分布的最大值，这些样本点最终会在局部密度最大值收敛，且收敛到相同局部最大值的点被认为是同一簇类的成员。\n\n##### 7.1 算法流程\n\n1. 计算 每个样本的均值漂移矩阵$m_h(x)$，即\n   $$\n   m_h(x) = \\frac{\\sum_{i=1}^{n}{G(\\frac{x_i - x}{h})}w(x_i)x_i}{\\sum_{i=1}^{n}{G(\\frac{x_i - x}{h})}w(x_i)}\n   $$\n\n2. 把$m_h(x)$ 赋给x；\n3. 如果 $||m_h(x) - x|| < \\epsilon $,结束循环，否则执行步骤1；\n\n##### 7.2  代码\n\n```scala\npackage com.strings.model.cluster\nimport util.control.Breaks._\nimport breeze.linalg.{Axis, DenseMatrix, DenseVector, sum, tile}\nimport breeze.numerics.{exp, pow, sqrt}\nimport com.strings.data.Data\nimport scala.collection.mutable.ArrayBuffer\n\nclass MeanShift(val kernel_bandwidth:Double) {\n\n  def euclidean_distance(x1:DenseVector[Double],x2:DenseVector[Double]): Double ={\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def gaussian_kernel(distance:DenseMatrix[Double],kernel_bandwidth:Double):DenseVector[Double] ={\n    val euclidean_distance = sqrt(sum(pow(distance,2),Axis._1))\n    val coefficient = 1.0 / (kernel_bandwidth*math.sqrt(2* math.Pi))\n    coefficient * exp(pow(euclidean_distance / kernel_bandwidth,2) * (-0.5))\n  }\n\n\n  def shiftPoint(point:DenseVector[Double],points:List[DenseVector[Double]],kernel_bandwidth:Double):DenseVector[Double] ={\n    val distance = points.map(t => point - t)\n    val point_weights = gaussian_kernel(DenseMatrix(distance:_*),kernel_bandwidth)\n    val tiled_weight = tile(point_weights,1,point.length)\n    val denominator = sum(point_weights)\n    sum(tiled_weight :* DenseMatrix(points:_*),Axis._0).t / denominator //各个元素对应相乘\n  }\n\n  def group_points(points:List[DenseVector[Double]]):List[Int] = {\n    val cluster_ids:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    var cluster_idx = 0\n    val cluster_centers:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n\n    for((point,i) <- points.zipWithIndex){\n      if(cluster_ids.length == 0){\n        cluster_ids.append(cluster_idx)\n        cluster_centers.append(point)\n        cluster_idx += 1\n      }else{\n        for((center,j) <- cluster_centers.zipWithIndex){\n          val dist = euclidean_distance(point, center)\n          if(dist < 0.1){\n            cluster_ids.append(j)\n          }\n        }\n\n        if(cluster_ids.length < i + 1) {\n          cluster_ids.append(cluster_idx)\n          cluster_centers.append(point)\n          cluster_idx += 1\n        }\n      }\n    }\n    cluster_ids.toList\n  }\n\n  def fit(points:List[DenseVector[Double]]):List[Int]={\n    val shift_points = points\n    var max_min_dist = 1.0\n    var iteration_number = 0\n    val still_shifting:Array[Boolean] = Array.fill(points.length)(true)\n    val MIN_DISTANCE = 1e-5\n    while (max_min_dist > MIN_DISTANCE) {\n      max_min_dist = 0.0\n      iteration_number += 1\n\n      for (i <- 0 until shift_points.length) {\n        breakable {\n          if (!still_shifting(i)) {\n            break\n          }\n        }\n        var p_new = shift_points(i).copy\n        val p_new_start = p_new.copy\n        p_new = shiftPoint(p_new, points, kernel_bandwidth)\n        val dist = euclidean_distance(p_new, p_new_start)\n        if(dist > max_min_dist){\n          max_min_dist = dist\n        }\n        if(dist < MIN_DISTANCE){\n          still_shifting(i) = false\n        }\n        shift_points(i) := p_new\n      }\n    }\n    group_points(shift_points)\n  }\n\n}\n\nobject MeanShift{\n\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).map(DenseVector(_)).toList\n    val ms = new MeanShift(kernel_bandwidth = 0.3)\n    val label = ms.fit(data)\n    println(label)\n  }\n\n}\n```\n\n代码详细地址请参考：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/MeanShift.scala\n\n#### 参考文献\n\n1. 周志华 机器学习 - 聚类部分\n\np.s. 该总结主要介绍每个聚类算法的算法步骤，以及scala的简单实现，并没有多注重效率，仅供自己学习使用。"},{"title":"adaboost算法","url":"/2020/05/08/adaboost算法/","content":"##### 1. 算法简介\n\nBoosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。其中最为成功应用的是，Yoav Freund和Robert Schapire在1995年提出的AdaBoost算法。\n\n​      AdaBoost是英文\"Adaptive Boosting\"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。\n\n##### 2. 算法过程\n\n给定训练数据集： $(x_1,y_1),...,(x_N,y_N)$，其中 $y_i \\in \\{1,-1\\}$，用于表示训练样本的类别标签$i=1,...,N$。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。\n\n##### 3. 算法详细步骤\n\n1. 初始化数据的权值分布\n   $$\n   D_1 = (w_{11},...,w_{1i},...,w_{1N}),w_{1i} = \\frac{1}{N},i=1,2,...,N\n   $$\n\n2. 对$m=1,2,...,M$\n\n   (a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n   $$\n   G_m(x):f \\rightarrow  \\{-1,+1\\}\n   $$\n   (b) 计算$G_m(x)$在训练数据集上的分类误差率\n   $$\n   e_m = P(G_m(x_i) \\neq y_i) = \\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)\n   $$\n   (c) 计算$G_m(x)$的系数\n   $$\n   \\alpha_m = \\frac{1}{2}\\log\\frac{1-e_m}{e_m}\n   $$\n   这里的对数是自然对数\n\n   (d) 更新训练数据集的权值分布\n   $$\n   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\\\\n   w_{m+1,i} = \\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),i=1,2,..,N\n   $$\n   这里，$Z_m$是规范化因子\n   $$\n   Z_m = \\sum_{i=1}^Nw_{mi}\\exp(-\\alpha_my_iG_m(x_i))\n   $$\n   它使$D_{m+1}$成为一个概率分布\n\n3. 构建基本分类器的线性组合\n   $$\n   f(x) = \\sum_{m=1}^{M}\\alpha_mG_m(x)\n   $$\n   得到最终的分类器\n   $$\n   G(x) = sign(f(x)) = sign(\\sum_{m=1}^M\\alpha_mG_m(x))\n   $$\n   对AdaBoost算法做下面的说明：\n\n    （1）首先，是初始化训练数据的权值分布$D_1$。假设有$N$个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值$w_1 = \\frac{1}{N}$。\n\n    （2）然后，训练弱分类器$h_i$。具体训练过程中是：如果某个训练样本点，被弱分类器$h_i$准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。\n\n    （3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n     换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。\n   \n   ##### 4. 代码实现\n   \n   代码实现地址：\n   \n   https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/AdaBoost.scala\n   \n   ##### 5. 参考文献\n   \n   李航 《统计学习方法》2012.3"},{"title":"降维方法-总结","url":"/2020/05/06/降维方法-总结/","content":"#### 1. 降维概述\n\n样本的特征数称为维数（dimensionality），当维数非常大时，也就是现在所说的“维数灾难”，具体表现在：在高维情形下，数据样本将变得十分稀疏，因为此时要满足训练样本为“密采样”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉...训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力；同时当维数很高时，计算距离也变得十分复杂，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数“低维计算，高维表现”的原因。\n\n缓解维数灾难的一个重要途径就是降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个低维嵌入，例如：数据属性中存在噪声属性、相似属性或冗余属性等，对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果。\n\n#### 2.降维方法分类\n\n![这里写图片描述](http://img.blog.csdn.net/20150522194801297)\n\n#### 3 线性方法\n\n##### 3.1 PCA主成分分析\n\n主成分分析（PCA）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中。简单来理解这一过程便是：PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。\n\n假设使用$d^{’}$个新基向量来表示原来样本，实质上是将样本投影到一个由$d^{’}$个基向量确定的一个超平面上（即舍弃了一些维度），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：\n\n> **最近重构性**：样本点到超平面的距离足够近，即尽可能在超平面附近；\n> **最大可分性**：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。\n\nPCA的算法描述如下：\n\n![](.\\降维方法-总结\\pca_算法步骤.png)\n\n##### 3.2 LDA 线性判别分析\n\n参考之前的博文\n\n#### 4 非线性方法\n\n##### 4.1 MDS \n\n不管是使用核函数升维还是对数据降维，我们都希望**原始空间样本点之间的距离在新空间中基本保持不变**，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。**“多维缩放”（MDS）**正是基于这样的思想，**MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持**。\n\n假定$m$个样本在原始空间中任意两两样本之间的距离矩阵为$D \\in R ^{m \\times m}$，其中第$i$行$j$列的元素$dist_{ij}$为样本$x_i$到$x_j$的距离。我们的目标便是获得样本在低维空间中的表示$Z \\in R^{d^{’} \\times m} $, $d'< d$，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即$||zi-zj||=dist_{ij}$。因此接下来我们要做的就是根据已有的距离矩阵$D$来求解出降维后的坐标矩阵$Z$。\n\n令降维后的样本坐标矩阵Z被中心化，**中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量**。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。\n\n![4.png](https://i.loli.net/2018/10/18/5bc851a4a4ee2.png)\n\n令$B = Z^TZ \\in R^{m \\times m}$,其中$B$为降维后的样本的內积矩阵，$b_{ij} = z_i^Tz_j$,有：\n$$\ndist_{ij}^{2} = ||z_i||^2 + ||z_j||^2 -2z_i^Tz_j\n= b_{ii} + b_{jj} - 2b_{ij}\n$$\n为了方便讨论，令降维后的样本$Z$被中心化，即$\\sum_{i=1}^{m}z_i = 0$,显然$B$的行与列之和均为0，即$\\sum_{i=1}^{m}b_{ij} =\\sum_{j=1}^{m}b_{ij} =  0$,容易知道：\n$$\n\\sum_{i=1}^{m}dist_{ij}^2 = tr(B) +mb_{jj}\n$$\n\n$$\n\\sum_{j=1}^{m}dist_{ij}^2 = tr(B) +mb_{ii}\n$$\n\n$$\n\\sum_{i=1}^m\\sum_{i=1}^{m}dist_{ij}^2 = 2m * tr(B)\n$$\n\n其中$tr(.)$表示矩阵的迹(trace),$tr(B) = \\sum_{i=1}^{m}||z_i||^2$,令\n$$\ndist_{i.}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{.j}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{..}^2=\\frac{1}{m^2}\\sum_{i=1}^{m}\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n由上面三个等式可知：\n$$\nb_{ij} = -0.5(dist_{ij}^2 - dist_{i.}^2 -dist_{.j}^2 + dist_{..}^2)\n$$\n通过降维前后保持不变的距离矩阵$D$求取內积矩阵$B$.\n\n对$B$做特征值分解(**eigenvalue decompostion**), $B = VUV^T$,其中$U=diag(\\lambda _1,\\lambda _2,...,\\lambda _d)$为特征值构成的对角矩阵，$\\lambda _1 \\geq \\lambda _2 \\geq ... \\geq \\lambda _d$,$V$为特征向量矩阵。假定其中有$d^*$个非零特征值，他们构成的对角矩阵 $U_* = diag(\\lambda _1,\\lambda _2,...,\\lambda _{d^{*}})$ ,其中$V_*$表示对应的特征向量矩阵，则$Z$可以表示为：\n$$\nZ = U_*^{\\frac{1}{2}}V_*^T \\in R^{d^*\\times m}\n$$\nMDS 的算法描述如下：\n\n![](.\\降维方法-总结\\mds_算法步骤.png)\n\n##### 4.2 Isomap\n\n等度量映射（Isomap)属于流行学习，流形学习（manifold learning）是一种借助拓扑流形概念的降维方法**，**流形是指在局部与欧式空间同胚的空间，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种**“邻域保持”**的思想 ，等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系。\n\n等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。**因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离**，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的**Dijkstra算法**或**Floyd算法**计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。\n\n![](./降维方法-总结/测地距离.png)\n\n从**MDS**算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵$B$，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量$w$，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。\n\nISOMAP的算法步骤如下：\n\n![](./降维方法-总结/isomap_算法步骤.png)\n\n对于近邻图的构建，常用的有两种方法：**一种是指定近邻点个数**，像**KNN**一样选取k个最近的邻居；**另一种是指定邻域半径**，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：\n\n> 若**邻域范围指定过大，则会造成“短路问题”**，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。\n> 若**邻域范围指定过小，则会造成“断路问题”**，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。\n\n##### 4.3 LLE \n\n不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本$x_i$的坐标可以通过它的邻域样本线性表出：\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE希望上式的关系在低维空间中得以保持。\n\n![](./降维方法-总结/LLE空间保持.png)\n\nLLE先为每个样本$x_i$找到其近邻下标集合$Q_i$,然后计算基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$.\n$$\n\\min_{w_1,w_2,...,w_m}\\sum_{i=1}^m||x_i - \\sum_{j \\in Q_i}w_{ij}{x_j}||_2^2\n$$\n\n$$\ns.t. \\sum_{j\\in Q_i}w_{ij} = 1\n$$\n\n其中$x_i$和$x_j$均为已知，令$C_{jk} = (x_i- x_j)^T(x_i-x_j)$,$w_{ij}$有闭式解\n$$\nw_{ij} = \\frac{\\sum_{k \\in Q_i}C_{jk}^{-1}}{\\sum_{l,s \\in Q_i}C_{ls}^{-1}}\n$$\n**LLE**在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解：\n$$\n\\min_{z_1,z_2,...,z_m}\\sum_{i=1}^m||z_i - \\sum_{j \\in Q_i}w_{ij}{z_j}||_2^2\n$$\n上式需要确定的是$x_i$对应的低维空间坐标$z_i$.\n\n令$Z =(z_1,z_2,...,z_m) \\in R^{d'\\times m}$,$(W)_{ij} = w_{ij}$,\n$$\nM = (I-W)^T(I-W)\n$$\n则优化目标可以重写为下式：\n$$\n\\begin{equation}\n\\min_z tr(ZMZ^T) \\\\\ns.t. ZZ^T = I \n\\end{equation}\n$$\n可以通过特征值分解求解，$M$最小的$d'$的特征值对应的特征向量组成的矩阵即为$Z^T$\n\nLLE算法步骤如下：\n\n![](./降维方法-总结/lle_算法步骤.png)\n\n##### 4.4 核PCA kernel PCA\n\n说起机器学习你中有我/我中有你/水乳相融...在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，**即先将样本映射到高维空间，再在高维空间中使用线性降维的方法**。下面主要介绍**核化主成分分析（KPCA）**的思想。\n\n若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：**即空间中的任一向量，都可以由该空间中的所有样本线性表示**。\n\n假定我们将在高维特征空间中的数据投影到由$W$确定的超平面上，即PCA欲求解\n$$\n(\\sum_{i=1}^mz_iz_i^T)W = \\lambda W\n$$\n其中$z_i$是样本点$x_i$在高维特征空间的像，可知：\n$$\nW = \\frac{1}{\\lambda}(\\sum_{i=1}^mz_iz^T_i)W = \\sum_{i=1}^m z_i\\frac{z_i^TW}{\\lambda} \\\\\n=\\sum_{i=1}^m z_i\\alpha _i\n$$\n其中$\\alpha _i = \\frac{1}{\\lambda}z_i^TW$.假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i = \\phi (x_i),i= 1,2,...,m$,若$\\phi$能被显式的表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。\n\n即（17）可变为\n$$\n(\\sum_{i=1}^m \\phi (x_i) \\phi (x_i)^T)W = \\lambda W\n$$\n（18）式可变为：\n$$\nW= \\sum_{i=1}^m \\phi(x_i)\\alpha_i\n$$\n一般情形下，我们不清楚$\\phi$的具体形式，于是引入核函数:\n$$\n\\kappa(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\n$$\n将（21)(20)带入到(19)式中可得：\n$$\nKA= \\lambda A\n$$\n其中$K$为$\\kappa$ 对应的核矩阵，$(K)_{ij} = \\kappa(x_i,x_j),A = (\\alpha_1;\\alpha_2;...;\\alpha_m)$,显然上式是个特征值分解问题，取$K$最大的$d'$个特征值对应的特征向量即可。\n\n##### 4.5 DiffusionMap\n\n扩散映射是一种降维方法\n\n1. 其通过 整合数据的局部几何关系 揭示 数据集在不同尺度的几何结构。\n2. 与PCA (principal component analysis)、MDS (Multidimensional Scaling) 这些降维方法相比，扩散映射 非线性，聚焦于发现数据集潜在的流形结构。\n3. 优点：对噪声鲁棒，计算代价较低\n\n算法步骤如下:\n\n![](./降维方法-总结/diffmap_算法步骤.png)"},{"title":"xgboost","url":"/2020/03/18/xgboost/","content":"#### 1. XGBoost 简介\n\n$XGBoost$的全称是$eXtremeGradientBoosting$，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。$XGBoost$是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用$XGBoost$进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，$XGBoost$的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从$XGBoost$的数学原理和工程实现上进行介绍，然后介绍$XGBoost$的优缺点，并在最后给出面试中经常遇到的关于$XGBoost$的问题。\n\n#### 2.  $XGBoost \\ $算法\n\n​    $XGBoost$是由 $k$个基模型组成的一个加法模型，假设我们第 $t$ 次迭代要训练的树模型是 $f_t(x)$ ，则有：\n\n​\t\n$$\n\\hat{y_i}^{(t)} = \\sum_{k=1}^{t}f_k(x_i) = \\hat{y_i}^{(t-1)} + f_t(x_i) \\label{1}\n$$\n其中，$\\hat{y_i}^{(t)}$是第$t$次迭代后样本的$i$的预测结果，$\\hat{y_i}^{(t-1)}$是前$t-1$棵树的预测结果，$f_t(x_i)$是第$t$棵树的模型。\n\n​\t$XGBoost  \\ $算法是$\\ GBDT\\ $算法的改进版本，其目标函数为：\n$$\n\\begin{aligned}\nObj^{(k)}&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_k)+\\sum\\limits_{i=1}^T\\Omega(f_i)\\\\\n&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\\Omega(f_k)+C\n\\end{aligned}\n$$\np.s. 上式中第一项是衡量模型的偏差，模型越不准确，第一项就会越大。第二项是衡量模型的偏差，模型越复杂，模型的学习就会越具体，到不同数据集上的表现就会差异巨大，方差就会越大。\n\n所以求解Obj的最小值，其实在求解方差和偏差的平衡点，以求解模型的泛化误差最小，运行速度最快。\n\n同理为了求损失函数$\\ l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处的二阶展开，不妨先对$\\ l(y^{(i)},x)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处进行二阶展开可得：\n$$\nl(y^{(i)},x)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+\\nabla_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})+\\dfrac{1}{2}\\nabla^2_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})^2  \\label{2}\n$$\n令$\\ x=\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}) \\ $，且记$\\ \\nabla_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ g_i\\ $、$\\ \\nabla^2_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ h_i\\ $则有：\n$$\nl\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)}) \\label{3}\n$$\n其中，$g_i$为损失函数的一阶导数，$h_i$为损失函数的二阶导数，注意这里是对$\\hat{y}^{(t-1)}_{i}$求导。它们被称为每个样本的梯度统计量。\n\n**以平方损失为例：**\n$$\nl(y^{(i)},\\hat{y}^{(t-1)}_i) = (y^{(i)},\\hat{y}^{(t-1)}_i)^2\n$$\n则：\n$$\n\\begin{aligned}\ng_i &= \\frac{\\partial{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{\\hat{y}^{(t-1)}_i}} = -2(y_i - \\hat{y}_i^{(t-1)}) \\\\\nh_i &= \\frac{\\partial ^2{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{(\\hat{y}^{(t-1)}_i})^2} = 2\n\\end{aligned}\n$$\n又因为在第$\\ k\\ $步$\\ \\hat{y}^{(i)}_{k-1} \\ $其实是已知的，所以$\\ l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\ $是一个常数函数，故对优化目标函数不会产生影响，将上述结论带入目标函数$\\ Obj^{(k)}\\ $可得：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\n$$\n\n##### 2.1  优化目标函数\n\n​\t以$\\ XGBoost\\ $算法的目标函数为例，对于任意决策树$\\ f_k \\ $，**假设其叶子结点个数$\\ T\\ $，该决策树是由所有结点对应的值组成的向量$\\ w\\in\\mathbb{R}^T\\ $，以及能够把特征向量映射到叶子结点的函数$\\ q(*):\\mathbb{R}^d\\rightarrow \\{1,2,\\cdots,T \\} \\ $构造而成的，且每个样本数据都存在唯一的叶子结点上。因此决策树$\\ f_k\\ $可以定义为$\\ f_k(x)=w_{q(x)} \\ $。**决策树的复杂度可以由正则项$\\ \\Omega(f_k)=\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2 \\ $来定义，该正则项表明决策树模型的复杂度可以由叶子结点的数量和叶子结点对应值向量$\\ w \\ $的$\\ L2\\ $范数决定。定义集合$\\ I_j=\\{i|q(x^{(i)})=j \\}\\ $为划分到叶子结点$\\ j \\ $的所有训练样本的集合，即之前训练样本的集合，现在都改写成叶子结点的集合，因此$\\ XGBoost\\ $算法的目标函数可以改写为：\n$$\n\\begin{aligned}\nObj^{(k)}&\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\\\\\n&=\\sum\\limits_{i=1}^m\\bigg[g_iw_{q(x^{(i)})}+\\dfrac{1}{2}h_jw^2_{q(x^{(i)})} \\bigg]+\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2\\\\\n&=\\sum\\limits_{j=1}^T\\bigg[(\\sum\\limits_{i\\in I_j}g_i)w_j+\\dfrac{1}{2}(\\sum\\limits_{i\\in I_j}h_i+\\lambda)w_j^2 \\bigg]+\\gamma T\n\\end{aligned}\n$$\n令$\\ G_j=\\sum\\limits_{i\\in I_j}g_i ,\\ H_j=\\sum\\limits_{i\\in I_j}h_i \\ $则有：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{j=1}^T\\bigg[G_jw_j+\\dfrac{1}{2}(H_j+\\lambda)w_j^2 \\bigg]\n$$\n分析可知当更新到第$\\ k\\ $步时，此时**决策树结构固定的情况下**，每个叶子结点有哪些样本是已知的，那么$\\ q(*)\\ $和$\\ I_j\\ $也是已知的；又因为$\\ g_i\\ $和$\\ h_i\\ $是第$\\ k-1\\ $步的导数，那么也是已知的，因此$\\ G_j\\ $和$\\ H_j\\ $都是已知的。令目标函数$\\ Obj^{(k)}\\ $的一阶导数为$\\ 0\\ $，即可求得叶子结点$\\ j\\ $对应的值为：\n$$\nw^*_j=-\\dfrac{G_j}{H_j+\\lambda}\n$$\n因此针对于结构固定的决策树，最优的目标函数$\\ Obj\\ $为：\n$$\nObj=-\\dfrac{1}{2}\\sum\\limits_{j=1}^T\\dfrac{G_j^2}{H_j+\\lambda}+\\gamma T\n$$\n上面的推导是建立在决策树结构固定的情况下，然而决策树结构数量是无穷的，所以实际上并不能穷举所有可能的决策树结构，什么样的决策树结构是最优的呢？通常使用贪心策略来生成决策树的每个结点，$\\ XGBoost \\ $算法的在决策树的生成阶段就对过拟合的问题进行了处理，因此无需独立的剪枝阶段，具体步骤可以归纳为：\n\n1. 从深度为$\\ 0\\ $的树开始对每个叶子结点穷举所有的可用特征；\n2. 针对每一个特征，把属于该结点的训练样本的该特征升序排列，通过线性扫描的方式来决定该特征的**最佳分裂点**，并采用最佳分裂点时的**收益**；\n3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该结点生成出左右两个新的叶子结点，并为每个新结点关联新的样本集；\n4. 退回到第一步，继续递归操作直到满足特定条件。\n\n因为对某个结点采取的是二分策略，分别对应左子结点和右子结点，除了当前待处理的结点，其他结点对应的$\\ Obj \\ $值都不变，所以对于收益的计算只需要考虑当前结点的$\\ Obj \\ $值即可，分裂前针对该结点的最优目标函数为：\n$$\nObj^{(before)}=-\\dfrac{1}{2}\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda}+\\gamma\n$$\n分裂后的最优目标函数为：\n$$\nObj^{(later)}=-\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda} \\bigg]+2\\gamma\n$$\n那么对于该目标函数来说，分裂后的收益为：\n$$\n\\begin{aligned}\nGain&=Obj^{(before)}-Obj^{(later)}\\\\\n&=\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda}-\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda} \\bigg]-\\gamma\n\\end{aligned}\n$$\n故可以用上述公式来决定最有分裂特征和最优特征分裂点。\n\n##### 2.2  总结\n\n​\t$XGBoost  \\ $算法的过程可以归纳为：\n\n1. 前向分布算法的每一步都生成一棵决策树；\n2. 拟合该决策树之前，先计算损失函数在每个样本数据上的一阶导$\\ g_i \\ $和二阶导$\\ h_i \\ $；\n3. 通过贪心策略生成一棵决策树，计算每个叶子结点的$\\ G_j\\ $和$\\ H_j\\ $并计算预测值$\\ w\\ $；\n4. 把新生成的决策树$\\ f_k(x)\\ $加入$\\ \\hat{y}^{(i)}_k=\\hat{y}^{(i)}_{k-1}+\\epsilon f_k(x^{(i)}) \\ $，其中$\\ \\epsilon\\ $是学习率主要控制模型的过拟合。\n\n#### 3 $XGBoost\\ $的优缺点\n\n​\t相比于普通的$\\ GBDT \\ $算法$\\ XGBoost\\ $算法的主要优点在于：\n\n- 不仅支持决策树作为基分类器，还支持其它线性分类器；\n- 使用了损失函数的二阶泰勒展开，因此与损失函数更接近，收敛速度更快；\n-  在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的  范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统GBDT的一个特性。；\n- $Shrinkage\\ $也就是之前说的$\\ \\epsilon\\ $，主要用于削弱每棵决策树的影响，让后面有更大的学习空间，实际应用中一般把$\\ \\epsilon\\ $设置的小点，迭代次数设置的大点；\n- 列抽样，$\\ XGBoost\\ $从随机森林算法中借鉴而来，支持列抽样可以降低过拟合，并且减少计算；\n- 支持对缺失值的处理，对于特征值缺失的样本，$\\ XGBoost\\ $可以学习这些缺失值的分裂方向；\n- 支持并行，boosting不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是tree粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第$t$次迭代的代价函数里包含了前面$t-1$次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。；\n- 近似算法，决策树结点在分裂时需要穷举每个可能的分裂点，当数据没法全部加载到内存中时，这种方法会比较慢，$\\ XGBoost\\ $提出了一种近似的方法去高效的生成候选分割点。\n\n缺点\n\n- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；\n- 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。\n\n#### 4. 关于XGBosst的若干问题\n\n##### 4.1 XGBoost与GBDT的联系和区别有哪些？\n\n1. GBDT是机器学习算法，XGBoost是该算法的工程实现。\n2. **正则项：** 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。\n3. **导数信息：** GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。\n4. **基分类器：** 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。\n5. **子采样：** 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。\n6. **缺失值处理：** 传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。\n7. **并行化：** 传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。\n\n##### 4.2 为什么XGBoost泰勒二阶展开后效果就比较好呢？\n\n（1）**从为什么会想到引入泰勒二阶的角度来说（可扩展性）：** XGBoost官网上有说，当目标函数是`MSE`时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如`logistic loss`的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把`MSE`推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与`MSE`统一？是因为`MSE`是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与`MSE`统一了，那就只用推导`MSE`就好了。\n\n（2）**从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：** 二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。\n\n##### 4.3 XGBoost对缺失值是怎么处理的？\n\n在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。\n\n#### 5 代码实现\n\nscala代码实现地址如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/XGBoost.scala\n\nScala代码实现参考了下面的python代码：\n\nhttps://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/xgboost.py\n\n#### 6 参考文献 \n\n- 陈天奇论文原文 XGBoost: A Scalable Tree Boosting System\n- 深入理解 XGBoost：Kaggle 最主流的集成算法\n\n"},{"title":"梯度提升树","url":"/2020/03/18/梯度提升树/","content":"提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。\n\n##### 1.1 提升树模型\n\n提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。\n\n提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示成决策树的加法模型。\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n​\t其中，$T(x;\\Theta_m)$表示决策树；$\\Theta_m$表示决策树的参数；$M$为树的个数.\n\n##### 1.2 提升树算法\n\n提升树算法采取前向分步算法。首先确定初始提升树$f_0(x) = 0$,第$m$步的模型是\n$$\nf_m(x) = f_{m-1}(x)+T(x;\\Theta_m)\n$$\n其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$,\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n由于树的线性组合可以很好的拟合训练数据，即数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。\n\n下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的一般决策问题。\n\n对于二分类分类问题，提升树算法只需要将$Adaboost$算中基本分类器限制为二类分类树即可，可以说这时的提升树算法是$Adaboost$算法的特殊情况，这里不再详细叙述。下面重点叙述回归问题的提升树。\n\n已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间。如果将输入空间$\\chi$划分成$J$个互不相交的区域$R_1,R_2,...,R_J$，并且在每个区域上确定输出的常量$c_j$,那么树可以表示为\n$$\nT(x;\\Theta) = \\sum_{j=1}^{J}c_jI(x \\in R_j)\n$$\n其中参数$\\Theta={(R_1,c_1),(R_2,c_2),...,(R_J,c_J)}$表示树的区域划分和各区域上常数，$J$是回归树的复杂度即叶子节点的个数。\n\n回归问题提升树使用以下前向分布算法：\n$$\n\\begin{aligned}\nf_0(x) &= 0\\\\\nf_m(x) &= f_{m-1}(x) + T(x;\\Theta_m),m = 1,2,...,M \\\\\nf_M(x) &= \\sum_{m=1}^{M}T(x;\\Theta_m) \\\\\n\\end{aligned}\n$$\n在前向分布算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n得到$\\hat{\\Theta}_m$,即第$m$棵树的参数。\n\n当采用平方损失函数时，\n$$\nL(y,f(x)) = (y - f(x))^{2}\n$$\n其损失变为\n$$\n\\begin{aligned}\nL(y,f_{m-1}(x) - T(x;\\Theta_m)) &=(y - f_{m-1}(x) - T(x;\\Theta_m))^{m}\\\\\n &= (r - T(x;\\Theta_m))^2\n\\end{aligned}\n$$\n这里,\n$$\nr = y - f_{m-1}(x)\n$$\n是当前模型拟合数据的**残差(residual)**,所以，对回归问题的提升树来说，只需要简单地拟合当前模型的残差。这样，算法是相当简单。 \n\n**算法1 回归问题的提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出：提升树$f_M(x)$\n\n(1) 初始化$f_0(x) = 0$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 按照式$r = y - f_{m-1}(x)$计算残差\n$$\nr_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N\n$$\n​     (b) 拟合残差$r_{mi}$学习一棵回归树，得到$T(x;\\Theta_m)$\n\n​\t (c) 更新$f_m(x) = f_{m-1}(x) + T(x;\\Theta_m)$\n\n(3) 得到回归问题提升树\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n\n##### 1.3 梯度提升\n\n梯度提升(Gradient Boosting）是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失时，每一步优化是很简单的。对于一般的损失函数而言，往往每一步优化并不是那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，**其关键是利用损失函数的负梯度在当前模型的值**：\n$$\n-[\\frac{\\partial{L(y,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n**作为回归问题提升树算法中残差的近似值**，拟合一个回归树。\n\n**算法2 梯度提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出: 回归树$\\hat{f}(x)$\n\n(1) 初始化$f_0(x) = \\arg \\min_{c}\\sum_{i=1}^{N}L(y_i,c)$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 对$i = 1,2, ...,N$计算残差\n$$\nr_{mi} = -[\\frac{\\partial{L(y_i,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n​     (b) 拟合$r_{mi}$学习一棵回归树，得到第$m$棵树的叶节点区域$R_{mj},j  = 1,2,...,J$\n\n​\t (c) 对于$j  = 1,2,...,J$，计算\n$$\nc_{mj} = \\arg \\min_{c}\\sum_{x_i \\in R_{mj}}L(y_i,f_{m-1}(x_i) +c)\n$$\n​    (d）更新$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^Jc_{mi}I(x \\in R_{mj})$\n\n(3) 得到回归问题提升树\n$$\n\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x\\in R_{mj})\n$$\n算法的第一步初始化，估计使损失函数极小化的常数值，它只有一个根节点的树，第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数来说，它的负梯度其实就是常说的残差，对于一般的损失函数，它就是残差的近似值。第2（b)步，估计回归树叶子节点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶子终点区域的值，使损失函数极小化。第2（d）步更新回归树。第3步，得到输出的最终模型。\n\n##### 1.4 提升树主要损失函数\n\n下面我们对提升树所用的损失函数做一个总结：\n\n1) 对于分类算法来说：其损失函数一般有对数损失函数和指数损失函数：\n\na）**指数损失函数**\n$$\nL(y_i,f(x_i)) = exp(-y_if(x_i))\n$$\n其负梯度误差为：\n$$\n-y_i.exp(-f(x_i))\n$$\nb）**对数损失函数**\n$$\nL(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))\n$$\n其负梯度为：\n$$\n\\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}\n$$\n化简为：\n$$\n\\frac{y_i}{(1+exp(y_if(x_i)))}\n$$\n2) 回归算法：常见的有以下四种\n\n1. **均方差损失函数**\n   $$\n   L(y_i,f(x_i)) = (y_i - f(x_i))^2\n   $$\n   其负梯度为：\n   $$\n   y_i - f(x_i)\n   $$\n   p.s. 损失函数为$L(y,f(x))=(y-f(x))^2$,我们需要最小化$J= \\sum_iL(y_i,f(x_i))$通过调整$f(x_1),f(x_2),...,f(x_n)$.我们把$f(x_i)$当成参数并求导\n   $$\n   \\frac{\\partial}{\\partial f(x_i)} = \\frac{\\partial \\sum_iL(y_i,f(x_i))}{\\partial f(x_i)} = \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} = f(x_i) - y_i\n   $$\n   所以，**我们在均方差损失函数下，可以把残差理解成负梯度。**\n   $$\n   y_i-f(x_i) = - \\frac{\\partial}{\\partial f(x_i)}\n   $$\n   \n2. 绝对损失函数：\n   $$\n   L(y_i,f(x_i) = |y_i - f(x_i)|\n   $$\n   其对应的负梯度为：\n   $$\n   sign (y_i - f(x_i))\n   $$\n\n3. Huber损失函数：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：\n   $$\n   \\begin{aligned}\n   L(y_i,f(x_i)) = \n   \\begin{cases} \n   \\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta(|y_i -f(x_i)| - \\frac{\\delta}{2}), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n   其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   y_i-f(x_i), |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n\n4. 分位数损失。它对应的是分位数回归的损失函数，表达式为\n   $$\n   L(x_i,f(x_i)) = \\sum_{y_i \\geq f(x_i)}\\theta|y_i - f(x_i)| + \\sum_{y_i < f(x_i)}(1-\\theta)|y_i - f(x_i)| \n   $$\n   其中，$\\theta$为分位数，需要在回归钱设置，其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   \\theta,  y_i \\geq f(x_i)\\\\\n   \\theta - 1, y_i < f(x_i)\n   \\end{cases}\n   \\end{aligned}\n   $$\n   ![](.\\梯度提升树\\损失函数.png)\n\n#####    1.5 总结及优缺点\n\n本文介绍了boosting族的提升树算法和梯度提升树（GBDT)算法，提升树算法的每轮弱学习器是拟合上一轮的残差生成的，GBDT算法的每轮弱学习器是拟合上一轮损失函数的负梯度生成的。提升树算法和GBDT算法都是用CART回归树作为弱学习器，只要确定模型的损失函数，提升树和GBDT就可以通过前向分布算法进行构建。\n\n\n\n梯度提升树主要的优点有：\n\n1） 可以灵活处理各种类型的数据，包括连续值和离散值。\n\n2）在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。\n\n3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。\n\n梯度提升树的主要缺点有：\n\n1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。\n\n##### 1.6 代码实现\n\nscala代码地址:https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/GBDT.scala\n\n##### 1.7 参考文献\n\n李航 《统计学习》\n\n"},{"title":"逻辑回归分类和softmax分类","url":"/2020/02/12/逻辑回归分类和softmax分类/","content":"### 逻辑回归分类和softmax分类\n\n### 1.逻辑回归\n\n#### 1.1 算法原理\n\n一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。\n\n#### 1.2 假设函数\n\n​\t回顾线性回归中的假设函数$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n=\\theta^Tx$这表示的是一个超平面，逻辑回归中的超平面与线性回归中的超平面并无什么本质上的差异，但是线性回归是回归问题逻辑回归是分类问题两者本质上不同，线性回归中样本集中在超平面附近且没有类别差异而逻辑回归中样本点被所超平面分割且有明确的类别。因此逻辑回归的假设函数需要判断样本点在超平面上还是超平面下，所以给出一种映射关系：\n$$\ng(z) = \\dfrac{1}{1+e^{-z}}\n$$\n函数$\\ g(z)\\ $称为$sigmoid$函数，其函数图像如下图所示：\n\n![](.\\逻辑回归分类和softmax分类\\sigmoid.jpg)\n\n使用$sigmoid$函数对超平面进行映射是因为它有一些很好的性质：\n\n- $sigmoid$函数把所有样本点都映射到(0,1)区间内\n\n- $sigmoid$函数连续可导，求导后的形式很$nice$\n  $$\n  \\begin{aligned}\n  g'(z) &=\\dfrac{\\mathrm{d}}{\\mathrm{d}z} \\dfrac{1}{1+e^{-z}}\\\\\n  &=\\dfrac{1}{(1+e^{-z})^2}(e^{-z})\\\\\n  &=\\dfrac{1}{1+e^{-z}} \\cdot \\left(1-  \\dfrac{1}{1+e^{-z}} \\right)\\\\\n  &=g(z)\\cdot(1-g(z))\n  \\end{aligned}\n  $$\n\n\n\n综上可以给出逻辑回归算法的假设函数$\\ h_\\theta(x)\\ $\n$$\nh_\\theta(x)=g(\\theta^Tx)=\\dfrac{1}{1+e^{-\\theta^Tx}}\n$$\n\n#### 1.3 构造逻辑回归算法的损失函数\n\n​\t不妨假设含有$m$个样本数据($x^{(1)}$,$y^{(1)}$)、($x^{(2)}$,$y^{(2)}$)、$\\cdots$、($x^{(m)}$,$y^{(m)}$)，$y^{(i)} \\in \\{0,1\\}$。由于$h_\\theta(x) \\in (0,1)$，且对于某个样本数据来说它只能属于两种类别中的某一类，所以有如下等式成立\n$$\n\\begin{aligned}\nP(y=1|x;\\theta)&=h_\\theta(x)\\\\\nP(y=0|x;\\theta)&=1-h_\\theta(x)\n\\end{aligned}\n$$\n\n训练梯度下降算法的完整迭代更新格式为\n$$\n\\theta_j:=\\theta_j-\\alpha \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \\ \\ ( j\\ \\  for \\ \\ 0 \\ \\sim \\ n)\n$$\n**可以发现逻辑回归模型的迭代更新格式和线性回归模型的迭代更新格式完全一样，但是它们的假设函数$\\ h_\\theta(x)\\ $的函数表达式是不一样的**。\n\n#### 1.5 梯度下降过程的向量化\n\n​\t向量化是使用矩阵计算来代替$for$循环 ，以简化计算过程提高效率 ，上述迭代更新的过程中有一个连加符号，如果使用$for$循环则需要执行$m$次。在此之前需要先定义一些矩阵，不妨令：\n$$\n\\begin{aligned}\nY&=\n\\begin{bmatrix}\n(y^{(1)}) \\\\ (y^{(2)}) \\\\ \\vdots \\\\ (y^{(m)})\n\\end{bmatrix}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n\\theta=\n\\begin{bmatrix}\n\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n\n\\end{bmatrix}\\\\\n~\\\\\nX&=\n\\begin{bmatrix}\n(x^{(1)})^T \\\\ (x^{(2)})^T \\\\ \\vdots \\\\ (x^{(m)})^T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n1 & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)}\n\\end{bmatrix}\\\\\n\\end{aligned}\n$$\n则\n$$\n\\begin{aligned} \\phi&=g(X\\theta)= \\begin{bmatrix} h_\\theta(x^{(1)}) \\\\ h_\\theta(x^{(2)}) \\\\ \\vdots \\\\ h_\\theta(x^{(m)}) \\end{bmatrix} =g\\left(\\begin{bmatrix} \\theta_0+\\theta_1x_1^{(1)}+\\cdots + \\theta_nx_n^{(1)} \\\\ \\theta_0+\\theta_1x_1^{(2)}+\\cdots + \\theta_nx_n^{(2)}\\\\ \\vdots \\\\ \\theta_0+\\theta_1x_1^{(m)}+\\cdots + \\theta_nx_n^{(m)} \\end{bmatrix}\\right)\\\\ ~\\\\ E&=h_\\theta(x)-y=\\phi-Y= \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_m \\end{bmatrix}\\\\ \\end{aligned}\\\\\n$$\n所以\n$$\n\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} = X\\Epsilon\n$$\n最后\n$$\n\\theta_j:=\\theta_j-\\alpha \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \\ \\ ( j\\ \\  for \\ \\ 0 \\ \\sim \\ n)\n$$\n向量化后可改为\n$$\n\\theta:=\\theta-\\alpha X\\Epsilon \\ \\ \\ \\ \\ \\\\\n$$\n\n#### 1.6 总结\n\n​\t逻辑回归算法是机器学习算法中比较好理解的分类模型，训练速度也很快。因为只需要存储各个维度的特征值的原因，对资源尤其是内存的占用会比较小 ，在引入了$softmax$以后可以处理多分类的问题。 任何机器学习模型都是有自己的假设，在这个假设成立的情况下模型才是适用的。逻辑回归的第一个基本假设是**假设样本数据的先验分布为伯努利分布。** \n\n总结一下：逻辑回归模型概率估算:\n$$\n\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdot x)\n$$\n逻辑函数：\n$$\n\\sigma(t)=\\frac{1}{1+exp(-t)}\n$$\n单个训练实例的损失函数:\n$$\nc(\\theta)=\n\\begin{cases}\n-log(h_{\\theta (x)}) & (y=1)\\\\\n-log(1-\\theta (x)) & (y=0)\n\\end{cases}\n$$\n$-log p$ 的图像所下：\n\n![](./逻辑回归分类和softmax分类/logx.png)\n\n我们可以看到，当$p$接近于$0$的时候，$-\\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。\n\n\n#### 1.7 scala 代码实现\n\n```scala\npackage ml.scrath.classification\n\nimport scala.collection.mutable.ArrayBuffer\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n\n\nobject LogitRegression{\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map{_.split(\",\").filter(_.length() > 0).map(_.toDouble)}\n      .toArray\n    val data = BDM(dataS:_*)\n\n    val features = data(0 to 98, 0 to 3)\n    val labels = data(0 to 98, 4)\n\n    val model = new LogitRegression\n    val w = model.fit(features,labels)\n    val predictions = model.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate)\n  }\n}\n\n\nclass LogitRegression (var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = (x_train * weights).map(sigmoid(_))\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def sigmoid(inX: Double) = {\n    1.0 / (1 + scala.math.exp(-inX))\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = (x_test * weights).map(sigmoid(_)).map(x => if(x > 0.5) 1.0 else 0.0)\n    output\n  }\n\n}\n```\n\n\n\n#### 1.7 python 代码实现\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # 为X增加一列特征x1，x1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # 梯度训练n_iterations轮\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n\n\nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X = data.data[data.target != 0]\n    y = data.target[data.target != 0]\n    y[y == 1] = 0\n    y[y == 2] = 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    Plot().plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n```\n\nPython结果展示如下【基于PCA将高维数据投影而得】：\n\n![](./逻辑回归分类和softmax分类/logistic.png)\n\n### 2.softmax回归\n\n#### 2.1 算法原理和步骤\n\n对逻辑回归模型做推广，可以支持多个类别了。\n\n原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。\n\n1. 用零（或小的随机值）初始化权重矩阵和偏置值.\n\n2. 对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:\n   $$\ns_k(x)= x\\cdot w_k\n   $$\n向量化表示上式的话，可以写为\n   \n$$\n   socres = X \\cdot W\n$$\n   $X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.\n\n3. 应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:\n   $$\n\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(x))}\n   $$\n   \n4. 计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:\n   $$\n   J(W)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}log(\\hat{p}_k^{(i)})\n   $$\n   \n5. 对于类别k的交叉熵梯度向量:\n   $$\n   \\Delta_{w_k}J(W)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}\n   $$\n   \n6. 更新每个类的权重$W$\n\n   $$\n   w_k = w_k - \\eta \\Delta_{w_k}J\n   $$\n   \n\n​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：\n$$\nH(p,q)=-\\sum_xp(x)logq(x)\n$$\n\n#### 2.2 scala 代码\n\n```scala\npackage ml.scrath.classification\n\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, _}\nimport breeze.numerics._\n\nobject softMax {\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map {\n        _.split(\",\").filter(_.length() > 0).map(_.toDouble)\n      }\n      .toArray\n    val data = BDM(dataS: _*)\n    val features = data(::, 0 to 3)\n    val labels = data(::, 4)\n\n    val soft = new SoftMaxRegression()\n    val w = soft.fit(features, labels)\n    println(w)\n    val predictions = soft.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate) // 正确率为0.9664\n\n  }\n}\n\nclass SoftMaxRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y: BDV[Double]): BDM[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n\n    val ncol = x_train.cols\n    val nclasses = y.toArray.distinct.length\n    var weights = BDM.ones[Double](ncol, nclasses) :* 1.0 / nclasses\n    val n_samples = x_train.rows\n\n    for (iterations <- 0 to num_iters) {\n      val logits = x_train * weights\n      val probs = softmax(logits)\n      val y_one_hot = one_hot(y)\n//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble\n      val error: BDM[Double] = probs - y_one_hot\n      val gradients = (x_train.t * error) :/ n_samples.toDouble\n\n      weights -= gradients :* lr\n    }\n    weights\n  }\n\n  def softmax(logits: BDM[Double]): BDM[Double] = {\n    val scores = exp(logits)\n    val divisor = sum(scores(*, ::))\n    for (i <- 0 to scores.cols - 1) {\n      scores(::, i) := scores(::, i) :/ divisor\n    }\n    scores\n  }\n\n  def one_hot(y: BDV[Double]): BDM[Double] = {\n    val n_samples = y.length\n    val n_classes = y.toArray.toSet.size\n    val one_hot = Array.ofDim[Double](n_samples, n_classes)\n    for (i <- 0 to n_samples - 1) {\n      one_hot(i)(y(i).toInt) = 1.0\n    }\n    BDM(one_hot: _*)\n  }\n\n  def predict(weights: BDM[Double], x: BDM[Double]): BDV[Int] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_test = BDM.horzcat(ones, x)\n    val predictions = argmax(x_test * weights, Axis._1)\n    predictions\n  }\n\n}\n```\n\n\n\n#### 2.3 python 代码\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 12 11:58:06 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\nclass SoftmaxRegressorII:\n\n    def __init__(self,learning_rate = 0.1,n_iters = 1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n\n    def train(self, X, y_true, n_classes):\n\n        x_train = np.column_stack((np.ones(len(X)),X))\n        \n        self.n_samples, n_features = x_train.shape\n        self.n_classes = n_classes\n        \n        self.weights = np.random.rand(n_features,self.n_classes)\n        all_losses = []\n        \n        for i in range(self.n_iters):\n            logits = np.dot(x_train, self.weights)\n            probs = self.softmax(logits)\n            y_one_hot = self.one_hot(y_true)\n            loss = self.cross_entropy(y_one_hot, probs)\n            all_losses.append(loss)\n\n            gradients = (1 / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))\n\n            self.weights = self.weights - self.learning_rate * gradients\n\n#            if i % 100 == 0:\n#                print(f'Iteration number: {i}, loss: {np.round(loss, 4)}')\n\n        return self.weights, all_losses\n\n    def predict(self, X):\n\n        x_test = np.column_stack((np.ones(len(X)), X))\n        scores = np.dot(x_test, self.weights)\n        probs = self.softmax(scores)\n        return np.argmax(probs, axis=1)[:, np.newaxis]\n\n    def softmax(self, logits):\n        exp = np.exp(logits)\n        sum_exp = np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        return exp / sum_exp\n\n    def cross_entropy(self, y_true, scores):\n        loss = - (1 / self.n_samples) * np.sum(y_true * np.log(scores))\n        return loss\n\n    def one_hot(self, y):\n        one_hot = np.zeros((self.n_samples, self.n_classes))\n        one_hot[np.arange(self.n_samples), y.T] = 1\n        return one_hot\n    \nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X= data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = SoftmaxRegressorII()\n    ll = clf.train(X_train, y_train,3)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Reduce dimension to two using PCA and plot the results\n    Plot().plot_in_2d(X_test, y_pred, title=\"SoftMax Regression\", accuracy=accuracy)\n```\n\n结果如图所示：\n\n![](./逻辑回归分类和softmax分类/softmax.png)"},{"title":"降维_线性判别分析","url":"/2020/01/16/降维_线性判别分析/","content":"#### 1. 算法概述\n\nLDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”，如下图所示。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。\n\n![](./降维_线性判别分析/lda_1.png)\n\n#### 2. 算法推导\n\nLDA多分类：假定存在$N$个类，且第$i$类示例树为$m_i$,我们定义全局散度矩阵：\n$$\nS_t = S_b + S_w= \\sum_{i= 1}^{m}(x_i - \\mu)(x_i - \\mu)^T\n$$\n其中$\\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即\n$$\nS_w = \\sum_{i=1}^{N}S_{w_i}\n$$\n其中\n$$\nS_{w_i} = \\sum_{x \\in X_i}(x - \\mu_i)(x - \\mu_i)^T\n$$\n由式$(1)-(3)$可得：\n$$\nS_b = S_t - S_w = \\sum_{i=1}^{N}(\\mu_i - \\mu)(\\mu_i - \\mu)^T\n$$\n多分类LDA有多种实现方式：使用$S_b,S_w,S_t$中的任意两即可。常见的一种实现是采用优化目标\n$$\n\\max_W \\dfrac{tr(W^TS_bW)}{tr(W^{T}S_wW)}\n$$\n其中$W \\in R^{d\\times(N-1)}$,$tr(.)$表示矩阵的迹。式$(5)$可以通过求解如下式的广义特征问题\n$$\nS_bW = \\lambda S_wW\n$$\n$W$的闭式解则是$S_w^{-1}S_b$的N-1个广义特征值所对应的特征向量组成的矩阵。\n\n​\t\t若将$W$视为一个投影矩阵，则多分类LDA将样本矩阵投影到$N-1$维空间。$N-1$通常原小于数据原来的维数，且投影过程中使用了类别信息，因此LDA也被视为一种数据降维的技术。\n\n#### 3. 实现步骤\n\n1. 对于每一类别，计算$d$维数据的均值向量；\n2. 构造类间散度矩阵$S_b$和类内散度矩阵$S_w$;\n3. 计算矩阵$S_w^{-1}S_b$的特征值及对应的特征向量；\n4. 选取前$k$特征值所对应的特征向量，构造$d \\times k$维的转换矩阵$W$,其中特征值以列的形式排列；\n5. 使用转换矩阵$W$将样本映射到新的特征子空间上。\n\n#### 4. LDA与PCA\n\nLDA和PCA都可以用作降维技术，下面比较一下相同点和不同点：\n\n##### 4.1 相同点\n\n​\t1）两者均可以对数据进行降维;\n\n​\t2）两者在降维时均使用了矩阵特征分解的思想;\n\n​\t3）两者都假设数据符合高斯分布.\n\n##### 4.2 不同点\n\n​\t1） LDA是有监督的降维技术，PCA是无监督的降维技术；\n\n​\t2） LDA还可以用于分类，后续在分类时，贴上分类的处理方法；\n\n​\t3） LDA最多可降低到$k$维（$k$是分类的个数),而PCA最多可降低到$n-1$维（$n$是数据的维数)；\n\n​\t4） LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。\n\n#### 5.  LDA的实现代码\n\n##### 5.1 Scala lda实现\n\n```scala\npackage ml.scrath.lda\n\nimport breeze.linalg._\nimport breeze.stats._\nimport org.apache.spark.rdd.RDD\n\nclass LinearDiscriminantAnalysis extends Serializable {\n\n  def fit(data: RDD[DenseVector[Double]], labels: RDD[Int],k:Int) = {\n    val sample = labels.zip(data)\n    computeLDA(sample,k)\n  }\n\n  def computeLDA(dataAndLabels: RDD[(Int, DenseVector[Double])],k:Int)= {\n\n    val featuresByClass = dataAndLabels.groupBy(_._1).values.map(x => rowsToMatrix(x.map(_._2)))\n    val meanByClass = featuresByClass.map(f => mean(f(::, *))) // 对行向量求平均值 each mean is a row vector, not col\n\n    //类内散度矩阵\n    val Sw = featuresByClass.zip(meanByClass).map(f => {\n      val featuresMinusMean: DenseMatrix[Double] = f._1(*, ::) - f._2.t // row vector, not column\n      featuresMinusMean.t * featuresMinusMean: DenseMatrix[Double]\n    }).reduce(_+_)\n\n    val numByClass = featuresByClass.map(_.rows : Double)\n    val features = rddToMatrix(dataAndLabels.map(_._2))\n    val totalMean = mean(features(::, *)) // A row-vector, not a column-vector\n\n    val Sb = meanByClass.zip(numByClass).map {\n      case (classMean, classNum) => {\n        val m = classMean - totalMean\n        (m.t * m : DenseMatrix[Double]) :* classNum : DenseMatrix[Double]\n      }\n    }.reduce(_+_)\n\n    val eigen = eig((inv(Sw): DenseMatrix[Double]) * Sb)\n\n    val eigenvectors = (0 until eigen.eigenvectors.cols).map(eigen.eigenvectors(::, _).toDenseMatrix.t)\n\n    val topEigenvectors = eigenvectors.zip(eigen.eigenvalues.toArray).sortBy(x => -scala.math.abs(x._2)).map(_._1).take(k)\n    val W = DenseMatrix.horzcat(topEigenvectors:_*)\n    (W,Sb,Sw)\n  }\n\n  def rowsToMatrix(in: TraversableOnce[DenseVector[Double]]): DenseMatrix[Double] = {\n    rowsToMatrix(in.toArray)\n  }\n\n  def rowsToMatrix(inArr: Array[DenseVector[Double]]): DenseMatrix[Double] = {\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n\n\n  def rddToMatrix(inArr1: RDD[DenseVector[Double]]): DenseMatrix[Double] = {\n    val inArr = inArr1.collect()\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n}\n```\n\n##### 5.2 scala 测试代码\n\n测试代码（其中数据iris.csv是由下面python代码生成）\n\n```scala\npackage ml.scrath.lda\n\nimport org.apache.spark.sql.SparkSession\nimport breeze.linalg.DenseVector\n\nobject TestLDA extends App {\n\n  val spark =\n    SparkSession.builder()\n      .appName(\"DataFrame-Basic\")\n      .master(\"local[4]\")\n      .config(\"spark.sql.warehouse.dir\", \"file:///E:/spark-warehouse\")\n      .getOrCreate()\n  val sc = spark.sparkContext\n  val irisData = sc.textFile(\"D:\\\\data\\\\iris.csv\")\n\n  val trainData = irisData.map {\n    _.split(\",\").dropRight(1).map(_.toDouble)\n  }.map(new DenseVector(_))\n\n  val labels = irisData.map {\n    _.split(\",\").apply(4).map(_.toInt).apply(0)\n  }\n\n  val start = System.currentTimeMillis()\n  val model = new LinearDiscriminantAnalysis\n  val k = 2\n  val res = model.fit(trainData, labels, k)\n\n  println(\"=====W======\")\n  println(res._1)\n  println(\"=======Sb====\")\n  println(res._2)\n  println(\"=======Sw====\")\n  println(res._3)\n\n}\n```\n\n##### 5.3 scala 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$\n\n![](./降维_线性判别分析/scala_lda_res.png)\n\n##### 5.4 Python lda 代码\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef LDA(X, y, k):\n    '''\n    X为数据集，y为label，k为目标维数\n    '''\n    label_ = np.unique(y)\n    mu = np.mean(X, axis=0)\n\n    Sw = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sw += np.dot((_X - _mean).T,\n                     _X - _mean)\n\n    print(Sw)\n    \n    Sb = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sb += len(_X) * np.dot(( _mean - mu).reshape(\n            (len(mu), 1)), (_mean - mu).reshape((1, len(mu))))\n        \n    print(Sb)\n\n    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # 计算Sw-1*Sb的特征值和特征矩阵\n\n    sorted_indices = np.argsort(eig_vals)\n    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]]  # 提取前k个特征向量\n    return topk_eig_vecs,Sb,Sw\n\n\nif '__main__' == __name__:\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n## 将数据写出到iris.csv文件，供scala使用，保证数据的一致性。\n    data_cat =  np.c_[X,y]\n    import pandas\n    iris_df = pandas.DataFrame(data_cat)\n    iris_df.to_csv(\"iris.csv\",index = 0,header = False)\n\n    W,Sb,Sw = LDA(X, y, 2)\n    X_new = np.dot((X), W)\n    plt.figure(1)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    \n    print(\"========W=========\")\n    print(W)\n    print(\"========Sb========\")\n    print(Sb)\n    print(\"========Sw========\")\n    print(Sw)\n    \n    \n    # 与sklearn中的LDA函数对比\n    lda = LinearDiscriminantAnalysis(n_components=2)\n    lda.fit(X, y)\n    X_new = lda.transform(X)\n#    print(X_new)\n    plt.figure(2)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    plt.show()\n\n```\n\n##### 5.5 Python 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$,可见scala和python得到结果是一致的。\n\n![](./降维_线性判别分析/python_lda_res.png)\n\n"},{"title":"线性回归","url":"/2020/01/09/线性回归/","content":"### 线性回归的代码实现\n\n#### 1. 线性回归推导\n\n​\t对于给定了$m$个样本点($x^{(1)}$,$y^{(1)}$)，($x^{(2)}$,$y^{(2)}$)，$\\dots$，($x^{(m)}$,$y^{(m)}$)，其中$x^{(i)}\\in\\R^{n}$。定义假设函数为$h_\\theta(x)$，即$h_\\theta(x)$为最终的拟合函数，$\\theta$为待拟合参数也称作权重。\n$$\n\\begin{aligned}\nh_\\theta(x)&=\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n \\\\\n&=\\begin{bmatrix}\n\\theta_0, & \\theta_1,& \\cdots ,& \\theta_n \n\\end{bmatrix}\n\\begin{bmatrix} 1\\\\x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\\\\\n&=\\theta^Tx\n\n\\end{aligned}\n$$\n\n##### 1.1 从样本数据出发推导损失函数\n\n​\t在样本数据中$y^{(i)}$是实际存在值而$h_\\theta(x^{(i)})$对应的是模型预测值，显然如果想要模型预测的效果好，那么对应的误差就要小，假设函数在任意样本点的误差为$|h_\\theta(x^{(i)})-y^{(i)}|$则$m$个样本点的误差和为$\\ \\sum\\limits_{i=1}^m|h_\\theta(x^{(i)})-y^{(i)}|$，因此问题就转化为求解$\\  \\begin{aligned}\\mathop{\\arg\\min}_{\\theta}\\ \\sum\\limits_{i=1}^m|h_\\theta(x^{(i)}-y^{(i)}|\\end{aligned}$为了后续求解最优值(绝对值函数不好求导)，所以损失函数采用了误差平方和的形式$\\ \\begin{aligned}\\mathop{\\arg\\min}_{\\theta} \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)}-y^{(i)})^2\\end{aligned}$。\n\n##### 1.2 从统计学理论出发推导损失函数\n\n​\t为什么线性回归问题的损失函数会是误差平方和的形式？这里从统计理论上进行解释，对于给定的$y^{(i)}$总能找到$\\varepsilon^{(i)}$使得这个等式成立$y^{(i)}=h_\\theta(x^{(i)})+\\varepsilon^{(i))}$，$\\ \\varepsilon^{(i)}$代表真实值和预测值之间的误差且$\\varepsilon^{(i)} \\sim \\mathcal{N}(0,\\sigma^2)$,简单解释下为什么误差$\\varepsilon^{(i)}$会服从均值为零的正态分布，误差的产生有很多种因素的影响，误差可以看作是这些因素(随机变量)之和共同作用而产生的，由中心极限定理可知随机变量和的分布近似的服从正态分布；更通俗易懂的解释是，当你在选择$h_\\theta(x)$时主观的会认定这个$h_\\theta(x)$是比较符合样本数据的，比如对一些样本数据可视化后，发现样本数据明显是趋近于一条直线，而你在对$h_\\theta(x)$的选择上肯定会选择直线方程作为$h_\\theta(x)$而不会选择多项式函数作为$h_\\theta(x)$。而这种$h_\\theta(x)$一旦选定，可以认为大部分数据都在$h_\\theta(x)$的附近，因此误差大部分集中在零值附近所以$\\mathcal{N}(0,\\sigma^2)$作为$\\varepsilon^{(i)}$的先验分布是比较合理的。\n\n​\t随机变量$\\varepsilon^{(i)}$的概率密度函数为：\n$$\np(\\varepsilon^{(i)}) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(\\varepsilon^{(i)})^2}{2\\sigma^2}}\n$$\n​\t代入$h_\\theta(x^{(i)}), \\ y^{(i)}$则\n$$\np(y^{(i)}|x^{(i)};\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(\\theta^Tx^{(i)}-y^{(i)})^2}{2\\sigma^2}}\n$$\n这里的$p(y^{(i)}|x^{(i)};\\theta)$并不代表条件概率，只是一个记号它表示给定$x^{(i)},\\ y^{(i)}\\ $和一组$\\theta$后的概率密度函数。由最大似然估计可知：\n$$\n\\begin{aligned}\nL(\\theta)&=\\prod\\limits_{i=1}^mp(y^{(i)}|x^{(i)};\\theta)\\\\\n&=\\prod\\limits_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(\\theta^Tx^{(i)}-y^{(i)})^2}{2\\sigma^2}}\n\\end{aligned}\n$$\n对$L(\\theta)$取对数从而得到对数化最大似然估计函数\n$$\n\\begin{aligned}\n\\mathcal L(\\theta) &= \\mathcal log(L(\\theta)) \\\\\n&=\\mathcal \\log \\prod\\limits_{i=1}^m\\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(\\theta^Tx^{(i)}-y^{(i)})^2}{2\\sigma^2}}\\\\\n&=\\sum\\limits_{i=1}^m \\mathcal \\log\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(\\theta^Tx^{(i)}-y^{(i)})^2}{2\\sigma^2}}\\\\\n&=m \\mathcal \\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})^2\n\\end{aligned}\n$$\n模型的损失函数$\\ J(\\theta)=\\dfrac{1}{2}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$，其实只是在先前的最优化模型前面乘了一个常系数$\\dfrac{1}{2}$对最优化的结果并不会产生影响。\n\n#### 2. 梯度下降算法求解凸优化问题\n\n​\t在机器学习算法中，求解最小化损失函数时，可以通过梯度下降法来一步步进行迭代求解，从而得到最小化损失函数的模型参数值，梯度下降算法不一定能够找到全局的最优解，有可能是一个局部最优解。然而，如果损失函数是凸函数，那么梯度下降法得到的解就一定是全局最优解。  在这里不加证明的给出梯度下降法的迭代格式:\n$$\n\\theta_j:=\\theta_j-\\alpha\\nabla_{\\theta_j} {J(\\theta)}\n$$\n其中$\\theta_j$为假设函数$h_\\theta(x)$的参数值，$\\alpha \\in (0,1]$且为常数代表模型学习速率，$\\nabla _{\\theta_j} J(\\theta)$为损失函数$J(\\theta)$对参数$\\theta_j$的梯度。\n\n​\t下面使用梯度下降算法求解上面推导出的线性回归模型的损失函数$\\ J(\\theta)=\\dfrac{1}{2}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$，为了实现该算法首先要求出损失函数$J(\\theta)$对参数$\\theta_j$的梯度：\n$$\n\\begin{aligned}\n\\nabla_{\\theta_j} J(\\theta) &=\\nabla_{\\theta_j} \\dfrac{1}{2} \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2\\\\\n&=2\\cdot\\dfrac{1}{2}\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)}) \\nabla_{\\theta_j}(h_\\theta(x^{(i)})-y^{(i)})\\\\\n&=\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)}) \\nabla_{\\theta_j}\\theta^Tx^{(i)}\\\\\n&=\\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n\\end{aligned}\n$$\n因此在线性回归模型中利用所有的样本数据，训练梯度下降算法的完整迭代格式为\n$$\n\\theta_j:=\\theta_j-\\alpha \\sum\\limits_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\ \\ \\ ( j\\ \\  for \\ \\ 0 \\ \\sim \\ n)\n$$\n上述迭代过程每次迭代都会使用所有的样本数据，数学上已经证明线性回归模型的损失函数通过梯度下降算法求解一定会全局收敛，所以如果要编程实现该算法只需要控制迭代次数即可，不过对于线性回归模型的求解一般不用梯度下降算法，还有更容易实现且更快捷的形式—正规方程。\n\n\n\n#### 3. 正规方程\n\n多元线性回归的损失函数为：\n$$\nJ=\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2\n$$\n其中：$\\hat{y}^{(i)} = \\theta_{0} + \\theta_{1}X_{1}^{(i)} + \\theta_{2}X_{2}^{(i)} + ... + \\theta_{n}X_{n}^{(i)}$ 。\n\n对 $J$ 求导为：\n$$\n\\nabla J=(\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})\n$$\n其中：$\\frac{\\partial J}{\\partial \\theta_i}$ 为偏导数，与导数的求法一样。\n\n\n\n对 $\\nabla J$ 进一步计算：\n$$\n\\nabla J(\\theta) =  \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} =   \\begin{pmatrix} \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-1) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_1^{(i)}) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_2^{(i)}) \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_n^{(i)}) \\end{pmatrix} = 2·\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n\n其中：$X_b = \\begin{pmatrix}\n1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n1 & X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)}\n\\end{pmatrix}$\n\n\n\n​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：\n$$\n\\nabla J(\\theta)  = \\frac{2}{m}\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n​        此时，目标函数就成了使 $\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：\n$$\nJ(\\theta) = MSE(y, \\hat{y})\n$$\n​\t\t\n\n注1. 有时候目标函数也去 $\\frac{1}{\\boldsymbol{2}m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ ,其中的**2**是为了抵消梯度中的2，实际的效果有限。\n\n注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。\n\n\n\n#### 2.实现代码\n\n##### 2.1 python版本实现\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  8 17:25:39 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\n\nclass LinearRegression():\n    def __init__(self,lr=.01, num_iters=10000,tolerance=1e-8):\n        self.lr = lr\n        self.num_iters = num_iters\n        self.w = None\n        self.toterance = tolerance\n\n    def fit(self,x_train, y_train):\n        n_samples = len(x_train)\n        x_train = np.column_stack((np.ones(n_samples), x_train)) #也可以写成np.c_\n        \n        self.w = .01 * np.ones(x_train.shape[1])\n        self.loss_ = [0]\n        \n        # w_{i} = w_{i} - lr * (h(w_{i}) - y)*x_{i} 迭代公式\n        self.count = 0\n        for iteration in range(self.num_iters):\n            self.count += 1\n            raw_output = np.matmul(x_train, self.w)\n            errors =  raw_output - y_train\n            loss = 1/(2 * n_samples) * errors.dot(errors)\n            delta_loss = loss - self.loss_[-1]\n\n            self.loss_.append(loss)\n            if np.abs(delta_loss) < self.toterance:\n                break\n            else:\n                grad = (1.0 /n_samples) *np.matmul(x_train.T, np.array(errors))\n                self.w -= self.lr * grad\n\n    def predict(self,x_test):\n        x_test = np.column_stack((np.ones(len(x_test)), x_test))\n        \n        output = np.matmul(x_test, self.w)\n        return output\n    \nif __name__ == '__main__':\n    \n    num_inputs = 2\n    num_examples = 10000\n    true_w = [6.4, -3.2]\n    true_b = 2.3\n    features = np.random.random((num_examples, num_inputs))\n    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n    labels += np.random.normal(0, 0.1,size = len(labels))\n    \n    import sklearn.model_selection\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    print(\"回归系数:\",lr.w)\n    print(\"迭代次数:\",lr.count)\n    \n    y_pred = lr.predict(x_test)\n    from sklearn import metrics\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    print(\"MSE: %.4f\" % mse)\n\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    print(\"MAE: %.4f\" % mae)\n\n    R2 = metrics.r2_score(y_test,y_pred)\n    print(\"R2: %.4f\" % R2)\n```\n\n##### 2.2 Scala版本实现\n\n```scala\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n * Scala 版本的实现\n */\n\nobject LinearRegression {\n  def main(args: Array[String]): Unit = {\n    val num_inputs = 2\n    val num_examples = 1000\n    val x_train: BDM[Double] = BDM.rand(num_examples, num_inputs)\n    val ones = BDM.ones[Double](num_examples, 1)\n    val x_cat = BDM.horzcat(ones, x_train)\n    val y_train = x_cat * BDV(2.3, 6.4, -3.2)\n\n    val model = new LinearRegression(num_iters = 10000)\n    val weights = model.fit(x_train, y_train)\n    val predictions = model.predict(weights, x_train)\n    println(\"梯度下降求解的权重为：\" + weights)\n    println(predictions)\n  }\n}\n\nclass LinearRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = x_train * weights\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = x_test * weights\n    output\n  }\n}\n```\n\n参考文献：1. [机器学习之梯度下降法与线性回归](https://segmentfault.com/a/1190000017048213)\n\n\n\n"},{"title":"神经网络","url":"/2019/11/27/神经网络-1/","content":"\n### 神经网络的numpy实现和公式推导\n\n\n\n\n\n​       过去10多年是神经网络发展的黄金时期，神经网络(深度学习)成为了新时代的一种浪潮，所以今天借用国外一个小哥实现纯numpy的神经网络，来记录神经网络的实现过程。\n\n![](./神经网络-1/nn_architecture.png)\n\n\n\n#### 概览\n\n​        在开始编程之前，先让我们准备一份基本的路线图。我们的目标是创建一个特定架构（层数、层大小、激活函数）的密集连接神经网络。然后训练这一神经网络并做出预测。\n\n![](./神经网络-1/blueprint.gif)\n\n上面的示意图展示了训练网络【特别是正向传播和反向传播的操作】时进行的操作，以及单次迭代不同阶段需要更新和读取的参数。\n\n#### 初始化神经网络层\n\n  让我们从初始化每一层的权重矩阵$W$和偏置向量$b$开始。下图展示了网络层l的权重矩阵和偏置向量，其中，上标$[l]$表示当前层的索引，$n$表示给定层中的神经元数量。\n\n![](./神经网络-1/params_sizes.png)\n\n我们的程序也将以类似的列表形式描述神经网络架构。列表的每一项是一个字典，描述单个网络层的基本参数：`input_dim`是网络层输入的信号向量的大小，`output_dim`是网络层输出的激活向量的大小，`activation`是网络层所用的激活函数。\n\n```python\nnn_architecture = [\n    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n]\n```\n\n值得注意的是，一个网络层的输出向量同时也是下一层的输入。\n\n```python\ndef init_layers(nn_architecture, seed = 99):\n    np.random.seed(seed)\n    number_of_layers = len(nn_architecture)\n    params_values = {}\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        layer_input_size = layer[\"input_dim\"]\n        layer_output_size = layer[\"output_dim\"]\n\n        params_values['W' + str(layer_idx)] = np.random.randn(\n            layer_output_size, layer_input_size) * 0.1\n        params_values['b' + str(layer_idx)] = np.random.randn(\n            layer_output_size, 1) * 0.1\n\n    return params_values\n```\n\n​        上面的代码初始化了网络层的参数。注意我们用随机的小数字填充矩阵**W**和向量**b**。这并不是偶然的。权重值无法使用相同的数字初始化，否则会造成**破坏性的对称问题**。**基本上，如果权重都一样，不管输入X是什么，隐藏层的所有单元也都一样**。这样，我们就会陷入初始状态，不管训练多久，网络多深，都无望摆脱。线性代数不会原谅我们。\n\n[^初始化方法包含很多种，我们这里简便起见，使用随机初始化的方式生成权重]: \n\n​       小数值增加了算法的效率。我们可以看看下面的sigmoid函数图像，大数值处的函数图像几乎是扁平的，这会对神经网络的学习速度造成显著影响。所有参数使用小随机数是一个简单的方法，但它保证了算法有一个**足够好**的开始。\n\n![](./神经网络-1/activations.gif)\n\n#### 激活函数\n\n​        激活函数只需一行代码就可以定义，但它们给神经网络带来了非线性和所需的表达力。**“没有它们，神经网络将变成线性函数的组合，也就是单个线性函数。”**激活函数有很多种，但在这个项目中，我决定使用其中两种——sigmoid和ReLU。为了同时支持前向传播和反向传播，我们还需要准备好它们的导数。\n\n```python\ndef sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0,Z)\n\ndef sigmoid_backward(dA, Z):\n    sig = sigmoid(Z)\n    return dA * sig * (1 - sig)\n\ndef relu_backward(dA, Z):\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0;\n    return dZ;\n```\n\n#### 前向传播\n\n​    我们设计的神经网络有一个简单的架构。输入矩阵**X**传入网络，沿着隐藏单元传播，最终得到预测向量**Y_hat**。为了让代码更易读，我将前向传播拆分成两个函数——单层前向传播，和整个神经网络前向传播。\n\n​\t    **前向传播的过程是：输入$a^{[l-1]}$, 输出$a^{[l]}$, 缓存为$z^{[l]}$,  从方便实现的角度上看，$z^{[l]}$是$w^{[l]}$，$b^{[l]}$的函数。**\n\n```python\ndef single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n    Z_curr = np.dot(W_curr, A_prev) + b_curr\n    if activation is \"relu\":\n        activation_func = relu\n    elif activation is \"sigmoid\":\n        activation_func = sigmoid\n    else:\n        raise Exception('Non-supported activation function')\n\n    return activation_func(Z_curr), Z_curr\n```\n这部分代码大概是最直接，最容易理解的。给定来自上一层的输入信号，我们计算仿射变换**Z**，接着应用选中的激活函数。基于NumPy，我们可以对整个网络层和整批样本一下子进行矩阵操作，无需迭代，这大大加速了计算。除了计算结果外，函数还返回了一个反向传播时需要用到的中间值**Z**。\n\n![](./神经网络-1/matrix_sizes_2.png)\n\n基于单层前向传播函数，编写整个前向传播步骤很容易。这是一个略微复杂一点的函数，它的角色不仅是进行预测，还包括组织中间值。\n$$\n\\begin{aligned}\nz^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}  \\\\\n\na^{[l]} = g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n\n\n```python\ndef full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n\n        activ_function_curr = layer[\"activation\"]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        b_curr = params_values[\"b\" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\n        memory[\"A\" + str(idx)] = A_prev\n        memory[\"Z\" + str(layer_idx)] = Z_curr\n\n    return A_curr, memory\n```\n\n#### 损失函数\n\n​          损失函数可以监测进展，确保我们向着正确的方向移动。**“一般来说，损失函数是为了显示我们离‘理想’解答还有多远。”**损失函数根据我们计划解决的问题而选用，Keras之类的框架提供了很多选项。因为我计划将神经网络用于二元分类问题，我决定使用交叉熵：\n$$\nJ(W,b) = \\dfrac{1}{m}{\\sum}_{i=1}^{m}L(\\hat{y}^{i} - y^{i})  \\\\\nL(\\hat{y} - y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))\n$$\n为了取得更多关于学习过程的信息，我决定另外实现一个计算精确度的函数。\n\n```python\n'''\nJ(W,b) = 1/m sum_{i}^{m}L(y^{hat}_{i} - y_{i})\nL(y^{hat}_{i} - y_{i}) = -(ylogy^{hat} + (1-y)log(1-y^{hat}))\n'''\n\ndef get_cost_value(Y_hat, Y):\n    m = Y_hat.shape[1]\n    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n    return np.squeeze(cost)\n\n# an auxiliary function that converts probability into class\ndef convert_prob_into_class(probs):\n    probs_ = np.copy(probs)\n    probs_[probs_ > 0.5] = 1\n    probs_[probs_ <= 0.5] = 0\n    return probs_\n\ndef get_accuracy_value(Y_hat, Y):\n    Y_hat_ = convert_prob_into_class(Y_hat)\n    return (Y_hat_ == Y).all(axis=0).mean()\n```\n\n#### 反向传播\n\n不幸的是，很多缺乏经验的深度学习爱好者都觉得反向传播很吓人，难以理解。微积分和线性代数的组合经常会吓退那些没有经过扎实的数学训练的人。所以不要过于担心你现在还不能理解这一切。相信我，我们都经历过这个过程。\n\n```python\ndef single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n    # number of examples\n    m = A_prev.shape[1]\n    \n    # selection of activation function\n    if activation is \"relu\":\n        backward_activation_func = relu_backward\n    elif activation is \"sigmoid\":\n        backward_activation_func = sigmoid_backward\n    else:\n        raise Exception('Non-supported activation function')\n    \n    # calculation of the activation function derivative\n    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n    \n    # derivative of the matrix W\n    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n    # derivative of the vector b\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n    # derivative of the matrix A_prev\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n\n    return dA_prev, dW_curr, db_curr\n\n```\n\n​        人们经常搞混反向传播和梯度下降，但事实上它们不一样。前者是为了高效地计算梯度，后者则是为了基于计算出的梯度进行优化。在神经网络中，我们计算损失函数在参数上的梯度，但反向传播可以用来计算任何函数的导数。**反向传播算法的精髓在于递归地使用求导的链式法则，通过组合导数已知的函数，计算函数的导数**。下面的公式描述了单个网络层上的反向传播过程。由于本文的重点在实际实现，所以我将省略求导过程。从公式上我们可以很明显地看到，为什么我们需要在前向传播时记住中间层的**A**、**Z**矩阵的值。\n\n$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} =\\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}} \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{W}^{[l]}} =  \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n\n$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}}  = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}   \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{b}^{[l]}}= \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n\n$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} =  \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}  \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n\n$$\\boldsymbol{dZ}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}= \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l]}} \\frac{\\partial {A}^{[l]} }{\\partial \\boldsymbol{Z}^{[l]}}=\\boldsymbol{dA}^{[l]} * g^{[l]}{'}(\\boldsymbol{Z}^{[l]})$$\n\n![](./神经网络-1/640.webp)\n\n和前向传播一样，我决定将计算拆分成两个函数。之前给出的是单个网络层的反向传播函数，基本上就是以NumPy方式重写上面的数学公式。而定义完整反向传播过程的函数，主要是读取、更新三个字典中的值。\n\n```python\ndef full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n    grads_values = {}\n    \n    # number of examples\n    m = Y.shape[1]\n    # a hack ensuring the same shape of the prediction vector and labels vector\n    Y = Y.reshape(Y_hat.shape)\n    \n    # initiation of gradient descent algorithm\n    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n    \n    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n        # we number network layers from 1\n        layer_idx_curr = layer_idx_prev + 1\n        # extraction of the activation function for the current layer\n        activ_function_curr = layer[\"activation\"]\n        \n        dA_curr = dA_prev\n        \n        A_prev = memory[\"A\" + str(layer_idx_prev)]\n        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n        \n        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n        \n        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n    \n    return grads_values\n```\n\n基于单个网络层的反向传播函数，我们从最后一层开始迭代计算所有参数上的导数，并最终返回包含所需梯度的python字典。\n\n\n\n#### 更新参数值\n\n反向传播是为了计算梯度，以根据梯度进行优化，更新网络的参数值。为了完成这一任务，我们将使用两个字典作为函数参数：`params_values`，其中保存了当前参数值；`grads_values`，其中保存了用于更新参数值所需的梯度信息。现在我们只需在每个网络层上应用以下等式即可。这是一个非常简单的优化算法，但我决定使用它作为更高级的优化算法的起点（大概会是我下一篇文章的主题）。\n\n$$\\boldsymbol{W}^{[l]} = \\boldsymbol{W}^{[l]} - \\alpha \\boldsymbol{dW}^{[l]} $$\n\n$$\\boldsymbol{b}^{[l]} = \\boldsymbol{b}^{[l]} - \\alpha \\boldsymbol{b}^{[l]} $$\n\n```\ndef update(params_values, grads_values, nn_architecture, learning_rate):\n\n    # iteration over network layers\n    for layer_idx, layer in enumerate(nn_architecture, 1):\n        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n\n    return params_values;\n```\n\n#### 整合一切\n\n万事俱备只欠东风。最困难的部分已经完成了——我们已经准备好了所需的函数，现在只需以正确的顺序把它们放到一起。\n\n```python\ndef train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n    # initiation of neural net parameters\n    params_values = init_layers(nn_architecture, 2)\n    # initiation of lists storing the history \n    # of metrics calculated during the learning process \n    cost_history = []\n    accuracy_history = []\n    \n    # performing calculations for subsequent iterations\n    for i in range(epochs):\n        # step forward\n        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n        \n        # calculating metrics and saving them in history\n        cost = get_cost_value(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        # step backward - calculating gradient\n        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n        # updating model state\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        if(i % 50 == 0):\n            if(verbose):\n                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n            if(callback is not None):\n                callback(i, params_values)\n            \n    return params_values\n```\n\n\n"}]