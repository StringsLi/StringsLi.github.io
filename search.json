[{"title":"风控算法工程师面试指南","url":"/2020/11/23/风控算法工程师面试指南/","content":"# 风控模型师面试指南\n\n## **一. 算法**\n\n## **1.逻辑回归**\n\n**1.1  逻辑回归的优缺点，在金融领域相比其他算法有什么优势，局限性在哪？**\n\n1）优点：\n\n- 实现简单，速度快，占用内存小，可在短时间内迭代多个版本的模型。\n- 模型的可解释性非常好，可以直接看到各个特征对模型结果的影响，可解释性在金融领域非常重要，所以在目前业界大部分使用的仍是逻辑回归模型。\n- 模型客群变化的敏感度不如其他高复杂度模型，因此稳健更好，鲁棒性更强。\n- 特征工程做得好，模型的效果不会太差，并且特征工程可以并行开发，大大加快开发的速度。\n- 模型的结果可以很方便的转化为策略规则，且线上部署简单。\n\n2）缺点和局限性:\n\n- 容易欠拟合，相比集成模型，准确度不是很高。\n- 对数据的要求比较高，逻辑回归对缺失值，异常值，共线性都比较敏感，且不能直接处理非线性的特征。所以在数据清洗和特征工程上会花去很大部分的时间。\n- 在金融领域对场景的适应能力有局限性，例如数据不平衡问题，高维特征，大量多类特征，逻辑回归在这方面不如决策树适应能力强。\n\n**1.2 : 逻辑回归是线性模型吗？逻辑回归和线性回归的区别？**\n\n- 逻辑回归是一种广义线性模型，它引入了$Sigmod$函数，是非线性模型，但本质上还是一个线性回归模型，因为除去$Sigmod$函数映射关系，其他的算法原理，步骤都是线性回归的。\n- 逻辑回归和线性回归首先都是广义的线性回归，在本质上没多大区别，区别在于逻辑回归多了个$Sigmod$函数，使样本映射到$[0,1]$之间的数值，从而来处理分类问题。另外逻辑回归是假设变量服从伯努利分布，线性回归假设变量服从高斯分布。逻辑回归输出的是离散型变量，用于分类，线性回归输出的是连续性的，用于预测。逻辑回归是用最大似然法去计算预测函数中的最优参数值，而线性回归是用最小二乘法去对自变量因变量关系进行拟合。\n\n**1.3：逻辑回归做分类的样本应该满足什么分布？**\n\n应该满足伯努利分布，逻辑回归的分类标签是基于样本特征通过伯努利分布产生的，分类器要做的就是估计这个分布。\n\n**1.4：逻辑回归解决过拟合的方法有哪些？**\n\n- 减少特征数量，在实际使用中会用很多方法进行特征筛选，例如基于$IV$值的大小，变量的稳定性，变量之间的相关性等。\n- 正则化，常用的有$L1$正则化和$L2$正则化。\n\n**1.5：什么是特征的离散化和特征交叉？逻辑回归为什么要对特征进行离散化？**\n\n- 特征离散化是将数值型特征（一般是连续型的）转变为离散特征，例如评分卡中的$woe$转化，就是将特征进行分箱，再将每个分箱映射到$woe$值上，就转换为了离散特征。特征交叉也叫作特征组合，是将单独的特征进行组合，使用相乘/相除/笛卡尔积等形成合成特征，有助于表示非线性关系。比如使用$One-Hot$向量的方式进行特征交叉。这种方式一般适用于离散的情况，我们可以把它看做基于业务理解的逻辑和操作，例如经度和纬度的交叉，年龄和性别的交叉等。\n- 实际工作中很少直接将连续型变量带入逻辑回归模型中，而是将特征进行离散化后再加入模型，例如评分卡的分箱和$woe$转化。这样做的优势有以下几个：1）特征离散化之后，起到了简化模型的作用，使模型变得更稳定，降低了模型过拟合的风险。2）离散化之后的特征对异常数据有很强的鲁棒性，实际工作中的哪些很难解释的异常数据一般不会做删除处理，如果特征不做离散化，这个异常数据带入模型，会给模型带来很大的干扰。3）离散特征的增加和减少都很容易，且稀疏向量的内积乘法运算速度快，易于模型的快速迭代。4）逻辑回归属于广义线性模型，表达能力有限，特征离散化之后，每个离散变量都有单独的权重，相当于给模型引入了非线性，能够提高模型的表达能力。5）离散化后的特征可进行特征交叉，进一步引入非线性，提高模型的表达能力。\n\n**1.6：在逻辑回归中，为什么要常常做特征组合（特征交叉）？**\n\n逻辑回归模型属于线性模型，线性模型不能很好处理非线性特征，特征组合可以引入非线性特征，提升模型的表达能力。另外，基本特征可以认为是全局建模，组合特征更加精细，是个性化建模，但对全局建模会对部分样本有偏，对每一个样本建模又会导致数据爆炸，过拟合，所以基本特征+特征组合兼顾了全局和个性化。\n\n**1.7 ：做评分卡中为什么要进行WOE化？**\n\n- 更好的解释性，变量离散化之后可将每个箱体映射到woe值，而不是通常做one-hot转换。\n- woe化之后可以计算每个变量的IV值，可用来筛选变量。\n- 对离散型变量，woe可以观察各个level间的跳转对odds的提升是否呈线性。\n- 对连续型变量，woe和IV值为分箱的合理性提供了一定的依据，也可分析变量在业务上的可解释性。\n- 用woe编码可以处理缺失值问题。\n\n**1.8：高度相关的特征带入逻辑回归到底有什么影响？为什么逻辑回归要将高度相关特征剔除？**\n\n- 在损失函数最终收敛的情况下，就算有很多相关度很高的特征，也不会影响模型的效果。假设一个特征将它重复100次，生成100个高度相关的特征。那么模型训练完之后，这100个特征和原来那一个特征扮演的效果一样，每一个特征的权重都是原来特征的1/100，只是可能中间很多特征的系数正负相互抵消了，比如做评分卡，如果引入了高度相关的特征，那么最后逻辑回归的系数符号可能就会不一致。\n- 虽然高度相关特征对模型结果没什么大的影响，但还是要剔除相关性高的特征，原因是一个可以减少特征数量，提高模型的训练速度，减少过拟合的风险。二是去掉高相关特征可以让模型的可解释性更好。尤其在做评分卡时，为了使最后每个特征的系数符号一致，必须做特征相关性筛选。\n\n**1.9：逻辑回归的特征系数的绝对值可以认为是特征的重要性吗？**\n\n首先特征系数的绝对值越大，对分类效果的影响越显著，但不能表示系数更大的特征重要性更高。因为改变变量的尺度就会改变系数的绝对值，而且如果特征是线性相关的，则系数可以从一个特征转移到另一个特征，特征间相关性越高，用系数解释变量的重要性就越不可靠。\n\n**1.10：逻辑回归为什么要用极大似然函数作为损失函数？**\n\n- 如果用最小二乘法，目标函数就是 $E_{w,b}=\\sum_{i=1}^{m}\\left ( y_{i}-\\frac{1}{1+e^{-\\left ( w^{T}x_{i}+b \\right )}}\\right )^2$ ,是非凸的，不容易求解，会得到局部最优。\n- 如果用最大似然估计，目标函数就是对数似然函数： $l_{w,b}=\\sum_{i=1}^{m}\\left ( -y_{i}\\left ( w^{T}x_{i}+b \\right )+ln\\left ( 1+e^{w^{T}x_{i}+b} \\right ) \\right )$,是关于 $(w,b)$ 的高阶连续可导凸函数，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。  \n\n## **2.决策树**\n\n**2.1：决策树模型的优缺点及适用性？**\n\n优点：\n\n- 易于理解，决策树可以生成IF..TEHN逻辑表达的树结构，可解释性很好。\n- 相比逻辑回归对数据的处理较简单，不太需要做例如数据离散化，归一化等操作。\n- 决策树是目前已知的对于处理非线性交互的最好的算法。\n- 模型的效果比较好，例如随机森林，$xgboost$都是基于决策树构建的。\n\n缺点：\n\n- 很容易在训练过程中生成过于复杂的树结构，造成过拟合。\n- 不适合处理高维数据，当属性数量过大时，部分决策树就不适用了。\n- 泛化能力能力比较差，对于没有出现过的值几乎没有办法。\n\n**2.2：简述一下决策树的原理以及树的构建过程。**\n\n决策树时基于树的结构进行决策的，学习过程包括特征选择，决策树的生成和剪枝过程。决策树的学习过程通常是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，选择最优特征，该特征有几种值就划分为多少子集，每个子集递归调用此方法，返回结点，返回的结点就是上一层的子节点，直到所有特征都已经用完，或者数据集只有一维特征为止。\n\n**2.3：简述一下ID3，C4.5，CART三类决策树的原理和异同点。**\n\n- ID3选择最佳分割点是基于信息增益的，信息增益越大，表明使用这个属性来划分所获得的“纯度提升”越大。C4.5对ID3进行了改进，因为ID3使用的信息增益对数据划分时，可能出现每个结点只包含一个样本，这些子节点的纯度已经达到最大，但是，这样的决策树并不具有泛化能力，无法对新样本进行预测。且ID3不能处理连续型变量和缺失值。而C4.5使用信息增益率来选择属性，克服了信息增益选择属性时偏向选择值多的属性的不足。且可以处理连续型变量和缺失值。\n- C4.5是基于ID3的改进版，只能用于分类。而CART树既可以做分类，也可以做回归。CART的本质是对特征空间进行二元划分，所以CART生成的是一颗二叉树，且可以对类别型变量和数值型变量进行分裂。对分类型变量进行划分时，分为等于该属性和不等于该属性，在对连续型变量进行划分时，分为大于和小于，在做分类是使用的是GINI系数作为划分标准，在做回归时使用的是均方误差。\n\n**2.4：分类树和回归树的区别在哪里？**\n\n- 分类树以C4.5为例，在对一个特征进行划分时，是穷举这个特征的每一个阈值，找到使得特征<=阈值和特征>阈值分成的两个分支的熵的最大值，按照该标准分支得到两个新的节点，用同样的方法继续分支，直到得到种类唯一的叶子节点，或者达到预设的终止条件为止。\n- 回归树的流程是类似分类树的，区别在于划分时的标准不再是最大熵，而是最小化均差，如果节点的预测值错的越离谱，均方差越大，通过最小化均差能够找到最可靠的分支依据。\n\n**2.5：决策树对缺失值是如何处理的？**\n\n决策树处理缺失要考虑以下三个问题：\n\n1. 当开始选择哪个属性来划分数据集时，样本在某几个属性上有缺失怎么处理：\n\n- 忽略这些缺失的样本。\n- 填充缺失值，例如给属性A填充一个均值或者用其他方法将缺失值补全。\n- 计算信息增益率时根据缺失率的大小对信息增益率进行打折，例如计算属性A的信息增益率，若属性A的缺失率为0.9，则将信息增益率乘以0.9作为最终的信息增益率。\n\n2. 一个属性已经被选择，那么在决定分割点时，有些样本在这个属性上有缺失怎么处理？\n\n- 忽略这些缺失的样本。\n- 填充缺失值，例如填充一个均值或者用其他方法将缺失值补全。\n- 把缺失的样本，按照无缺失的样本被划分的子集样本个数的相对比率，分配到各个子集上去，至于那些缺失样本分到子集1，哪些样本分配到子集2，这个没有一定准则，可以随机而动。\n- 把缺失的样本分配给所有的子集，也就是每个子集都有缺失的样本。\n- 单独将缺失的样本归为一个分支。\n\n3.决策树模型构建好后，测试集上的某些属性是缺失的，这些属性该怎么处理？\n\n- 如果有单独的缺失值分支，依据此分支。\n- 把待分类的样本的属性A分配一个最常出现的值，然后进行分支预测。\n- 待分类的样本在到达属性A结点时就终止分类，然后根据此时A结点所覆盖的叶子节点类别状况为其分配一个发生概率最高的类。\n\n**2.6：为什么决策树不需要对数据做归一化等预处理？**\n\n决策树是一种概率模型，所以不需要做归一化，因为它不关心变量的值，而是关心变量的分布和变量之间的条件概率，所以归一化这种数值缩放，不影响分裂结点位置。\n\n**2.7：如何解决决策树的过拟合问题？**\n\n- 预剪枝的方法：通过提前停止树的构建而对树剪枝，是目前解决过拟合的主要方法。常用的剪枝条件包括限制树的深度，限制叶节点最小样本数，限制叶节点的最小样本权重，限制叶节点的信息增益值的阈值等。\n- 后剪枝的方法：首先构造完整的决策树，允许树过度拟合数据，然后应单个结点代替子树，节点的分类采用子树的主要分类。剪枝方法有错误率降低剪枝，悲观错误剪枝，代价复杂度剪枝\n\n\n\n## **3.集成学习**\n\n**3.1：什么是集成学习？集成学习有哪些框架？简单介绍各个框架的常用算法。**\n\n- 集成学习是一种优化手段和策略，通常是结合多个简单的弱分类器来集成模型组，去做更可靠的决策。一般的弱分类器可以是决策树，$SVM$，$kNN$等构成，其中的模型可以单独来训练，并且这些弱分类器以某种方式结合在一起去做出一个总体预测。集成学习就是找出哪些弱分类器可以结合在一起，以及如何结合的方法。目前集成学习主要有$bagging$，$boosting$，$stacking$三种：\n- $bagging$：对训练集进行随机子抽样，对每个子训练集构建基模型，对所有的基模型的预测结果进行综合产生最后的预测结果。如果是分类算法，则用多数投票法确定最终类别，如果是回归算法，则将各个回归结果做算术平均作为最终的预测值。常用的$bagging$算法：随机森林\n- $boosting$：训练过程为阶梯状，基模型按照次序进行训练（实际上可以做到并行处理），先给定一个初始训练数据，训练出第一个基模型，根据基模型的表现对样本进行调整，在之前基模型预测错误的样本上投入更多的关注，然后用调整后的样本训练下一个基模型，重复上述过程N次，将N个基模型进行加权结合，输出最后的结果。常用的算法有GBDT，XGBOOST等。\n- $stacking$：是一种组合分类器的方法，以两层为例，第一层由多个基学习器组成，其输入为原始训练集，第二层的模型则是以第一层基学习器的输出作为训练集进行再训练(一般用LR进行回归组合），从而得到完整的$stacking$模型。要得到$stacking$模型，关键在于如何构造第二层的特征，构造第二层特征的原则是尽可能的避免信息泄露，因此对原始训练集常常采用类似于K折交叉验证的划分方法。各个基模型要采用相同的$Kfold$，这样得到的第二层特征的每一折（对应于之前的K折划分）都将不会泄露进该折数据的目标值信息 ，从而尽可能的降低过拟合的风险。\n\n**3.2: 简单描述一下模型的偏差和方差？bagging和boosting主要关注哪个？**\n\n- 偏差描述的是预测值与真实值的差距，偏差越大，越偏离真实数据。\n- 方差描述的是预测值的变化范围，离散程度，方差越大，数据分布越分散。\n- Bagging算法是对训练样本进行采样，产生出若干不同的子集，再从每个数据子集中训练出一个分类器，取这些分类器的平均，所以是降低模型的方差（variance）。Bagging算法和Random Forest这种并行算法都有这个效果。Boosting则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代不断进行，误差会越来越小，所以模型的偏差（bias）会不断降低。\n\n\n**3.3：简述一下随机森林的原理，随机森林的构造过程。**\n\n随机森林是$bagging$算法的代表，使用了CART树作为弱分类器，将多个不同的决策树进行组合，利用这种组合来降低单棵决策树的可能带来的片面性和判断不准确性。对于普通的决策树，是在所有样本特征中找一个最优特征来做决策树的左右子树划分，而随机森林会先通过自助采样的方法（bootstrap）得到$N$个训练集，然后在单个训练集上会随机选择一部分特征，来选择一个最优特征来做决策树的左右子树划分，最后得到N棵决策树，对于分类问题，按多数投票的准则确定最终结果，对于回归问题，由多棵决策树的预测值的平均数作为最终结果。随机森林的随机性体现在两方面，一个是选取样本的随机性，一个是选取特征的随机性，这样进一步增强了模型的泛化能力。\n\n**3.4：随机森林的优缺点？**\n\n优点：\n\n- 训练可以高度并行化，训练速度快，效率高。\n- 两个随机性的引入，使得随机森林不容易过拟合，具有很好的抗噪声能力。\n- 由于每次不再考虑全部的特征属性，二是特征的一个子集，所以相对于bagging计算开销更小，效率更高。\n- 对于数据的适应能力强，可以处理连续型和离散型的变量，数据无需规范化。\n- 可以输出变量的重要程度，被认为是一种不错的降维方法。\n\n缺点：\n\n- 在某些噪声较大的分类问题和或回归问题上容易过拟合。\n- 模型的可解释性比较差，无法控制模型内部的运行。\n- 对于小数据或者低维数据，效果可能会不太好。\n\n**3.5：随机森林为什么不容易过拟合？**\n\n随机森林由很多棵树组合在一起，单看每一棵树可以是过拟合的，但是既然是过拟合，就会拟合到非常小的细节，随机森林通过引入随机性，让每一棵树过拟合的细节不同，再将这些树组合在一起，过拟合的部分就会抵消掉，不过随机森林还是可能会出现过拟合的现象，只是出现的概率相对较低。\n\n**3.6：随机森林输出特征重要性的原理？**\n\n- 随机森林对于特征重要性的评估思想：判断每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。其中关于贡献的计算方式可以是基尼指数或袋外数据错误率。\n- 基于基尼系数：如果特征X出现在决策树J中的结点M，则计算节点M分枝前后的Gini指数变化量，假设随机森林由N棵树，则计算N次的Gini系数，最后将所有的Gini系数做一个归一化处理就得到了该特征的重要性。\n- 基于袋外数据错误率：袋外数据指的是每次随机抽取未被抽取达到的数据，假设袋外的样本数为O，将这O个数据作为测试集，代入已生成好的随机森林分类器，得到预测的分类结果，其中预测错误的样本数为X，则袋外数据误差为X/O，这个袋外数据误差记为errOOB1，下一步对袋外数据的特征A加入噪声干扰，再次计算袋外误差errOOB2，假设随机森林由N个分类器，则特征A的重要性为：sum(errOOB2-errOOB1)/N,其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征A不重要。\n\n**3.7：简单描述一下Adaboost的算法原理和流程。**\n\n- Adaboost基于分类器的错误率分配不同的权重系数，最后得到累加加权的的预测结果。\n\n算法流程：\n\n- 给数据中每一个样本一个权重，若有N个样本，则每个样本的权重为1/N.\n- 训练数据的每一个样本，得到第一个分类器。\n- 计算该分类器的错误率，根据错误率计算给分类器分配的权重。\n- 将第一个分类器分错的样本权重增加，分对的样本权重减少，然后再用新的样本权重训练数据，得到新的分类器。\n- 迭代这个训练步骤直到分类器错误为0或达到迭代次数。\n- 将所有的弱分类器加权求和，得到分类结果（分类器权重），错误率低的分类器获得更高的决定系数，从而在数据进行预测起关键作用。\n\n**3.8：Adaboost的优点和缺点？**\n\n优点：\n\n- 分类精度高，构造简单，结果可理解。\n- 可以使用各种回归分类模型来构建弱学习器，非常灵活。\n- 不容易过拟合。\n\n缺点：\n\n- 训练时会过于偏向分类困难的数据，导致Adaboost容易受噪声数据干扰。\n- 依赖于弱分类器，训练时间可能比较长。\n\n**3.9：简单说一下GBDT的原理。**\n\n- GBDT是boosting的一种方法，主要思想是每一次建立单个分类器时，是在之前建立的模型的损失函数的梯度下降方向。损失函数越大，说明模型越容易出错，如果我们的模型能让损失函数持续的下降，则说明我们的模型在持续不断的改进，而最好的方式就是让损失函数在其梯度的方向上下降。\n- GBDT的核心在于每一棵树学的是之前所有树结论和的残差，残差就是真实值与预测值的差值，所以为了得到残差，GBDT中的树全部是回归树，之所以不用分类树，是因为分类的结果相减是没有意义的。\n- Shrinkage（缩减）是 GBDT 的一个重要演进分支，Shrinkage的思想在于每次走一小步来逼近真实的结果，要比直接迈一大步的方式更好，这样做可以有效减少过拟合的风险。它认为每棵树只学到了一小部分，累加的时候只累加这一小部分，通过多学习几棵树来弥补不足。这累加的一小部分（步长*残差）来逐步逼近目标，所以各个树的残差是渐变的而不是陡变的。\n- GBDT可以用于回归问题（线性和非线性），也可用于分类问题。\n\n**3.10：为什么对于高维稀疏特征不太适合用GBDT？**\n\n- GBDT在每一次分割时需要比较大量的特征，特征太多，模型训练很耗费时间。\n- 树的分割往往只考虑了少部分特征，大部分的特征都用不到，所有的高维稀疏的特征会造成大量的特征浪费。\n\n**3.11：GBDT和随机森林的异同点？**\n\n相同点：\n\n- 都是由多棵树构成，最终的结果也是由多棵树决定。\n\n不同点：\n\n- 随机森林可以由分类树和回归树组成，GBDT只能由回归树组成。\n- 随机森林的树可以并行生成，而GBDT只能串行生成，所以随机森林的训练速度相对较快。\n- 随机森林关注减小模型的方差，GBDT关注减小模型的偏差。\n- 随机森林对异常值不敏感，GBDT对异常值非常敏感。\n- 随机森林最终的结果是多数投票或简单平均，而GBDT是加权累计起来。\n\n**3.12：GBDT的优缺点？**\n\n优点：\n\n- GBDT每一次的残差计算都增大了分错样本的权重，而分对的权重都趋近于0，因此泛化性能比较好。\n- 可以灵活的处理各种类型的数据。\n\n缺点：\n\n- 对异常值比较敏感。\n- 由于分类器之间存在依赖关系，所以很难进行并行计算。\n\n**3.13：XGBOOST和GBDT的区别在哪里？**\n\n- 传统的GBDT是以CART树作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题），线性分类器的速度是比较快的，这时候xgboost的速度优势就体现了出来。\n- 传统的GBDT在优化时只使用一阶导数，而xgboost对损失函数做了二阶泰勒展开，同时用到了一阶和二阶导数，并且xgboost支持使用自定义损失函数，只要损失函数可一阶，二阶求导。\n- xgboost在损失函数里加入了正则项，用来减小模型的方差，防止过拟合，正则项里包含了树的叶节点的个数， 每个叶子节点上输出的score的L2模的平方和。\n- xgboost里有一个参数叫学习速率（learning_rate）， xgboost在进行完一次迭代后，会将叶子节点的权重乘上学习速率，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把learing_rate设置得小一点，然后迭代次数(n_estimators)设置得大一点。\n- xgboost借鉴了随机森林的原理，支持行抽样(subsample)和列抽样(colsample_bytree,colsample_bylevel)， 行抽样指的是随机森林里对数据集进行有放回抽样，列抽样指的是对特征进行随机选择，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。\n\n**3.14：为什么XGBOOST要用泰勒展开，优势在哪里？**\n\nxgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，仅仅依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法的优化分开来了，这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可以用于分类，也可以用于回归。\n\n**3.15：XGBOOST是如何寻找最优特征的？**\n\nxgboost在训练过程中给出各个特征的增益评分，最大增益的特征会被选出来作为分裂依据，从而记忆了每个特征在模型训练时的重要性，从根到叶子中间节点涉及某特征的次数作为该特征重要性排序。\n\n**3.16：XGBOOST是如何处理缺失值的？**\n\nxgboost为缺失值设定了默认的分裂方向，xgboost在树的构建过程中选择能够最小化训练误差的方向作为默认的分裂方向，即在训练时将缺失值划入左子树计算训练误差，再划入右子树计算训练误差，然后将缺失值划入误差小的方向。\n\n**3.17：XGBOOST的并行化是如何实现的？**\n\n- xgboost的并行不是在tree粒度上的并行，xgboost也是一次迭代完才能进行下一次迭代（第t次迭代的损失函数包含了第t-1次迭代的预测值），它的并行处理是在特征粒度上的，在决策树的学习中首先要对特征的值进行排序，然后找出最佳的分割点，xgboost在训练之前，就预先对数据做了排序， 然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。\n- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。\n\n**3.18：XGBOOST采样时有放回的还是无放回的？**\n\nxgboost属于boosting方法的一种，所以采样时样本是不放回的，因而每轮计算样本不重复，另外，xgboost支持子采样，每轮计算可以不使用全部的样本，以减少过拟合。另外一点是xgboost还支持列采样，每轮计算按百分比随机抽取一部分特征进行训练，既可以提高速度又能减少过拟合。\n\n**3.19：XGBOOST的调参步骤是怎样的？**\n\nPS：这里使用Gridsearch cv来穷举检索最佳的参数，如果时间允许，可以通过设置步数先粗调，再细调。\n\n- 保持learning rate和其他booster相关的参数不变，调节和estimators的参数。learing_rate可设为0.1, max_depth设为4-6之间，min_child_weight设为1，subsample和colsample_bytree设为0.8 ，其他的参数都设为默认值即可。\n- 调节max_depth 和 min_child_weight参数，首先，我们先大范围地粗调参数，然后再小范围地微调。\n- gamma参数调优\n- subsample和colsample_bytree 调优\n- 正则化参数调优，选择L1正则化或者L2正则化\n- 缩小learning rate，得到最佳的learning rate值\n\n**3.20：XGBOOST特征重要性的输出原理？**\n\nxgboost是用get_score方法输出特征重要性的，其中importance_type*参数*支持三种特征重要性的计算方法：\n\n- importance_type*=*weight（默认值），使用特征在所有树中作为划分属性的次数。\n- importance_type*=*gain，使用特征在作为划分属性时loss平均的降低量。\n- importance_type*=*cover，使用特征在作为划分属性时对样本的覆盖度。\n\n**3.21：LightGBM相比XGBOOST在原理和性能上的差异？**\n\n1.速度和内存上的优化：\n\n- xgboost用的是预排序（pre-sorted）的方法， 空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。\n- LightGBM用的是直方图（Histogram）的决策树算法，直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。\n\n2.准确率上的优化：\n\n- xgboost 通过level（depth）-wise策略生长树， Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n- LightGBM通过leaf-wise（best-first）策略来生长树， Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。\n\n3.对类别型特征的处理**：**\n\n- xgboost不支持直接导入类别型变量，需要预先对类别型变量作亚编码等处理。如果类别型特征较多，会导致哑变量处理后衍生后的特征过多，学习树会生长的非常不平衡，并且需要非常深的深度才能来达到较好的准确率。\n- LightGBM可以支持直接导入类别型变量（导入前需要将字符型转为整数型，并且需要声明类别型特征的字段名），它没有对类别型特征进行独热编码，因此速度比独热编码快得多。LightGBM使用了一个特殊的算法来确定属性特征的分割值。基本思想是对类别按照与目标标签的相关性进行重排序，具体一点是对于保存了类别特征的直方图根据其累计值(sum_gradient/sum_hessian)重排序,在排序好的直方图上选取最佳切分位置。\n\n\n\n## **二. 特征工程**\n\n**4.1：什么是特征工程？为什么特征工程对机器学习很重要？**\n\n- 特征工程指的是使用专业知识和技巧来处理数据，使得特征在机器学习算法上发挥更好的作用的过程。这个过程包含了数据预处理，特征构建，特征筛选等。特征工程的目的就是筛选出好的特征，得到更好的训练数据，使模型达到更好的效果。\n- 从数据中提取出来的特征好坏会直接影响到模型的效果，有的时候，如果特征工程做得好，仅使用一些简单的机器学习算法，也能达到很好的效果。由此可见特征工程在实际的机器学习中的重要性。\n\n\n\n**4.2：特征工程的一般步骤是什么？什么是特征工程的迭代？**\n\n特征工程常规步骤：\n\n- 数据获取，数据的可用性评估（覆盖率，准确率，获取难度）\n- 探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。\n- 特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。\n- 特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，四则运算，基于业务理解进行头脑风暴构建特征等。\n- 特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有过滤法，包装法，嵌入法。\n\n特征工程的迭代:\n\n- 选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。\n- 设计特征：可以自动进行特征提取工作，也可以手工进行特征的构建。\n- 选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。\n- 计算模型：计算模型在该特征上所提升的准确率。\n- 上线测试：通过在线测试的效果来评估特征是否有效。\n\n\n\n**4.3：常用的特征工程方法有哪些？**\n\n- 特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。\n- 特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。\n- 特征筛选：过滤法，封装法，嵌入法。\n\n\n\n**4.4：在实际的风控建模中怎么做好特征工程？**\n\n本人工作中的一些经验总结：\n\n- 因为做风控模型大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。\n- 基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。\n- 如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上多下功夫，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。\n\n\n\n**4.5：实际项目中原始数据通常有哪些问题？你是如何解决的？**\n\n- 一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。\n- 数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。\n- 有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。\n- 大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成维度灾难，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。\n\n\n\n**4.6：在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？**\n\n- 特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。\n- 特征与目标变量是高度相关的，衡量的指标主要是IV。\n- 特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。\n- 特征的覆盖率，一般来说覆盖率要达到70%以上。\n- 特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。\n- 特征的及时性，最好是能代表用户最近的信用风险情况。\n\n\n\n**4.7：为什么探索性数据分析（EDA）在机器学习中非常重要？**\n\n- EDA不单是看看数据的分布，而是对数据整体有一个大概的了解。通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律。从中发现关键性的价值信息，这些信息对于后续建模及对模型的正确理解有很重要的意义。\n- 通过EDA可以发现数据的异常，可以分析每个特征与目标变量之间的关系，特征与特征之间的关系，为特征构建和特征筛选提供有价值的信息。\n- EDA分析可以验证数据是不是你认为的那样，实际情况中由于数据和特征量比较大，往往忽视这些数据是如何生成的，数据突出的问题或模型的实施中的错误会被长时间忽视，这可能会导致基于错误信息做出决策。\n\n\n\n**4.8：缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？**\n\n- 首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。\n- 风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。\n\n\n\n**4.9：如何发现数据中的异常值？对异常值是怎么处理的？**\n\n- 一种是基于统计的异常点检测算法例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有欧式距离，绝对距离。\n- 对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。\n\n\n\n**4.10：对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？**\n\n- 时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。\n- 连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。\n- 离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。\n\n\n\n**4.11：如何处理样本不平衡的问题？**\n\n- 在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种抽样方法，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。\n- 确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。\n- 对数据集进行抽样，一种是进行欠采样，通过减少大类的数据样本来降低数据的不平衡，另一种是进行过采样，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。\n- 尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。\n- 尝试从新的角度来理解问题，可以把那些小类样本当做异常点，因此该分类问题转化为异常检测问题或变化趋势检测问题，这种方法笔者很少用到，就不详细说明了。\n\n\n\n**4.12：特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？**\n\n常规的特征衍生方法：\n\n- 基于对业务的深入理解，进行头脑风暴，构造特征。\n- 特征交叉，例如对类别特征进行交叉相乘。\n- 分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。\n- 重构数值量（单位转换，整数小数拆分，构造阶段性特征）\n- 特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。\n\n平时工作特征衍生的做法：\n\n- 因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个\"在敏感时间段（深夜）的通话次数占比\"，如果占比较高，用户的风险也较大。\n- 平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。\n- 对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。\n\n\n\n**4.13：特征筛选的作用和目的？筛选的特征需要满足什么要求？**\n\n作用和目的：\n\n- 简化模型，增加模型的可解释性， 降低模型过拟合的风险。\n- 缩短模型的训练时间。\n- 避免维度灾难。\n\n筛选特征满足的要求：\n\n- 具有良好的区分能力。\n- 可解释性好，与目标变量的关系在业务上能解释的通。\n- 在时间序列上有比较好的稳定性。\n- 特征的用户覆盖率符合要求。\n\n\n\n**4.14：特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？**\n\nFilter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n\n- 相关系数，方差（适用于连续型变量），卡方检验（适用于类别型变量），信息熵，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。\n- 优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。\n- 缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。\n\nWrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。\n\n- 方法有完全搜索（递归消除法），启发式搜索（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。\n- 优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。\n- 缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。\n\nEmbedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n\n- 一种是基于惩罚项，例如岭回归，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。\n- 优点：效果最好速度最快，模式单调，快速并且效果明显。\n- 缺点：如何参数设置， 需要对模型的算法原理有较好的理解。\n\n\n\n\n\n## **三.模型评估和优化**\n\n**5.1：简单介绍一下风控模型常用的评估指标。**\n\n- 混淆矩阵指标：精准率，查全率，假正率。当模型最后转化为规则时，一般用这三个指标来衡量规则的有效性。要么注重精准率，要么注重查全率，两者不可兼而得之。\n- ROC曲线和AUC值，ROC曲线是一种对于查全率和假正率的权衡，具体方法是在不同阈值下以查全率作为纵轴，假正率作为横轴绘制出一条曲线。曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。在对角线（随机线）左边的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。从另一个角度看，由于画ROC曲线时都是先将所有样本按分类器的预测概率排序，所以AUC反映的是分类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n- KS：用于区分预测正负样本分隔程度的评价指标，KS越大，表示模型能将好坏样本区分开的程度越大。KS的绘制方法是先将每个样本的预测结果化为概率或者分数，将最低分到最高分（分数越低，坏的概率越大）进行排序做样本划分，横轴就是样本的累计占比，纵轴则是好坏用户的累计占比分布曲线，KS值为两个分布的最大差值（绝对值）。KS值仅能代表模型的区隔能力，KS不是越高越好，KS如果过高，说明好坏样本分的过于开了，这样整体分数（概率）就是比较极端化的分布状态，这样的结果基本不能用。\n- 基尼系数：其横轴是根据分数（概率）由高到低累计的好用户占总的好用户的比例，纵轴是分数（概率）从高到低坏用户占总的坏用户的比例。由于分数高者为低风险用户，所以累计坏用户比例的增长速度会低于累计好用户比例，因此，基尼曲线会呈现向下弯曲的形式，向下突出的半月形的面积除以下方三角形的面积即是基尼系数。基尼系数越大，表示模型对于好坏用户的区分能力越好。\n\n\n\n**5.2：为什么ROC适合不平衡数据的评价？**\n\n- ROC曲线的纵轴是TPR= ![\\frac{TP}{TP+FN}](https://www.zhihu.com/equation?tex=%5Cfrac%7BTP%7D%7BTP%2BFN%7D) ，横轴是FPR= ![\\frac{FP}{FP+TN}](https://www.zhihu.com/equation?tex=%5Cfrac%7BFP%7D%7BFP%2BTN%7D) ，TPR聚焦于正例，FPR聚焦于与负例，所以ROC兼顾了正样本和负样本的权衡，使其成为一个比较均衡的评估方法。\n- 因为TPR用到的TP和FN都是正样本，FPR用到的FP和TN都是负样本，所以说正样本或负样本发生了改变，TPR和FPR也不会相互影响，因此即使类别分布发生了改变，数据变得不平衡了，ROC曲线也不会产生大的变化。ROC曲线的优点，即具有鲁棒性，在类别分布发生明显改变的情况下依然能客观地识别出较好的分类器。\n\n\n\n**5.3：AUC和KS的关系是什么？**\n\n![img](https://pic3.zhimg.com/v2-51c5764548effcfc1348c1390fb47a52_b.jpg) ![img](https://pic3.zhimg.com/80/v2-51c5764548effcfc1348c1390fb47a52_hd.jpg)\n\n- 左图是KS曲线，红色的是TPR曲线（累计正样本占比），蓝色的是FPR曲线（累计负样本占比）。由于按照正样本预测概率降序排列，所以排在前面的样本为正的概率更大，但为正的概率是递减的；相反排在前面的样本为负的概率更小，但为负的概率递增。所以KS图中，TPR曲线在FPR曲线上方，并且TPR曲线的导数递减，FPR曲线的导数递增，而KS曲线先上升到达峰值P点（导数为0）后下降，P点对应的C值就是KS值。ROC图中，ROC曲线的导数是递减的，且刚开始导数大于1，逐渐递减到导数为1的T点（T点对应P点），然后导数继续降低。另外，A值对应X值，B值对应Y值，且C=B-A=Y-X\n- 在用KS评估模型时，除了看P点对应的KS值C，还要看P点的横坐标F值的大小，F值表示的是将分数从低到高排序后的累计样本占比，F值越小，说明模型对正样本的预测越精确，也就是说在识别出正样本的同时也能保证对负样本更小的误杀率。\n- 假设F值不变，C值增大，即P点沿着垂直方向向上移动，那么A值应该减小，B值应该增大；对应地，X值减小，Y值增大，T点会向左上角移动；所以ROC曲线下方的面积会增大，也就是AUC值增大。\n- 假设C值不变，F值减小，即P点沿着水平方向向左移动，因为C=B-A，所以A和B减小相同的幅度，也是就说X和Y减小相同的幅度，即T点沿着斜率为1的切线方向向下移动，此时ROC曲线下方的面积也会增大，即AUC值增大。\n- 所以P点的位置决定了T点的位置，C值和F值均会影响AUC值。AUC值看上去更像一个综合评估指标，但缺乏对模型细节的评估。而KS值结合F值，可以评估每一段评分的效果，还可以找出评分切分的阈值等。\n\n\n\n**5.4：什么是模型的欠拟合和过拟合？**\n\n- 欠拟合指的是模型没有很好的捕捉到数据特征，不能很好的拟合数据。\n- 过拟合指的是模型把数据学习的太彻底，以至于把噪声数据学习进去了，这样模型在预测未知数据时，就不能正确的分类，模型的泛化能力太差。\n\n\n\n**5.5：如何判断模型是否存在过拟合或欠拟合？对应的解决方法有哪些？**\n\n- 判断模型是否存在过拟合/欠拟合主要用学习曲线，学习曲线指的是通过画出不同训练集大小时训练集和交叉验证的准确率，可以看到模型在新数据上的表现，进而来判断模型是否方差偏高（过拟合）或偏差过高（欠拟合）。当训练集和测试集的误差收敛但却很高时，即为欠拟合，当训练集和测试集的误差之间有大的差距时，为过拟合。\n- 解决欠拟合的方法：增加效果好的特征，添加多项式特征，减小正则化参数等。\n- 解决过拟合的方法：使用更多的数据，选择更加合适的模型，加入正则项等。\n\n\n\n**5.6：什么是正则化？什么是L1正则化和L2正则化？**\n\n- 正则化是在模型的loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项，它会向学习算法略微做些修正，从而让模型能更好地泛化。这样反过来能提高模型在不可见数据上的性能。\n- L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解，所以L1正则化会趋向于产生少量的特征。\n- L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），所以L2正则化会使特征的解趋近于0，但不会为0。\n\n\n\n**5.7：正则化为什么可以防止过拟合？**\n\n最简单的解释是正则化对模型参数添加了先验，在数据少的时候，先验知识可以防止过拟合。\n\n\n\n**5.8：什么是交叉验证？交叉验证的目的是什么？有哪些优点？**\n\n交叉验证概念：\n\n交叉验证，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓\"交叉\"。　\n\n交叉验证的目的：\n\n评估给定算法在特定数据集上训练后的泛化性能，比单次划分训练集和测试集的方法更加稳定，全面。\n\n交叉验证的优点：\n\n- 如果只是对数据随机划分为训练集和测试集，假如很幸运地将难以分类的样本划分进训练集中，则在测试集会得出一个很高的分数，但如果不够幸运地将难以分类的样本划分进测试集中，则会得到一个很低的分数。所以得出的结果随机性太大，不够具有代表性。而交叉验证中每个样本都会出现在训练集和测试集中各一次，因此，模型需要对所有样本的泛化能力都很好，才能使其最后交叉验证得分，及其平均值都很高，这样的结果更加稳定，全面，具有说服力。\n- 对数据集多次划分后，还可以通过每个样本的得分比较，来反映模型对于训练集选择的敏感性信息。\n- 对数据的使用更加高效，可以得到更为精确的模型。\n\n\n\n**5.8：交叉验证常用的方法有哪些？**\n\n- 标准K折交叉验证：K是自定义的数字，通常取5或10，如果设为5折，则会训练5个模型，得到5个精度值。\n- 分层K折交叉验证：如果一个数据集经过标准K折划分后，在测试集上只有一种类别，则无法给出分类器整体性能的信息，这种情况用标准K折是不合理的。而在分层K折交叉验证中，每个折中的类别比例与整个数据集类别比例相同，这样能对泛化性能做出更可靠的估计。\n- 留一法交叉验证：每次划分时，把单个数据点作为测试集，如果数据量小，能得到更好的估计结果，数据量很大时则不适用。\n- 打乱划分交叉验证：每次划分数据时为训练集取样train_size个点，为测试集取样test_size个点，将这一划分划分方法重复n_splits次。这种方法还允许每次迭代中使用部分数据，可通过设置train_size和test_size之和不为0来实现，用这种方法对数据进行二次采样可能对大型数据上的试验很用用。另外也有分层划分的形式（ StratifiedShuffleSplit），为分类任务提供更可靠的结果。\n- 分组交叉验证：适用于数据中的分组高度相关时，以group数组作为参数，group数组表示数据中的分组，在创建训练集和测试集的时候不应该将其分开，也不应该与类别标签弄混。\n\n\n\n## 参考文献\n\n1.   https://zhuanlan.zhihu.com/p/56175215 "},{"title":"神经网络公式推导","url":"/2020/06/29/神经网络公式推导/","content":"#### 神经网络的反向传播公式\n\n\n\n##### 1. 逻辑回归反向传播公式的推导\n\n逻辑回归是最简单的神经网络，先入手逻辑回归，有助于后面的理解。\n\n![](D:\\ml-md\\神经网络2\\逻辑回归反向传播.jpg)\n\n上图是一个逻辑回归正向传播的示意图。具体细节不再描述。\n\n损失函数为：\n$$\nL(a,y) = -yloga -(1-y)log(1-a)\n$$\n反向传播的目的为了求$dw和db$，从而采用梯度下降法进行迭代优化，那么反向传播就是从后向前一步步的求微分，从而得到$dw,db$,具体过程如下：\n\n1.  $da = \\frac{dL(a,y)}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n2.  $dz = \\frac{dL(a,y)}{dz} = \\frac{dL}{da} \\frac{da}{dz}= da.g^{'}(z) = a - y$，其中sigmoid函数导数计算公式$g^{'}(z) = g(z)(1-g(z))$\n3. $dw = dz.x$\n4. $db = dz$\n\n这样就完成了逻辑回归的反向传播\n\n\n\n##### 2. 单隐层神经网络的反向传播公式推导\n\n神经网络计算中，与逻辑回归十分相似，但中间会有多层计算。下图是一个双层神经网络，有一个输入层，一个隐藏层和一个输出层。 \n\n![](D:\\ml-md\\神经网络2\\二层神经网络反向传播.jpg)\n\n前向传播如图所示。其中$L(a^{[2]},y)$为交叉熵损失函数，假设有两个输出则：\n$$\nL(a^{[2]},y) = -yloga^{[2]} -(1-y)log(1-a^{[2]})\n$$\n反向传播公式如下：\n\n$da^{[2]} = \\frac{dL(a^{[2]},y)}{da} = -\\frac{y}{a^{[2]}} + \\frac{1-y}{1-a^{[2]}}$\n\n1.  $dz^{[2]} = \\frac{dL(a,y)}{dz^{[2]}} = \\frac{dL}{da^{[2]}} \\frac{da^{[2]}}{dz^{[2]}}= da^{[2]}.g^{'}(z^{[2]}) = a^{[2]} - y$，其中sigmoid函数导数计算公式$g^{'}(z) = g(z)(1-g(z))$\n2. $dw ^{[2]}= dz^{[2]}.(a^{[1]})^T$\n3. $db^{[2]} = dz^{[2]}$\n4. $dz^{[1]} = \\frac{dL}{da^{[1]}}\\frac{da^{[1]}}{dz^{[1]}}  = (w^{[2]})^Tdz^{[2]}*(g^{[1]})^{'}(z^{[1]})$\n5. $dw^{[1]} = dz^{[1]}.x^T$\n6. $db^{[1]} = dz^{[1]}$\n\n##### 3. 深层神经网络的前向和反向传播\n\n下面是一个四层的神经网络：\n\n![](D:\\ml-md\\神经网络2\\深层神经网络反向传播.jpg)\n\n前向传播过程：\n$$\n\\begin{aligned}\nz^{[l]} &= w^{[l]}a^{[l-1]} + b^{[l]} \\\\\na^{[l]} &= g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n与单隐层神经网络反向传播类似，我们可以直接写出深层神经网络的反向传播递推公式：\n$$\n\\begin{aligned}\n\\mathrm{d}z^{[l]} &= \\mathrm{d}a^{[l]} \\cdot g^{[l]'}(z^{[l]}) \\\\\n\\mathrm{d}w^{[l]} &= \\mathrm{d}z^{[l]} \\cdot a^{[l-1]} \\\\\n\\mathrm{d}b^{[l]} &= \\mathrm{d}z^{[l]} \\\\\n\\mathrm{d}a^{[l-1]} &= w^{[l]^T} \\cdot \\mathrm{d}z^{[l]}\n\\end{aligned}\n$$\n\n\n**NOTE:上面由于前向传播使用的$WX$,导致使用正常样本时，需要将$X$取转置设置，很是不爽，于是在下面给出$XW$的情形**。\n\n这里只关注单隐层的情形：\n\n输入层，隐藏层，输出层。输入层的节点数目由数据集的维度决定（我们的数据集是2：x和y），同样的，输出层的节点数目也取决于数据集中的类别(同样是2：0和1)。值得注意的是：2个类别可以用1个节点来表示，但考虑到网络的扩展性，我们将输出节点的数目定为2。网络的输入是(x,y)坐标，输出是0或者1。用图片表示的话网络结构如下：\n\n![](D:\\ml-md\\神经网络2\\nn-from-scratch-3-layer-network.png)\n\n隐藏层的维度（节点数）是可以设置的，节点越多，能够匹配的函数模型越复杂。但这也会耗费更多的计算资源，也会增加过拟合的可能性。隐藏层大小的设定更像是一门艺术，它需要根据问题的具体情况进行分析。稍后我们将分析隐藏层节点的个数是如何对我们的输出进行影响的。\n\n除此之外，我们还需要为隐藏层选择一个合适的激活函数。激活函数用来将该层的输入转化为输出。非线性的激活函数能够让我们做一些非线性的假设。常用的非线性激活函数有：$tanh/sigmoid/ReLU$等。本文中使用的激活函数是tanh（我个人建议用ReLU），这个激活函数的有效性也经过了很多方案的验证。一个好的激活函数具有以下性质：保证数据输入与输出也是可微的。比如说$dtanh(x)/dx = 1-（tanh(x)*tanh(x)）$。这样可以保证我们只计算一次tanh(x)的值就可以用在之后的计算导数过程中。\n\n为了使网络的输出是一个概率，所以输出层的激活函数就只能是softmax（逻辑回归只能输出二分类而softmax可以输出多个分类），这里用softmax的原因是它可以将分数转化为概率。\n\n\n**神经网络如何预测**\n\n我们设计的神经网络通过前向传播来进行预测，可以简单的将前向传播理解为一系列的矩阵乘法运算和使用激活函数的结果。如果输入$x$代表一个二维向量，那么我们计算输出$\\hat{y}$（同样也是二维）的方法如下：\n$$\n\\begin{aligned}\nz_1 &= xW_1 + b_1 \\\\\na_1 &= tanh(z_1) \\\\\nz_2 &= a_1W_2 + b_2 \\\\\na_2 &= \\hat{y} = softmax(z_2) \\\\\n\\end{aligned}\n$$\n$z_i$ 代表第$i$层的输入，$a_i$ 代表第$i$层应用激活函数后的输出 。$W_1,b_1,W_2,b_2$ 是网络的一些参数，具体的值需要通过训练数据来得到，你可以把他们看作层与层之间的矩阵变化数据。 矩阵的维度可以通过上述的矩阵乘法得到。举例说明：当我们使用的隐藏层有100个节点时，那么就有$W_1 \\in \\mathbb{R}^{2 \\times 100},b_1 \\in \\mathbb{R}^{100},W_2 \\in \\mathbb{R}^{100*2},b_2 \\in \\mathbb{R}^2$,这也可以解释为什么当节点数量增加时计算量也会随之增加。\n\n**参数的学习过程**\n\n参数学习是指神经网络寻找能使训练数据误差最小的$W_1, b_1, W_2, b_2$值的过程。我们把衡量误差的函数叫做损失函数。当输出通过$softmax$得到时，常用的损失函数分类交叉熵损失（也叫负对数似然函数）。如果有$N$组训练值和$C$个分类，我们的预测结果$\\hat{y}$与真实值$y$之间的损失定义为：\n$$\nL(y,\\hat{y}) = -\\frac{1}{N}\\sum_{n \\in N}\\sum_{i \\in C}y_{n,i}log\\hat{y}_{n,i}\n$$\n上面的式子并没有看起来那么复杂，它代表我们训练数据和预测错误时损失的累加和。$y 与 \\hat{y}$之间的差距越大，网络训练的损失也就越大。在参数学习过程中，训练损失越来越小，与训练数据的似然度也不断提高。\n\n在寻找最小值的过程中，我们可以使用梯度下降的方法。本文中实现了一种最普通的梯度下降方法，也叫固定学习率的批梯度下降方法。在实际应用中，SGD（随机梯度下降）或minibatch梯度下降法（还有Adam）可能会有更好的表现。如果你需要进一步的学习和研究\n\n\n\n梯度下降方法的输入是损失函数对于各项参数的梯度（向量的差分）：$\\frac{\\partial{L}}{\\partial{W_1}},\\frac{\\partial{L}}{\\partial{b_1}},\\frac{\\partial{L}}{\\partial{W_2}},\\frac{\\partial{L}}{\\partial{b_2}}$为了得到上述值，我们采用了著名的后向传播算法，这是一种根据输出计算梯度的有效算法。\n\n根据后向传播算法，我们可以得出以下结论：\n\n\n\n二分类的交叉熵损失函数为$L(a_2,y)$：\n$$\nL(a_2,y) = -yloga_2 -(1-y)log(1-a_2)\n$$\n反向传播公式如下：\n\n$da_2 = \\frac{dL(a_2,y)}{da} = -\\frac{y}{a_2} + \\frac{1-y}{1-a_2}$\n\n1.  $dz_2 = \\frac{dL}{da_2}\\frac{da_2}{dz_2} = a_2 - y$， 这里对应$z$ 对应$\\delta $,由于softmax的导数$a-(1-a_2)$\n\n2. $dW_2=  \\frac{\\partial L}{\\partial W_2}= a_1^Tdz_2$\n\n3. $db_2 =  \\frac{\\partial L}{\\partial b_2}=dz_2$\n\n4. $dz_1 = \\frac{dL}{da_1}\\frac{da_1}{dz_1}  =(g_1)^{'}(z_1) *dz_2W_2^T = (1- tanh^2z_1) * dz_2 W_2^T$\n\n5. $dW_1 = \\frac{\\partial L}{\\partial W_1} = x^Tdz_1$\n\n6. $db_1 = dz_1$\n\n   \n\n参考文献：\n\n1. 吴恩达 神经网络和深度学习\n2. https://blog.csdn.net/lst227405/article/details/56495625"},{"title":"leetcode算法总结系列1-堆","url":"/2020/06/29/leetcode算法总结系列1-堆/","content":"\n\n### Leetcode算法总结系列1-堆\n\n[TOC]\n\n##### 1.0 堆的介绍\n\n堆（Heap）是一个可以被看成近似完全二叉树的数组。树上的每一个结点对应数组的一个元素。除了最底层外，该树是完全充满的，而且是从左到右填充。—— 来自：《算法导论》\n\n堆包括最大堆和最小堆：最大堆的每一个节点（除了根结点）的值不大于其父节点；最小堆的每一个节点（除了根结点）的值不小于其父节点。\n\n堆常见的操作：\n\nHEAPIFY 建堆：把一个乱序的数组变成堆结构的数组，时间复杂度为 $O(n)$。\nHEAPPUSH：把一个数值放进已经是堆结构的数组中，并保持堆结构，时间复杂度为 $O(log\\ n)$。\nHEAPPOP：从最大堆中取出最大值或从最小堆中取出最小值，并将剩余的数组保持堆结构，时间复杂度为 $O(log\\ n)$。\nHEAPSORT：借由 HEAPFY 建堆和 HEAPPOP 堆数组进行排序，时间复杂度为 $O(n\\ log\\ n)$，空间复杂度为 $O(1)$。\n堆结构的一个常见应用是建立优先队列（Priority Queue）。\n\n##### 1.1 前K个高频单词\n\nid：692\n\nurl：https://leetcode-cn.com/problems/top-k-frequent-words/\n\n描述：\n\n给一非空的单词列表，返回前 k 个出现次数最多的单词。\n\n返回的答案应该按单词出现频率由高到低排序。如果不同的单词有相同出现频率，按字母顺序排序。\n\n示例 1：\n\n输入: [\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"], k = 2\n输出: [\"i\", \"love\"]\n解析: \"i\" 和 \"love\" 为出现次数最多的两个单词，均为2次。\n    注意，按字母顺序 \"i\" 在 \"love\" 之前。\n\n\n\n算法：\n\n1. 计算每个单词的频率，然后将其添加到存储到大小为 k 的小根堆中。它将频率最小的候选项放在堆的顶部。最后，我们从堆中弹出最多 k 次，并反转结果，就可以得到前 k 个高频单词。\n2. 在 Python 中，我们使用 heapq\\heapify，它可以在线性时间内将列表转换为堆，从而简化了我们的工作。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_692_topKFrequent {\n    public static List<String> topKFrequent(String[] words, int k){\n        Map<String,Integer> countMap = new HashMap<String, Integer>();\n        for(String word:words){\n            countMap.put(word,countMap.getOrDefault(word,0)+1);\n        }\n\n        PriorityQueue<String> queue = new PriorityQueue<String>(\n                (o1, o2) -> countMap.get(o1) == countMap.get(o2)?\n                        o2.compareTo(o1):countMap.get(o1) - countMap.get(o2)\n        );\n\n        for(String word:countMap.keySet()){\n            queue.offer(word);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<String> ans = new ArrayList<>();\n        while (!queue.isEmpty()){\n            ans.add(queue.poll());\n        }\n        Collections.reverse(ans);\n        return ans;\n\n    }\n\n    public static void main(String[] args) {\n        String[] words = {\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"};\n        topKFrequent(words,2);\n    }\n}\n\n```\n\n复杂度分析\n\n时间复杂度： $O(N \\log{k})$。其中 $N$ 是 words 的长度。我们用 $O(N)$的时间计算每个单词的频率，然后将 $N$ 个单词添加到堆中，添加每个单词的时间为 $O(\\log {k})$ 。最后，我们从堆中弹出最多 $k$ 次。因为 $k \\leq N$ 的值，总共是 $O(N \\log{k})$\n空间复杂度：$O(N)$,用于存储我们计数的空间\n\n\n\n```python\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, words: List[str], k: int) -> List[str]:\n        count = Counter(words)\n        return heapq.nsmallest(k,count,lambda i: (-count[i], i))\n```\n\n\n\n##### 1.2 [前 K 个高频元素](https://leetcode-cn.com/problems/top-k-frequent-elements/)\n\nid:347\n\n给定一个非空的整数数组，返回其中出现频率前 **k** 高的元素。\n\n**示例 1:**\n\n```text\n输入: nums = [1,1,1,2,2,3], k = 2\n输出: [1,2]\n```\n\n- 你可以假设给定的 *k* 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。\n- 你的算法的时间复杂度**必须**优于 O(*n* log *n*) , *n* 是数组的大小。\n\n\n\n```java\nclass Solution {\n    public List<Integer> topKFrequent(int[] nums, int k) {\n        Map<Integer,Integer> count = new HashMap<>();\n        for(int n:nums){\n            count.put(n,count.getOrDefault(n,0) + 1);\n        }\n\n        PriorityQueue<Integer> queue = new PriorityQueue<>((o1,o2) -> count.get(o1) - count.get(o2));\n\n        for(Integer i: count.keySet()){\n            queue.add(i);\n            while (queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<Integer> res = new ArrayList<>();\n        while (!queue.isEmpty()){\n            res.add(queue.poll());\n        }\n        Collections.reverse(res);\n        return res;\n    }\n}\n```\n\n```python\nfrom typing import List\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        count = Counter(nums)\n        return heapq.nlargest(k, count.keys(), count.get)\n```\n\n##### 1.3 [ 数组中的第K个最大元素](https://leetcode-cn.com/problems/kth-largest-element-in-an-array/)\n\nid:215\n\n在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。\n\n示例 1:\n\n输入: [3,2,1,5,6,4] 和 k = 2\n输出: 5\n\n```java\nclass Solution {\n    public int findKthLargest(int[] nums, int k) {\n        PriorityQueue<Integer> queue = new PriorityQueue<>(Comparator.comparingInt(n -> n));\n\n        for(int n:nums){\n            queue.add(n);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n        return queue.poll();\n    }\n}\n```\n\n\n\n```python\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        return heapq.nlargest(k,nums)[-1]\n```\n\n##### 1.4  [面试题40. 最小的k个数](https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/)\n\n输入整数数组 arr ，找出其中最小的 k 个数。例如，输入4、5、1、6、2、7、3、8这8个数字，则最小的4个数字是1、2、3、4。\n\n \n\n示例 1：\n\n输入：arr = [3,2,1], k = 2\n输出：[1,2] 或者 [2,1]\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_40_mian_getLeastNumbers {\n\n    public static int[]  getLeastNumbers(int[] arr, int k) {\n        if(k == 0) return new int[0];\n        PriorityQueue<Integer> priorityQueue = new PriorityQueue<>((a, b) -> (b -a));\n        for(int i: arr){\n            if(priorityQueue.size() < k){\n                priorityQueue.add(i);\n            }else{\n                if(priorityQueue.peek() > i){\n                    priorityQueue.remove();\n                    priorityQueue.add(i);\n                }\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (priorityQueue.size() > 0){\n            res[count++] = priorityQueue.remove();\n        }\n        return res;\n    }\n\n    public static int[] getLeastNumbers2(int[] arr, int k) {\n        if(k == 0 || arr.length == 0){\n            return new int[0];\n        }\n        PriorityQueue<Integer> heap = new PriorityQueue<>((o1, o2) -> o2 - o1);\n\n        for(int i:arr){\n            heap.offer(i);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (!heap.isEmpty()){\n            res[count++] = heap.poll();\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {3,2,1};\n        System.out.println(Arrays.toString(getLeastNumbers2(arr,2)));\n        System.out.println(Arrays.toString(getLeastNumbers(arr,2)));\n    }\n}\n\n```\n\n```scala\n    def getLeastNumbers(arr: Array[Int], k: Int): Array[Int] = {\n        val heap = scala.collection.mutable.PriorityQueue.empty[Int].reverse\n        arr.foreach(x => heap.enqueue(x))\n        (0 until k).map(_ => heap.dequeue()).toArray\n    }\n```\n\n##### 1.5 [最接近原点的 K 个点](https://leetcode-cn.com/problems/k-closest-points-to-origin/)\n\nid：973\n\n我们有一个由平面上的点组成的列表 `points`。需要从中找出 `K` 个距离原点 `(0, 0)` 最近的点。\n\n（这里，平面上两点之间的距离是欧几里德距离。）\n\n你可以按任何顺序返回答案。除了点坐标的顺序之外，答案确保是唯一的。\n\n \n\n**示例 1：**\n\n```\n输入：points = [[1,3],[-2,2]], K = 1\n输出：[[-2,2]]\n解释： \n(1, 3) 和原点之间的距离为 sqrt(10)，\n(-2, 2) 和原点之间的距离为 sqrt(8)，\n由于 sqrt(8) < sqrt(10)，(-2, 2) 离原点更近。\n我们只需要距离原点最近的 K = 1 个点，所以答案就是 [[-2,2]]。\n```\n\n二、大根堆(前 K 小) / 小根堆（前 K 大),Java中有现成的 PriorityQueue，实现起来最简单：$O(NlogK)$\n本题是求前 K 小，因此用一个容量为 K 的大根堆，每次 poll 出最大的数，那堆中保留的就是前 K 小啦（注意不是小根堆！小根堆的话需要把全部的元素都入堆，那是 O(NlogN)O(NlogN)😂，就不是 O(NlogK)O(NlogK)啦～～）\n这个方法比快排慢，但是因为 Java 中提供了现成的 PriorityQueue（默认小根堆），所以实现起来最简单，没几行代码\n\n```java\n\npackage com.strings.leetcode.heap;\n\nimport scala.actors.threadpool.Arrays;\n\nimport java.util.PriorityQueue;\n\npublic class Problem_973j_kClosest {\n    public static int[][] kClosest(int[][] points, int K) {\n        if(K == 0 || points.length == 0){\n            return new int[0][0];\n        }\n        PriorityQueue<int[]> heap = new PriorityQueue<>((o1, o2) ->\n                o2[0]*o2[0] + o2[1]*o2[1] - o1[0]*o1[0] - o1[1]*o1[1]\n        );\n\n        for(int[] p:points){\n            heap.offer(p);\n            if(heap.size() > K){\n                heap.poll();\n            }\n        }\n        int[][] res = new int[K][2];\n        int count = 0;\n        for(int[] i:heap){\n            res[count++] = i;\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[][] points = {{1,3},{-2,2}};\n        int K = 1;\n        System.out.println(Arrays.deepToString(kClosest(points,1)));\n    }\n}\n```\n\n```python\nclass Solution:\n    def kClosest(self, points: List[List[int]], K: int) -> List[List[int]]:\n        return heapq.nsmallest(K, points, key = lambda point: point[0] ** 2 + point[1] ** 2 )\n```\n\n```scala\nobject Solution {\n  def diff(arr: Array[Int]) = -arr(0)*arr(0) - arr(1)*arr(1)\n  def kClosest(points: Array[Array[Int]], K: Int): Array[Array[Int]] = {\n    val heap = scala.collection.mutable.PriorityQueue[Array[Int]]()(Ordering.by(diff))\n    points.foreach(x => heap.enqueue(x))\n    (0 until K).map(_ => heap.dequeue()).toArray\n  }\n}\n```\n\n\n\n##### 1.6 [查找和最小的K对数字](https://leetcode-cn.com/problems/find-k-pairs-with-smallest-sums/)\n\nid：373.\n\n给定两个以升序排列的整形数组 **nums1** 和 **nums2**, 以及一个整数 **k**。\n\n定义一对值 **(u,v)**，其中第一个元素来自 **nums1**，第二个元素来自 **nums2**。\n\n找到和最小的 k 对数字 **(u1,v1), (u2,v2) ... (uk,vk)**。\n\n**示例 1:**\n\n```\n输入: nums1 = [1,7,11], nums2 = [2,4,6], k = 3\n输出: [1,2],[1,4],[1,6]\n解释: 返回序列中的前 3 对数：\n     [1,2],[1,4],[1,6],[7,2],[7,4],[11,2],[7,6],[11,4],[11,6]\n```\n\n```scala\nclass Solution {\n    public List<List<Integer>> kSmallestPairs(int[] nums1, int[] nums2, int k) {\n      List<List<Integer>> pair = new ArrayList<>();\n        for (int i = 0; i < nums1.length ; i++) {\n            for (int j = 0; j < nums2.length; j++) {\n                List<Integer> res = new ArrayList<>();\n                res.add(nums1[i]);\n                res.add(nums2[j]);\n                pair.add(res);\n            }\n        }\n\n        PriorityQueue<List<Integer>> heap = new PriorityQueue<>((o1, o2) ->\n                o2.get(0) + o2.get(1) - o1.get(0) - o1.get(1)\n                );\n\n        for(List<Integer> lst:pair){\n            heap.offer(lst);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n        List<List<Integer>> ans = new ArrayList<>();\n        if(k==0 || nums1.length==0 || nums2.length == 0){\n            return ans;\n        }\n        for(List<Integer> lst:heap){\n            ans.add(lst);\n        }\n        return ans;\n\n    }\n}\n```\n\n```scala\nimport scala.collection.mutable.ArrayBuffer\n\nobject Solution {\n    def diff(num: List[Int]) = {-num(0) - num(1)}\n    def kSmallestPairs(nums1: Array[Int], nums2: Array[Int], k: Int): List[List[Int]] = {\n        if(k==0||nums1.length== 0){ List()}\n        else{\n        val pair:ArrayBuffer[List[Int]] = new ArrayBuffer[List[Int]]()\n        for(i <- 0 until nums1.length){\n            for(j <- 0 until nums2.length){\n                pair.append(List(nums1(i),nums2(j)))\n            }\n        }\n        val heap = scala.collection.mutable.PriorityQueue[List[Int]]()(Ordering.by(diff))\n        pair.toArray.foreach(x => heap.enqueue(x))\n        var tmpk= k\n        if(heap.size < k){\n            tmpk = heap.size\n        }\n        (0 until tmpk).map(_ => heap.dequeue()).toList\n        }\n\n    }\n}\n```\n\n##### 1.7  [数据流中的中位数](https://leetcode-cn.com/problems/shu-ju-liu-zhong-de-zhong-wei-shu-lcof/)\n\n###### [数据流的中位数](https://leetcode-cn.com/problems/find-median-from-data-stream/)\n\n###### [连续中值](https://leetcode-cn.com/problems/continuous-median-lcci/)\n\n这三个题都是一样的。\n\n将输入的数分成两部分：较小的一部分和较大的一部分\n\n1. lowPart ：定义为较小的一部分，用最大堆\n   允许lowPart的大小比highPart多1\n2. highPart ： 定义为较大的一部分，用最小堆\n   如果size是奇数，那么中位数就是lowPart的最大值，也就是堆顶\n\n否则，最大值是lowPart和highPart的堆顶平均值\n\n维护\n\n每进入一个数，先加入lowPart，然后将lowPart的最大值（堆顶）移出到highPart\n\n如果这时size是奇数，此时highPart将最小值移出到lowPart\n\n\n\n```java\nclass MedianFinder {\n\n        private PriorityQueue<Integer> lowPart;\n    private PriorityQueue<Integer> highPart;\n    int size;\n    /** initialize your data structure here. */\n    public MedianFinder() {\n        lowPart = new PriorityQueue<Integer>((x, y) -> y - x);  //最大堆\n        highPart = new PriorityQueue<Integer>();\n        size = 0;\n    }\n\n    public void addNum(int num) {\n        size++;\n        lowPart.offer(num);\n        highPart.offer(lowPart.poll());\n        if((size & 1) == 1){\n            lowPart.offer(highPart.poll());\n        }\n    }\n\n    public double findMedian() {\n        if((size & 1) == 1){\n            return (double) lowPart.peek();\n        }else{\n            return (double) (lowPart.peek() + highPart.peek()) / 2;\n        }\n    }\n}\n\n/**\n * Your MedianFinder object will be instantiated and called as such:\n * MedianFinder obj = new MedianFinder();\n * obj.addNum(num);\n * double param_2 = obj.findMedian();\n */\n```\n\n```python\nclass MedianFinder:\n\n    def __init__(self):\n        # 当前大顶堆和小顶堆的元素个数之和\n        self.count = 0\n        self.max_heap = []\n        self.min_heap = []\n\n    def addNum(self, num: int) -> None:\n        self.count += 1\n        # 因为 Python 中的堆默认是小顶堆，所以要传入一个 tuple，用于比较的元素需是相反数，\n        # 才能模拟出大顶堆的效果\n        heapq.heappush(self.max_heap, (-num, num))\n        _, max_heap_top = heapq.heappop(self.max_heap)\n        heapq.heappush(self.min_heap, max_heap_top)\n        if self.count & 1:\n            min_heap_top = heapq.heappop(self.min_heap)\n            heapq.heappush(self.max_heap, (-min_heap_top, min_heap_top))\n\n    def findMedian(self) -> float:\n        if self.count & 1:\n            # 如果两个堆合起来的元素个数是奇数，数据流的中位数大顶堆的堆顶元素\n            return self.max_heap[0][1]\n        else:\n            # 如果两个堆合起来的元素个数是偶数，数据流的中位数就是各自堆顶元素的平均值\n            return (self.min_heap[0] + self.max_heap[0][1]) / 2\n\n        \n\n\n# Your MedianFinder object will be instantiated and called as such:\n# obj = MedianFinder()\n# obj.addNum(num)\n# param_2 = obj.findMedian()\n```\n\n##### 1.8 [距离相等的条形码](https://leetcode-cn.com/problems/distant-barcodes/)\n\nid：1054\n\n在一个仓库里，有一排条形码，其中第 `i` 个条形码为 `barcodes[i]`。\n\n请你重新排列这些条形码，使其中两个相邻的条形码 **不能** 相等。 你可以返回任何满足该要求的答案，此题保证存在答案。\n\n \n\n**示例 1：**\n\n```\n输入：[1,1,1,2,2,2]\n输出：[2,1,2,1,2,1]\n```\n\n**示例 2：**\n\n```\n输入：[1,1,1,1,2,2,3,3]\n输出：[1,3,1,3,2,1,2,1]\n```\n\n贪心堆\n\n```java\npackage com.strings.leetcode.heap;\n\n\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\nclass Problem_1054j_rearrangeBarcodes {\n    public static int[] rearrangeBarcodes2(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2){\n            return barcodes;\n        }\n        Map<Integer,Integer> countMap = new HashMap<>();\n        for(int b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(int i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1){\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = countMap.get(a);\n            int freqB = countMap.get(b);\n            if(freqA > 1){\n                countMap.put(a,freqA-1);\n                maxHeap.offer(a);\n            }\n            if(freqB > 1){\n                countMap.put(b,freqB-1);\n                maxHeap.add(b);\n            }\n        }\n        if(maxHeap.size() > 0){\n            res[idx] = maxHeap.poll();\n        }\n\n        return res;\n    }\n\n    public static int[] rearrangeBarcodes(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2) return barcodes;\n        Map<Integer, Integer> map = new HashMap<>();\n        for(int i : barcodes) {\n            map.put(i, map.getOrDefault(i, 0) + 1);\n        }\n        //大顶堆\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> map.get(b) - map.get(a));\n        for(int i : map.keySet()) {\n            maxHeap.offer(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1) {\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = map.get(a);\n            int freqB = map.get(b);\n            if(freqA > 1) {\n                map.put(a, freqA - 1);\n                maxHeap.add(a);\n            }\n            if(freqB > 1) {\n                map.put(b, freqB - 1);\n                maxHeap.add(b);\n            }\n        }\n        //收尾\n        if(maxHeap.size() > 0) res[idx] = maxHeap.poll();\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] bor = {1,1,2};\n        System.out.println(Arrays.toString(rearrangeBarcodes(bor)));\n        System.out.println(Arrays.toString(rearrangeBarcodes2(bor)));\n    }\n}\n\n```\n\n##### 1.9 [ 重构字符串](https://leetcode-cn.com/problems/reorganize-string/)\n\n给定一个字符串S，检查是否能重新排布其中的字母，使得两相邻的字符不同。\n\n若可行，输出任意可行的结果。若不可行，返回空字符串。\n\n示例 1:\n\n输入: S = \"aab\"\n输出: \"aba\"\n示例 2:\n\n输入: S = \"aaab\"\n输出: \"\"\n\n\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\npublic class Problem_767_reorganizeString {\n\n    public static String reorganizeString(String S) {\n        if(S == null || S.length() < 2){\n            return S;\n        }\n        char[] barcodes = S.toCharArray();\n            Map<Character,Integer> countMap = new HashMap<>();\n            for(char b:barcodes){\n                countMap.put(b,countMap.getOrDefault(b,0)+1);\n            }\n            if(Collections.max(countMap.values()) > (S.length()+1)/2){\n                return \"\";\n            }\n            PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n            for(char i:countMap.keySet()){\n                maxHeap.add(i);\n            }\n            char[] res = new char[barcodes.length];\n            int idx = 0;\n            while(maxHeap.size() > 1){\n                char a = maxHeap.poll();\n                char b = maxHeap.poll();\n                res[idx++] = a;\n                res[idx++] = b;\n                int freqA = countMap.get(a);\n                int freqB = countMap.get(b);\n                if(freqA > 1){\n                    countMap.put(a,freqA-1);\n                    maxHeap.offer(a);\n                }\n                if(freqB > 1){\n                    countMap.put(b,freqB-1);\n                    maxHeap.add(b);\n                }\n            }\n            if(maxHeap.size() > 0){\n                res[idx] = maxHeap.poll();\n            }\n            return String.valueOf(res);\n    }\n\n    public static void main(String[] args) {\n        String S1 = \"aab\";\n        String S2 = \"aaab\";\n\n        System.out.println(reorganizeString(S1));\n        System.out.println(reorganizeString(S2));\n    }\n}\n\n```\n\n##### 1.10 [有序矩阵中第K小的元素](https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/)\n\nid:378\n\n给定一个 n x n 矩阵，其中每行和每列元素均按升序排序，找到矩阵中第k小的元素。\n请注意，它是排序后的第 k 小元素，而不是第 k 个不同的元素。\n\n \n\n示例:\n\nmatrix = [\n   [ 1,  5,  9],\n   [10, 11, 13],\n   [12, 13, 15]\n],\nk = 8,\n\n返回 13。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Comparator;\nimport java.util.PriorityQueue;\n\npublic class Problem_378_kthSmallest {\n    public static int kthSmallest(int[][] matrix, int k) {\n        PriorityQueue<Integer> heap = new PriorityQueue<>(Comparator.reverseOrder());\n        for(int[] arr:matrix){\n            for(int i: arr){\n                heap.offer(i);\n                if(heap.size() > k){\n                    heap.poll();\n                }\n            }\n        }\n        return heap.peek();\n    }\n\n    public static void main(String[] args) {\n        int[][] matrix = {{1,  5,  9},{10, 11, 13},{12, 13, 15}};\n        System.out.println(kthSmallest(matrix,8));\n    }\n\n}\n```\n\n##### 1.11 [根据字符出现频率排序](https://leetcode-cn.com/problems/sort-characters-by-frequency/)\n\nid:451\n\n给定一个字符串，请将字符串里的字符按照出现的频率降序排列。\n\n示例 1:\n\n输入:\n\"tree\"\n\n输出:\n\"eert\"\n\n解释:\n'e'出现两次，'r'和't'都只出现一次。\n因此'e'必须出现在'r'和't'之前。此外，\"eetr\"也是一个有效的答案。\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_451j_frequencySort {\n\n    public static String frequencySort(String s){\n        if(s == null || s.length() < 2){\n            return s;\n        }\n        char[] barcodes = s.toCharArray();\n        Map<Character,Integer> countMap = new HashMap<>();\n        for(char b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n\n        PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(char i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n\n        StringBuilder sb = new StringBuilder();\n        while (!maxHeap.isEmpty()){  // 注意不能使用for(char c:maxHeap)\n            char a = maxHeap.poll();\n            int freq = countMap.get(a);\n            for (int i = 0; i < freq; i++) {\n                sb.append(a);\n            }\n        }\n\n        return sb.toString();\n    }\n\n    public static void main(String[] args) {\n        String s  = \"Aabb\";\n        System.out.println(frequencySort(s));\n    }\n}\n\n```\n\n"},{"title":"聚类算法总结","url":"/2020/05/19/聚类算法总结/","content":"### 聚类算法总结\n\n聚类是一种经典的无监督学习方法，无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。\n\n聚类直观上来说是将相似的样本聚在一起，从而形成一个类簇（cluster）。那首先的问题是如何来度量相似性（similarity measure）呢？这便是距离度量，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是性能度量，性能度量为评价聚类结果的好坏提供了一系列有效性指标。\n\n#### 1 Kmeans聚类\n\nK-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛：\n\n假设输入空间 $\\cal X \\in \\R^n \\ $ 为$\\ n\\ $维向量的集合，$\\ \\cal{X}=\\{x^{(1)} ,x^{(2)},\\cdots,x^{(m)} \\} \\ $，$ \\ \\mathcal  C\\ $为输入空间$\\ \\cal X\\ $的一个划分，不妨令$\\ \\mathcal C=\\{ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\} \\ $，因此可以定义$\\ k\\text{-}means\\ $算法的损失函数为\n$$\nJ(\\mathcal C)=\\sum\\limits_{k=1}^K\\sum\\limits_{x^{(i)}\\in \\mathbb C_k}\\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2  \\tag{1}\n$$\n其中$\\ \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)}  \\ $是簇$\\ \\mathbb C_k\\ $的聚类中心。\n\n事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数$J(\\mathcal C)$\n\n##### 1.1 算法流程\n\n1. 首先随机初始化$\\ K\\ $个聚类中心，$\\ \\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(K)} \\ $；\n\n2. 然后根据这$\\ K\\ $个聚类中心给出输入空间$\\ \\mathcal X \\ $的一个划分，$\\ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\ $；\n\n   - 样本离哪个簇的聚类中心最近，则该样本就划归到那个簇\n     $$\n     \\mathop{\\arg\\min}_{k}\\ \\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2 \\tag{2}\n     $$\n\n3. 再根据这个划分来更新这$\\ K\\ $个聚类中心\n   $$\n   \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)} \\tag{3}\n   $$\n\n4. 重复2、3步骤直至收敛\n\n   - 即$\\ K\\ $个聚类中心不再变化\n\n##### 1.2  算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.model.Model\nimport com.strings.data.Data\n\nimport scala.collection.mutable.ListBuffer\nimport scala.util.Random\n\nclass Kmeans(val k:Int = 3,\n             val max_iter:Int = 100,\n             val seed:Long = 1234L,\n             val tolerance: Double = 1e-4) extends Model{\n\n  private var centroids = List[DenseVector[Double]]()\n  private var cluster = ListBuffer[(Int,DenseVector[Double])]()\n\n  var iterations:Int = 0\n\n  def _init_random_centroids(data : List[DenseVector[Double]]):List[DenseVector[Double]] = {\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closest_centroid2(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n        var close_i = 0\n        var closest_dist = -1.0\n        centroids.zipWithIndex.foreach(centroid => {\n          val distance = squaredDistance(centroid._1,row)\n          if(closest_dist>distance || closest_dist == -1.0){\n            closest_dist = distance\n            close_i = centroid._2\n          }\n        })\n    (close_i,row)\n  }\n\n  def _closest_centroid(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n      val distWithIndex =  centroids.zipWithIndex.map(x =>\n                          (squaredDistance(x._1,row),x._2)\n                          ).minBy(_._1)\n      (distWithIndex._2,row)\n  }\n\n  def train(data:List[DenseVector[Double]]):Unit = {\n    centroids = _init_random_centroids(data)\n    var flag = true\n    for(_ <- Range(0,max_iter) if flag){\n      iterations += 1\n      data.foreach{d =>\n        val b = _closest_centroid(centroids, d)\n        cluster.append(b)\n      }\n      val prev_centroid = centroids\n      centroids = _calculate_centroids(cluster)\n      cluster = ListBuffer[(Int,DenseVector[Double])]()\n      val diff = prev_centroid.zip(centroids).map(x => squaredDistance(x._2,x._1))\n      if( diff.sum < tolerance){\n        flag = false\n      }\n    }\n\n  }\n\n  def predit(data:List[DenseVector[Double]]):List[(Int,DenseVector[Double])]= {\n    data.map(x => _closest_centroid(centroids,x))\n  }\n\n  def _calculate_centroids(cluster:ListBuffer[(Int,DenseVector[Double])]):List[DenseVector[Double]]= {\n    cluster.groupBy(_._1).map { x =>\n      val temp = x._2.map(_._2)\n      temp.reduce((a, b) => a :+ b).map(_ / temp.length)\n    }.toList\n  }\n\n  /**\n   * @param x input matrix\n   * @return predict vector value\n   */\n  override def predict(x: DenseMatrix[Double]): DenseVector[Double] = {\n    DenseVector[Double]()\n  }\n}\n\nobject Kmeans{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).map(DenseVector(_)).toList\n    val kmeans = new Kmeans(max_iter = 100)\n    kmeans.train(data)\n    println(kmeans.centroids)\n\n  }\n}\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala \n\n#### 2 GMM 聚类\n\nGMM聚类又称高斯混合聚类，即采用高斯分布来描述原型。现假设每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成。\n\n##### 2.1 算法过程\n\n高斯分布的定义，对于$n$维样本空间$\\mathcal X$中的随机向量$x$，若$x$服从高斯分布，其概率密度函数为\n$$\np(x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}  \\tag{4}\n$$\n其中$\\mu$是$n$维均值向量，$\\Sigma$是$n\\times n$的协方差矩阵,由(4)可以看出，高斯分布完全由均值向量和协方差矩阵$\\Sigma$这两个参数确定。为了明确高斯分布与相应的参数的依赖关系，将概率密度函数记为$p(x|\\mu,\\Sigma)$.\n\n我们可以定义高斯混合分布\n$$\np_{\\mathcal M}(x) = \\sum_{i=1}{k}\\alpha_i.p(x|\\mu_i,\\Sigma_i) \\tag{5}\n$$\n$α_i$称为混合系数,满足$\\sum_{i=1}^{k}\\alpha_i=1$，假设样本的生成过程由高斯混合分布给出：首先，根据$\\alpha_1,\\alpha_2,...,\\alpha_k$定义的先验分布选择高斯混合成分，其中$α_i$为选择第$i$混合成分的概率，然后根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。\n\n若训练集$D={x_1,x_2,...,x_m}$由上述过程生成，令随机变量$z_j \\in{1,2,...,k}$表示生成样本的高斯混合成分，根据贝叶斯定理，$z_j$的后验分布对应于\n$$\np_{\\mathcal M}(z_j = i | x_j) = \\frac{P(z_j=i).p_{\\mathcal M}(x_j|z_j=i)}{p_{\\mathcal M}(x_j)} \\\\\n=\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}   \\tag{6}\n$$\n当高斯混合分布(5)已知时，高斯混合聚类将样本集$D$划分为$k$个簇$\\mathcal C = {C_1,C_2,...,C_k}$,将每个样本$x_j$的簇标记为$\\lambda_j$如下确定：\n$$\n\\lambda_j = \\arg \\max _{i\\in{1,2...,k}}\\gamma_{ji} \\tag{7}\n$$\n对于(5)式，模型参数$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$如何求解？显然给定样本集$D$，可以采用极大似然估计，即最大化对数似然：\n$$\nLL(D) = \\ln(\\prod_{j}^mp_{\\mathcal M}(x_j)) \\\\\n=\\sum_{j=1}^{m}ln(\\sum_{i=1}^{k}\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)) \\tag{8}\n$$\n采用EM算法进行迭代优化求解，下面做个简单地推导：\n\n若参数$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$能使(8)式最大化，则由$\\frac{\\partial LL(D)}{\\partial \\mu_i} = 0$有：\n$$\n\\sum_{j=1}^{m}\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}(x_j - \\mu_i) \\tag{9}\n$$\n由(6)式以及$\\gamma_{ji} = p_{\\mathcal M}(z_j = i |x_j)$有\n$$\n\\mu_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{10}\n$$\n即混合成分的均值可以通过样本的加权平均来估计，样本权重是每个样本属于该成分的后验概率，类似的，由$\\frac{\\partial LL(D)}{\\partial \\Sigma_i} = 0$可以得到：\n$$\n\\Sigma_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}(x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{11}\n$$\n对于混合系数，使用拉格朗日法可以得到：\n$$\n\\alpha_i = \\frac{1}{m}\\sum_{j=1}^{m}\\gamma_{ji} \\tag{12}\n$$\n即每个高斯成分的混合系数由样本属于该成分的平均后验概率确定。\n\n由上述推导可得高斯混合模型的EM算法：在每步迭代中，先根据当前参数来计算每个样本属于高斯成分的后验概率$\\gamma_{ji}$(**E步**), 再根据式(10)-(12)更新参数模型$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$（**M步**）.\n\n##### 2.2  算法流程\n\nGMM算法步骤如下：\n\n![](./聚类算法总结/gmm_算法步骤.png)\n\n##### 2.3 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{*, Axis, DenseMatrix, DenseVector, argmax, det, max, norm, pinv, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport com.strings.utils.MatrixUtils\nimport org.slf4j.LoggerFactory\n\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass GMM(k:Int = 3,\n          max_iterations:Int = 2000,\n          tolerance:Double = 1e-8) {\n\n  private val logger = LoggerFactory.getLogger(classOf[GMM])\n\n  var means:Array[DenseVector[Double]] = new Array[DenseVector[Double]](k)\n  var vars:Array[DenseMatrix[Double]] = new Array[DenseMatrix[Double]](k)\n  var sample_assignments:DenseVector[Int] = _\n  var priors:DenseVector[Double] = _\n  var responsibility:DenseMatrix[Double] = _\n  var responsibilities:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n\n  def _initialize(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val X_lst = (0 until n_samples).map(X.t(::,_))\n    val rng =  new scala.util.Random()\n    priors = DenseVector.ones[Double](k) :/ k.toDouble\n    means = rng.shuffle(X_lst).take(k).toArray\n//    means = X_lst.take(k).toArray\n    for(i <- 0 until k){\n      vars(i) = MatrixUtils.calculate_covariance_matrix(X)\n    }\n  }\n\n  def multivariate_gaussian(X:DenseMatrix[Double],i:Int):DenseVector[Double]={\n    val n_features = X.cols\n    val mean = means(i)\n    val covar = vars(i)\n    val determinant = det(covar)\n    val likelihoods = DenseVector.zeros[Double](X.rows)\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n\n    for((sample,index) <- X_arr.zipWithIndex){\n      val d = n_features\n      val coeff = 1.0 / (math.pow(2 * Math.PI,d/2) * math.sqrt(determinant))\n      val gram = (sample :- mean).t * pinv(covar) * (sample :- mean)\n      val exponent = math.exp(-0.5 * gram)\n      likelihoods(index) = coeff * exponent\n    }\n    likelihoods\n  }\n\n  def _get_likelihoods(X:DenseMatrix[Double]):DenseMatrix[Double] = {\n    val n_samples = X.rows\n    val likelihoods = DenseMatrix.zeros[Double](n_samples,k)\n    for(i <- 0 until k){\n      likelihoods(::,i) := multivariate_gaussian(X,i)\n    }\n    likelihoods\n  }\n\n  def _expectation(X:DenseMatrix[Double]): Unit ={\n    val weighted_likelihoods = _get_likelihoods(X)(*,::).map(x => x :* priors)\n    val sum_likelihoods = sum(weighted_likelihoods,Axis._1)\n    responsibility = weighted_likelihoods(::,*).map(x => x :/ sum_likelihoods) // 列除\n    sample_assignments = argmax(responsibility,Axis._1)\n    responsibilities.append(max(responsibility,Axis._1))\n  }\n\n  def _maximization(X:DenseMatrix[Double]): Unit ={\n    for(i <- 0 until k){\n      val resp = responsibility(::,i)\n      val mean = sum(X(::,*).map(f => resp :* f),Axis._0) :/ sum(resp)\n      means(i) = mean.t\n      val diff = X(*,::).map(f => f :- mean.t)\n      val covariance = diff.t * diff(::,*).map(f => f :* resp) :/sum(resp) // 注意diff(::,*)是取列运算\n      vars(i) = covariance\n    }\n    val n_samples = X.rows\n    priors = sum(responsibility,Axis._0).t :/ n_samples.toDouble\n  }\n\n  def predict(X:DenseMatrix[Double]): DenseVector[Double] = {\n    _initialize(X)\n    var iter = 0\n    var flag = true\n    for (_ <- 0 until max_iterations if flag) {\n      iter += 1\n      _expectation(X)\n      _maximization(X)\n      breakable {\n        if (responsibilities.length < 2) {\n          break()\n        }else{\n          val n = responsibilities.length\n          val diff = norm(responsibilities(n-1) - responsibilities(n-2), 2)\n          println(diff)\n          if (diff <= tolerance) flag = false\n        }\n    }\n  }\n    logger.info(s\"$iter 之后收敛\")\n    _expectation(X)\n    sample_assignments.map(_.toDouble)\n  }\n\n}\n\nobject GMM{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val gmm = new GMM(max_iterations = 100)\n    gmm._initialize(DenseMatrix(data:_*))\n\n    val pred = gmm.predict(DenseMatrix(data:_*))\n    println(pred)\n\n  }\n}\n```\n\n详细请参考:https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/GMM.scala\n\n#### 3 DBSCAN聚类\n\n密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。\n\nDBSCAN是一种著名的密度聚类算法，它是一组“邻域”（neighbhood）参数($\\epsilon $,MinPts)来刻画样本分布的紧密程度.给定数据集$D = \\{x_1,x_2,...,x_m\\}$,定义下面几个概念：\n\n1. $\\epsilon$-邻域：对于$x_j \\in D$,其$\\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\\epsilon$的样本，即$N_{\\epsilon}(x_j) = {x_i \\in D | dist(x_i,x_j) \\leq \\epsilon}$;\n2. 核心对象(core object):若$x_j$的$\\epsilon$-邻域至少包含MinPts个样本，即$|N_{\\epsilon}(x_j)| \\geq MinPts$,则$x_j$是一个核心对象；\n3. 密度直达(directly density-reachable):若$x_j$位于$x_i$的$\\epsilon$-邻域中，且$x_i$是核心对象，则称$x_j$是由$x_i$密度直达；\n4. 密度可达(density-reachable)：对$x_i$与$x_j$，若存在样本序列$p_1,p_2,...,p_n$,其中$p_1 = x_i,p_n = x_j$,且$p_{i+1}$由$p_i$密度直达，则称$x_j$由$x_i$密度可达；\n5. 密度相连(density-connected):对$x_i$与$x_j$，若存在$x_k$使得$x_i$与$x_j$均由$x_k$密度可达，则称$x_i$与$x_j$密度相连；\n\n下图给出了上述概念的直观显示：\n\n![](./聚类算法总结/dbscan_密度直达.png)\n\n##### 3.1 算法步骤\n\nDBSCAN的算法的思想是找出一个核心对象所有密度可达的样本集合形成簇。首先从数据集中任选一个核心对象$A$，找出所有$A$密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：\n\n![](./聚类算法总结/dbscan_算法步骤.png)\n\n##### 3.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass DBSCAN(eps:Double = 1.0,\n             min_samples:Int = 5) {\n\n  var visited_samples:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n  var neighbors: Map[Int, Array[Int]] = Map()\n  var X:DenseMatrix[Double] = _\n  var clusters:ArrayBuffer[Array[Int]] = new ArrayBuffer[Array[Int]]()\n\n  def euclidean_distance(x1:DenseVector[Double],x2:DenseVector[Double]): Double ={\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def _get_neighbors(sample_i:Int): Array[Int] ={\n    val neighbors:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n    val X_arr2 =  X_arr.indices.filter(i => i != sample_i).map(X_arr(_))\n    for((_sample,inx) <- X_arr2.zipWithIndex){\n      val dist = euclidean_distance(_sample,X_arr(sample_i))\n      if(dist < eps){\n        neighbors.append(inx)\n      }\n    }\n    neighbors.toArray\n  }\n\n  def _expand_cluster(sample_i:Int, neighbor:Array[Int]): Array[Int] ={\n    val cluster:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    cluster.append(sample_i)\n    for(neighbor_i <- neighbor){\n      if(!visited_samples.contains(neighbor_i)){\n        visited_samples.append(neighbor_i)\n        neighbors += (neighbor_i -> _get_neighbors(neighbor_i) )\n        if (neighbors(neighbor_i).length >= min_samples){   //        neighbors.get(neighbor_i).size 结果是1\n          val expanded_cluster = _expand_cluster(neighbor_i,neighbors(neighbor_i))\n          cluster.append(expanded_cluster:_*)\n        }else{\n          cluster.append(neighbor_i)\n        }\n      }\n    }\n    cluster.toArray\n  }\n\n  def _get_cluster_labels(): Array[Int] ={\n    val labels = Array.fill(X.rows)(clusters.length)\n    for((cluster,cluster_i) <- clusters.zipWithIndex){\n      for(sample_i <- cluster){\n        labels(sample_i) = cluster_i\n      }\n    }\n    labels\n  }\n\n  def predict(XX:DenseMatrix[Double]): Array[Int] ={\n    X = XX\n    visited_samples = new ArrayBuffer[Int]()\n    neighbors = Map()\n    val n_samples = X.rows\n    for(sample_i <- 0 until n_samples){\n      breakable {\n        if (visited_samples.contains(sample_i)) {\n          break()\n        }else{\n          neighbors += (sample_i -> _get_neighbors(sample_i))\n          if(neighbors.get(sample_i).size >= min_samples){\n            visited_samples.append(sample_i)\n          }\n          val new_cluster = _expand_cluster(sample_i,neighbors(sample_i))\n          clusters.append(new_cluster)\n        }\n      }\n    }\n    _get_cluster_labels()\n  }\n\n}\n\nobject DBSCAN{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val dbscan = new DBSCAN(eps = .7,min_samples = 5)\n\n    val pred = dbscan.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n  }\n}\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/DBSCAN.scala\n\n#### 4 PAM聚类算法\n\nK-means是每次选**簇的**均值**作为新的中心，迭代直到簇中对象分布不再变化。其缺点是对于离群点是敏感的，因为一个具有很大极端值的对象会扭曲数据分布。那么我们可以考虑新的簇中心不选择均值而是选择**簇内的某个对象**，只要使总的代价降低就可以。\n\nPAM（partitioning around medoid，围绕中心点的划分）是具有代表性的k-medoids算法。\n\n它最初随机选择k个对象作为中心点，该算法反复的用非代表对象（非中心点）代替代表对象，试图找出更好的中心点，以改进聚类的质量。 K均值聚类一般使用欧几里得距离，而PAM可以使用任意的距离来计算。因此， PAM可以容纳混合数据类型，并且不仅限于连续变量。\n\n##### 4.1  算法步骤\n\n​       PAM算法如下：\n​       (1) 随机选择K个观测值（每个都称为中心点）；\n​       (2) 计算观测值到各个中心的距离/相异性；\n​       (3) 把每个观测值分配到最近的中心点；\n​       (4) 计算每个中心点到每个观测值的距离的总和（总成本）；\n​       (5) 选择一个该类中不是中心的点，并和中心点互换；\n​       (6) 重新把每个点分配到距它最近的中心点；\n​       (7) 再次计算总成本；\n​       (8) 如果总成本比步骤(4)计算的总成本少，把新的点作为中心点；\n​       (9) 重复步骤(5)～(8)直到中心点不再改变。\n\n##### 4.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\n\n/**\n *  Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.\n *\n * @param k nums of cluster\n */\n\n\nclass PAM(k:Int = 2,seed:Long = 1234L) {\n\n  def _init_random_medoids(X: DenseMatrix[Double]): IndexedSeq[DenseVector[Double]] ={\n    val n_samples = X.rows\n    val data = (0 until n_samples).map(X.t(::,_))\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closet_medoid(sample:DenseVector[Double],medoids:IndexedSeq[DenseVector[Double]]): Int ={\n    val distWithIndex =  medoids.zipWithIndex.map(x =>\n      (squaredDistance(x._1,sample),x._2)\n    ).minBy(_._1)\n    distWithIndex._2\n  }\n\n  def _create_clusters(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[Array[Int]] ={\n    val clusterss = new Array[Int](X.rows)\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((sample,inx) <- data.zipWithIndex){\n      val medoid_i = _closet_medoid(sample,medoids)\n      clusterss(inx) = medoid_i\n    }\n    clusterss.zipWithIndex.groupBy(_._1).toArray.sortBy(_._1).map(_._2.map(_._2))\n  }\n\n  def _calculate_cost(X:DenseMatrix[Double],clusters:Array[Array[Int]],medoids:IndexedSeq[DenseVector[Double]]):Double={\n    var cost = 0.0\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((cluster,i) <- clusters.zipWithIndex){\n      val medoid = medoids(i)\n      for(sample_i <- cluster){\n        cost += squaredDistance(data(sample_i),medoid)\n      }\n    }\n    cost\n  }\n\n  def _get_cluster_labels(clusters:Array[Array[Int]],X:DenseMatrix[Double]): Array[Int] ={\n    val y_pred = Array.fill(X.rows)(0)\n    for(cluster_i <- 0 until clusters.length){\n      val cluster = clusters(cluster_i)\n      for(sample_i <- cluster){\n        y_pred(sample_i) = cluster_i\n      }\n    }\n    y_pred\n  }\n  def _get_no_medoids(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[DenseVector[Double]] ={\n    val non_medoids:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n    val data = (0 until X.rows).map(X.t(::,_))\n\n    for(sample <- data){\n      if(!medoids.contains(sample)) non_medoids.append(sample)\n    }\n    non_medoids.toArray\n  }\n\n  def predict(X:DenseMatrix[Double]): Array[Int] ={\n    var medoids = _init_random_medoids(X)\n    val clusters = _create_clusters(X,medoids)\n    var cost = _calculate_cost(X, clusters, medoids)\n    breakable {\n      while (true) {\n        var best_medoids = medoids\n        var lowest_cost = cost\n        for (medoid <- medoids) {\n          val non_medoids = _get_no_medoids(X, medoids)\n          for (sample <- non_medoids) {\n            val new_medoids = new Array[DenseVector[Double]](medoids.length)\n            for (i <- 0 until medoids.length) {\n              new_medoids(i) = medoids(i)\n            }\n            val inx: IndexedSeq[Int] = medoids.indices.filter(i => medoids(i) == medoid)\n            inx.foreach(i => new_medoids(i) = sample)\n\n            val new_clusters = _create_clusters(X, new_medoids)\n            val new_cost = _calculate_cost(X, new_clusters, new_medoids)\n\n            if (new_cost < lowest_cost) {\n              lowest_cost = new_cost\n              best_medoids = new_medoids\n            }\n          }\n        }\n        if (lowest_cost < cost) {\n          cost = lowest_cost\n          medoids = best_medoids\n        } else {\n          break()\n        }\n      }\n    }\n    val finaly_clusters = _create_clusters(X,medoids)\n    _get_cluster_labels(finaly_clusters,X)\n  }\n\n}\n\nobject PAM{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val pam = new PAM(k = 3)\n\n    val pred = pam.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n    val acc =  Metric.accuracy(pred.map(_.toDouble),target) * 100\n    println(f\"准确率为: $acc%-5.2f%%\")\n  }\n}\n\n```\n\n详细请参考：https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/PAM.scala\n\n#### 5. LVQ聚类\n\nLVQ又称“学习向量量化”(Learning Vector Quantization)也是试图找到一组原型向量来刻画聚类结构，但与一般聚类算法不同的是，LVQ假设样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。\n\n给定样本集$\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$,每个样本$x_j$是由$n$个属性描述的特征向量$(x_{j1};x_{j2};...;x_{jn})$,$y_j \\in \\mathcal Y$是样本$x_j$的类别标记. LVQ的目标是学得一组$n$维原型向量$\\{p_1,p_2,...,p_q\\}$,每个原型向量代表一个聚类簇，簇标记为$t_i \\in \\mathcal Y$.\n\n##### 5.1 算法步骤\n\nLVQ的算法步骤如下：\n\n![](./聚类算法总结/lvq_算法步骤.png)\n\n##### 5.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, argmin, sum}\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\n\nclass LVQ(t:Array[Int],\n          lr:Double = 0.1,\n          nums_iters:Int = 400) {\n\n  val c = t.distinct.length\n  val q = t.length\n  var C: Map[Int, ArrayBuffer[Int]] = Map()\n  var p: DenseMatrix[Double] = _\n  var labels: DenseVector[Int] = _\n\n  def euclidean_distance(x1: DenseVector[Double], x2: DenseVector[Double]): Double = {\n    require(x1.length == x2.length)\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def fit(X: DenseMatrix[Double], y: DenseVector[Int]) = {\n    p = DenseMatrix.zeros[Double](q, X.cols)\n    for (i <- 0 until q) {\n      C += (i -> ArrayBuffer[Int]())\n      val candidate_indices = y.toArray.indices.filter(f => y(f) == t(i))\n      val target_indice = Random.shuffle(candidate_indices.toList).take(1).apply(0)\n      p(i, ::) := X(target_indice, ::)\n    }\n\n\n    var p_arr = (0 until p.rows).map(p.t(::, _))\n    for (_ <- 0 until nums_iters) {\n      val j = Random.shuffle(Range(0, y.length).toList).take(1).apply(0)\n      val x_j = X(j, ::).t\n      val d = p_arr.map(f => euclidean_distance(f, x_j))\n      val idx: Int = argmin(d.toArray)\n      if (y(j) == t(idx)) {\n        p(idx, ::) := p(idx, ::) :+ ((X(j, ::) :- p(idx, ::)) :* lr)  // :+ 和 :* 运算优先级一致\n      } else {\n        p(idx, ::) := p(idx, ::) :- ((X(j, ::) :- p(idx, ::)) :* lr)\n      }\n\n    }\n    p_arr = (0 until p.rows).map(p.t(::, _))\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      C(idx).append(j)\n    }\n\n    labels = DenseVector.zeros[Int](X.rows)\n    for (i <- 0 until q) {\n      for (j <- C(i)) {\n        labels(j) = i\n      }\n    }\n  }\n\n  def predict(X: DenseMatrix[Double]): DenseVector[Int] = {\n    val p_arr = (0 until p.rows).map(p.t(::, _))\n    val preds_y: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      preds_y.append(t(idx))\n    }\n    DenseVector(preds_y.toArray)\n  }\n}\n\nobject LVQ{\n  def main(args: Array[String]): Unit = {\n\n    val X = Array(Array(0.697,0.460),Array(0.774,0.376),Array(0.634,0.264),Array(0.608,0.318),Array(0.556,0.215),\n                  Array(0.403,0.237),Array(0.481,0.149),Array(0.437,0.211),Array(0.666,0.091),Array(0.243,0.267),\n                  Array(0.245,0.057),Array(0.343,0.099),Array(0.639,0.161),Array(0.657,0.198),Array(0.360,0.370),\n                  Array(0.593,0.042),Array(0.719,0.103),Array(0.359,0.188),Array(0.339,0.241),Array(0.282,0.257),\n                  Array(0.748,0.232),Array(0.714,0.346),Array(0.483,0.312),Array(0.478,0.437),Array(0.525,0.369),\n                  Array(0.751,0.489),Array(0.532,0.472),Array(0.473,0.376),Array(0.725,0.445),Array(0.446,0.459))\n\n   val XX = DenseMatrix(X:_*)\n   val y = DenseVector.zeros[Int](XX.rows)\n\n    for(i <- 9 until 21){\n      y(i) = 1\n    }\n\n    val t = Array(0,1,1,0,0)\n    println(y)\n    val lvq = new LVQ(t)\n    lvq.fit(XX,y)\n\n    println(lvq.C)\n    println(lvq.labels)\n    println(lvq.predict(XX))\n\n  }\n}\n```\n\n详细代码请参考：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/LVQ.scala\n\n代码实现参考了python代码：\n\nhttps://github.com/fengyang95/tiny_ml/blob/master/tinyml/cluster/LVQ.py\n\n#### 6 层次聚类\n\n层次聚类(hierarchical clustering)是一种基于树形结构的聚类方法，常用的是**自底向上**的结合策略（**AGNES算法**），它将数据集中的每个样本看作一个初始聚类簇，然后再算法运行的每一步中找到距离最近的两个聚类簇进行合并，该过程不断重复，直至达到预设的聚类簇个数。这里的关键是如何计算聚类簇之间的距离。实际上，每个簇是一个样本集合，只需要采用集合的某种距离即可。例如，给定聚类簇$C_i$与$C_j$,可以通过下面的式子来计算距离：\n\n最小距离：\n$$\nd_{\\min}(C_i,C_j) = \\min _{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{15}\n$$\n最大距离：\n$$\nd_{\\max}(C_i,C_j) = \\max_{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{16}\n$$\n平均距离：\n$$\nd_{avg}(C_i,C_j) = \\frac{1}{|C_i||C_j|}dist(x,z)  \\tag{17}\n$$\n显然，最小距离由两个簇的最近的样本决定；最大距离由两个簇的最远样本决定，而平均距离则由两个簇的所有样本决定。\n\n##### 6.1  算法步骤\n\nAGNES 算法步骤如下,在1-9行，算法先对仅含一个样本的初始聚类簇和相应的距离进行初始化，然后在11-23行，AGNES不断合并距离最近的聚类簇，并对合并得到的聚类簇的距离矩阵进行更新；上述过程不断重复，直至达到预设的聚类簇数。\n\n![](./聚类算法总结/hc_算法步骤.png)\n\n##### 6.2 算法实现\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector}\nimport com.strings.data.Data\nimport com.strings.utils.MatrixUtils\n\nimport scala.collection.mutable.ArrayBuffer\n\n\ncase class ClusterNode(vector:DenseVector[Double],\n                       id:Int,\n                       left:ClusterNode = null,\n                       right:ClusterNode = null,\n                       distance:Double = -1.0,\n                       count:Int = 1)\n\nclass HierarchicalCluster(k:Int) {\n\n  var labels:DenseVector[Int] = _\n\n  def fit(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val n_features = X.cols\n    val X_arr = (0 until n_samples).map(X.t(::,_))\n    val nodes:ArrayBuffer[ClusterNode] = new ArrayBuffer[ClusterNode]()\n    for((sample,inx) <- X_arr.zipWithIndex){\n      nodes.append(ClusterNode(sample,inx))\n    }\n    labels = DenseVector.ones[Int](n_samples) :* (-1)\n    var distances: Map[(Int, Int), Double] = Map()\n    var curret_cluster_id = -1\n    while (nodes.length > k){\n      var min_dist = Double.MaxValue\n      val nodes_len = nodes.length\n      var closest_part:(Int, Int) = 0 -> 0\n      for(i <- 0 until nodes_len - 1){\n        for(j <- i+1 until nodes_len){\n          val d_key = nodes(i).id -> nodes(j).id\n          if(!distances.contains(d_key)){\n            distances += (d_key -> MatrixUtils.euclidean_distance(nodes(i).vector,nodes(j).vector))\n          }\n          val d = distances(d_key)\n          if(d < min_dist){\n            min_dist = d\n            closest_part = i -> j\n          }\n        }\n      }\n\n      val part1 = closest_part._1\n      val part2 = closest_part._2\n      val node1 = nodes(part1)\n      val node2 = nodes(part2)\n      val new_vec = DenseVector.ones[Double](n_features)\n      for(i <- 0 until n_features){\n        new_vec(i) = (node1.vector(i) * node1.count + node2.vector(i) * node2.count)/\n          (node1.count + node2.count)\n      }\n      val new_count = node1.count + node2.count\n      val new_node = ClusterNode(new_vec,curret_cluster_id,node1,node2, min_dist,new_count)\n\n      curret_cluster_id -= 1\n      nodes.remove(part2)\n      nodes.remove(part1)\n      nodes.append(new_node)\n    }\n    calc_label(nodes)\n\n  }\n\n  def calc_label(nodes:ArrayBuffer[ClusterNode]): Unit ={\n    for((node,inx) <- nodes.zipWithIndex){\n      leaf_traveral(node,inx)\n    }\n  }\n\n  def leaf_traveral(node:ClusterNode,label:Int): Unit ={\n    if(node.left == null && node.right == null){\n      labels(node.id) = label\n    }\n    if(node.left != null){\n      leaf_traveral(node.left,label)\n    }\n    if(node.right != null){\n      leaf_traveral(node.right,label)\n    }\n  }\n}\n\nobject HierarchicalCluster{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4))\n    val dd = DenseMatrix(data:_*)\n    val hc = new HierarchicalCluster(k=3)\n    hc.fit(dd)\n    println(hc.labels)\n  }\n}\n\n```\n\n算法实现参考了：\n\nhttps://zhuanlan.zhihu.com/p/32438294\n\n\n\n#### 参考文献\n\n1. 周志华 机器学习 - 聚类部分\n\np.s. 该总结主要介绍每个聚类算法的算法步骤，以及scala的简单实现，并没有多注重效率，仅供自己学习使用。"},{"title":"adaboost算法","url":"/2020/05/08/adaboost算法/","content":"##### 1. 算法简介\n\nBoosting, 也称为增强学习或提升法，是一种重要的集成学习技术， 能够将预测精度仅比随机猜度略高的弱学习器增强为预测精度高的强学习器，这在直接构造强学习器非常困难的情况下，为学习算法的设计提供了一种有效的新思路和新方法。其中最为成功应用的是，Yoav Freund和Robert Schapire在1995年提出的AdaBoost算法。\n\n​      AdaBoost是英文\"Adaptive Boosting\"（自适应增强）的缩写，它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。\n\n##### 2. 算法过程\n\n给定训练数据集： $(x_1,y_1),...,(x_N,y_N)$，其中 $y_i \\in \\{1,-1\\}$，用于表示训练样本的类别标签$i=1,...,N$。Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。\n\n##### 3. 算法详细步骤\n\n1. 初始化数据的权值分布\n   $$\n   D_1 = (w_{11},...,w_{1i},...,w_{1N}),w_{1i} = \\frac{1}{N},i=1,2,...,N\n   $$\n\n2. 对$m=1,2,...,M$\n\n   (a) 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器\n   $$\n   G_m(x):f \\rightarrow  \\{-1,+1\\}\n   $$\n   (b) 计算$G_m(x)$在训练数据集上的分类误差率\n   $$\n   e_m = P(G_m(x_i) \\neq y_i) = \\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)\n   $$\n   (c) 计算$G_m(x)$的系数\n   $$\n   \\alpha_m = \\frac{1}{2}\\log\\frac{1-e_m}{e_m}\n   $$\n   这里的对数是自然对数\n\n   (d) 更新训练数据集的权值分布\n   $$\n   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\\\\n   w_{m+1,i} = \\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),i=1,2,..,N\n   $$\n   这里，$Z_m$是规范化因子\n   $$\n   Z_m = \\sum_{i=1}^Nw_{mi}\\exp(-\\alpha_my_iG_m(x_i))\n   $$\n   它使$D_{m+1}$成为一个概率分布\n\n3. 构建基本分类器的线性组合\n   $$\n   f(x) = \\sum_{m=1}^{M}\\alpha_mG_m(x)\n   $$\n   得到最终的分类器\n   $$\n   G(x) = sign(f(x)) = sign(\\sum_{m=1}^M\\alpha_mG_m(x))\n   $$\n   对AdaBoost算法做下面的说明：\n\n    （1）首先，是初始化训练数据的权值分布$D_1$。假设有$N$个训练样本数据，则每一个训练样本最开始时，都被赋予相同的权值$w_1 = \\frac{1}{N}$。\n\n    （2）然后，训练弱分类器$h_i$。具体训练过程中是：如果某个训练样本点，被弱分类器$h_i$准确地分类，那么在构造下一个训练集中，它对应的权值要减小；相反，如果某个训练样本点被错误分类，那么它的权值就应该增大。权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。\n\n    （3）最后，将各个训练得到的弱分类器组合成一个强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。\n\n     换而言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。\n   \n   ##### 4. 代码实现\n   \n   代码实现地址：\n   \n   https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/AdaBoost.scala\n   \n   ##### 5. 参考文献\n   \n   李航 《统计学习方法》2012.3"},{"title":"降维方法-总结","url":"/2020/05/06/降维方法-总结/","content":"#### 1. 降维概述\n\n样本的特征数称为维数（dimensionality），当维数非常大时，也就是现在所说的“维数灾难”，具体表现在：在高维情形下，数据样本将变得十分稀疏，因为此时要满足训练样本为“密采样”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉...训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力；同时当维数很高时，计算距离也变得十分复杂，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数“低维计算，高维表现”的原因。\n\n缓解维数灾难的一个重要途径就是降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个低维嵌入，例如：数据属性中存在噪声属性、相似属性或冗余属性等，对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果。\n\n#### 2.降维方法分类\n\n![这里写图片描述](http://img.blog.csdn.net/20150522194801297)\n\n#### 3 线性方法\n\n##### 3.1 PCA主成分分析\n\n主成分分析（PCA）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中。简单来理解这一过程便是：PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。\n\n假设使用$d^{’}$个新基向量来表示原来样本，实质上是将样本投影到一个由$d^{’}$个基向量确定的一个超平面上（即舍弃了一些维度），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：\n\n> **最近重构性**：样本点到超平面的距离足够近，即尽可能在超平面附近；\n> **最大可分性**：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。\n\nPCA的算法描述如下：\n\n![](.\\降维方法-总结\\pca_算法步骤.png)\n\n##### 3.2 LDA 线性判别分析\n\n参考之前的博文\n\n#### 4 非线性方法\n\n##### 4.1 MDS \n\n不管是使用核函数升维还是对数据降维，我们都希望**原始空间样本点之间的距离在新空间中基本保持不变**，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。**“多维缩放”（MDS）**正是基于这样的思想，**MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持**。\n\n假定$m$个样本在原始空间中任意两两样本之间的距离矩阵为$D \\in R ^{m \\times m}$，其中第$i$行$j$列的元素$dist_{ij}$为样本$x_i$到$x_j$的距离。我们的目标便是获得样本在低维空间中的表示$Z \\in R^{d^{’} \\times m} $, $d'< d$，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即$||zi-zj||=dist_{ij}$。因此接下来我们要做的就是根据已有的距离矩阵$D$来求解出降维后的坐标矩阵$Z$。\n\n令降维后的样本坐标矩阵Z被中心化，**中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量**。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。\n\n![4.png](https://i.loli.net/2018/10/18/5bc851a4a4ee2.png)\n\n令$B = Z^TZ \\in R^{m \\times m}$,其中$B$为降维后的样本的內积矩阵，$b_{ij} = z_i^Tz_j$,有：\n$$\ndist_{ij}^{2} = ||z_i||^2 + ||z_j||^2 -2z_i^Tz_j\n= b_{ii} + b_{jj} - 2b_{ij}\n$$\n为了方便讨论，令降维后的样本$Z$被中心化，即$\\sum_{i=1}^{m}z_i = 0$,显然$B$的行与列之和均为0，即$\\sum_{i=1}^{m}b_{ij} =\\sum_{j=1}^{m}b_{ij} =  0$,容易知道：\n$$\n\\sum_{i=1}^{m}dist_{ij}^2 = tr(B) +mb_{jj}\n$$\n\n$$\n\\sum_{j=1}^{m}dist_{ij}^2 = tr(B) +mb_{ii}\n$$\n\n$$\n\\sum_{i=1}^m\\sum_{i=1}^{m}dist_{ij}^2 = 2m * tr(B)\n$$\n\n其中$tr(.)$表示矩阵的迹(trace),$tr(B) = \\sum_{i=1}^{m}||z_i||^2$,令\n$$\ndist_{i.}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{.j}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{..}^2=\\frac{1}{m^2}\\sum_{i=1}^{m}\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n由上面三个等式可知：\n$$\nb_{ij} = -0.5(dist_{ij}^2 - dist_{i.}^2 -dist_{.j}^2 + dist_{..}^2)\n$$\n通过降维前后保持不变的距离矩阵$D$求取內积矩阵$B$.\n\n对$B$做特征值分解(**eigenvalue decompostion**), $B = VUV^T$,其中$U=diag(\\lambda _1,\\lambda _2,...,\\lambda _d)$为特征值构成的对角矩阵，$\\lambda _1 \\geq \\lambda _2 \\geq ... \\geq \\lambda _d$,$V$为特征向量矩阵。假定其中有$d^*$个非零特征值，他们构成的对角矩阵 $U_* = diag(\\lambda _1,\\lambda _2,...,\\lambda _{d^{*}})$ ,其中$V_*$表示对应的特征向量矩阵，则$Z$可以表示为：\n$$\nZ = U_*^{\\frac{1}{2}}V_*^T \\in R^{d^*\\times m}\n$$\nMDS 的算法描述如下：\n\n![](.\\降维方法-总结\\mds_算法步骤.png)\n\n##### 4.2 Isomap\n\n等度量映射（Isomap)属于流行学习，流形学习（manifold learning）是一种借助拓扑流形概念的降维方法**，**流形是指在局部与欧式空间同胚的空间，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种**“邻域保持”**的思想 ，等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系。\n\n等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。**因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离**，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的**Dijkstra算法**或**Floyd算法**计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。\n\n![](./降维方法-总结/测地距离.png)\n\n从**MDS**算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵$B$，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量$w$，因此对于需要降维的新样本无从下手，书中给出的权宜之计是利用已知高/低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。\n\nISOMAP的算法步骤如下：\n\n![](./降维方法-总结/isomap_算法步骤.png)\n\n对于近邻图的构建，常用的有两种方法：**一种是指定近邻点个数**，像**KNN**一样选取k个最近的邻居；**另一种是指定邻域半径**，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：\n\n> 若**邻域范围指定过大，则会造成“短路问题”**，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。\n> 若**邻域范围指定过小，则会造成“断路问题”**，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。\n\n##### 4.3 LLE \n\n不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本$x_i$的坐标可以通过它的邻域样本线性表出：\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLE希望上式的关系在低维空间中得以保持。\n\n![](./降维方法-总结/LLE空间保持.png)\n\nLLE先为每个样本$x_i$找到其近邻下标集合$Q_i$,然后计算基于$Q_i$中的样本点对$x_i$进行线性重构的系数$w_i$.\n$$\n\\min_{w_1,w_2,...,w_m}\\sum_{i=1}^m||x_i - \\sum_{j \\in Q_i}w_{ij}{x_j}||_2^2\n$$\n\n$$\ns.t. \\sum_{j\\in Q_i}w_{ij} = 1\n$$\n\n其中$x_i$和$x_j$均为已知，令$C_{jk} = (x_i- x_j)^T(x_i-x_j)$,$w_{ij}$有闭式解\n$$\nw_{ij} = \\frac{\\sum_{k \\in Q_i}C_{jk}^{-1}}{\\sum_{l,s \\in Q_i}C_{ls}^{-1}}\n$$\n**LLE**在低维空间中保持$w_i$不变，于是$x_i$对应的低维空间坐标$z_i$可通过下式求解：\n$$\n\\min_{z_1,z_2,...,z_m}\\sum_{i=1}^m||z_i - \\sum_{j \\in Q_i}w_{ij}{z_j}||_2^2\n$$\n上式需要确定的是$x_i$对应的低维空间坐标$z_i$.\n\n令$Z =(z_1,z_2,...,z_m) \\in R^{d'\\times m}$,$(W)_{ij} = w_{ij}$,\n$$\nM = (I-W)^T(I-W)\n$$\n则优化目标可以重写为下式：\n$$\n\\begin{equation}\n\\min_z tr(ZMZ^T) \\\\\ns.t. ZZ^T = I \n\\end{equation}\n$$\n可以通过特征值分解求解，$M$最小的$d'$的特征值对应的特征向量组成的矩阵即为$Z^T$\n\nLLE算法步骤如下：\n\n![](./降维方法-总结/lle_算法步骤.png)\n\n##### 4.4 核PCA kernel PCA\n\n说起机器学习你中有我/我中有你/水乳相融...在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，**即先将样本映射到高维空间，再在高维空间中使用线性降维的方法**。下面主要介绍**核化主成分分析（KPCA）**的思想。\n\n若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：**即空间中的任一向量，都可以由该空间中的所有样本线性表示**。\n\n假定我们将在高维特征空间中的数据投影到由$W$确定的超平面上，即PCA欲求解\n$$\n(\\sum_{i=1}^mz_iz_i^T)W = \\lambda W\n$$\n其中$z_i$是样本点$x_i$在高维特征空间的像，可知：\n$$\nW = \\frac{1}{\\lambda}(\\sum_{i=1}^mz_iz^T_i)W = \\sum_{i=1}^m z_i\\frac{z_i^TW}{\\lambda} \\\\\n=\\sum_{i=1}^m z_i\\alpha _i\n$$\n其中$\\alpha _i = \\frac{1}{\\lambda}z_i^TW$.假定$z_i$是由原始属性空间中的样本点$x_i$通过映射$\\phi$产生，即$z_i = \\phi (x_i),i= 1,2,...,m$,若$\\phi$能被显式的表达出来，则通过它将样本映射至高维特征空间，再在特征空间中实施PCA即可。\n\n即（17）可变为\n$$\n(\\sum_{i=1}^m \\phi (x_i) \\phi (x_i)^T)W = \\lambda W\n$$\n（18）式可变为：\n$$\nW= \\sum_{i=1}^m \\phi(x_i)\\alpha_i\n$$\n一般情形下，我们不清楚$\\phi$的具体形式，于是引入核函数:\n$$\n\\kappa(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\n$$\n将（21)(20)带入到(19)式中可得：\n$$\nKA= \\lambda A\n$$\n其中$K$为$\\kappa$ 对应的核矩阵，$(K)_{ij} = \\kappa(x_i,x_j),A = (\\alpha_1;\\alpha_2;...;\\alpha_m)$,显然上式是个特征值分解问题，取$K$最大的$d'$个特征值对应的特征向量即可。\n\n##### 4.5 DiffusionMap\n\n扩散映射是一种降维方法\n\n1. 其通过 整合数据的局部几何关系 揭示 数据集在不同尺度的几何结构。\n2. 与PCA (principal component analysis)、MDS (Multidimensional Scaling) 这些降维方法相比，扩散映射 非线性，聚焦于发现数据集潜在的流形结构。\n3. 优点：对噪声鲁棒，计算代价较低\n\n算法步骤如下:\n\n![](./降维方法-总结/diffmap_算法步骤.png)"},{"title":"xgboost","url":"/2020/03/18/xgboost/","content":"#### 1. XGBoost 简介\n\n$XGBoost$的全称是$eXtremeGradientBoosting$，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。$XGBoost$是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量的Kaggle选手选用$XGBoost$进行数据挖掘比赛，是各大数据科学比赛的必杀武器；在工业界大规模数据方面，$XGBoost$的分布式版本有广泛的可移植性，支持在Kubernetes、Hadoop、SGE、MPI、 Dask等各个分布式环境上运行，使得它可以很好地解决工业界大规模数据的问题。本文将从$XGBoost$的数学原理和工程实现上进行介绍，然后介绍$XGBoost$的优缺点，并在最后给出面试中经常遇到的关于$XGBoost$的问题。\n\n#### 2.  $XGBoost \\ $算法\n\n​    $XGBoost$是由 $k$个基模型组成的一个加法模型，假设我们第 $t$ 次迭代要训练的树模型是 $f_t(x)$ ，则有：\n\n​\t\n$$\n\\hat{y_i}^{(t)} = \\sum_{k=1}^{t}f_k(x_i) = \\hat{y_i}^{(t-1)} + f_t(x_i)\n$$\n其中，$\\hat{y_i}^{(t)}$是第$t$次迭代后样本的$i$的预测结果，$\\hat{y_i}^{(t-1)}$是前$t-1$棵树的预测结果，$f_t(x_i)$是第$t$棵树的模型。\n\n​\t$XGBoost  \\ $算法是$\\ GBDT\\ $算法的改进版本，其目标函数为：\n$$\n\\begin{aligned}\nObj^{(k)}&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_k)+\\sum\\limits_{i=1}^T\\Omega(f_i)\\\\\n&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\\Omega(f_k)+C\n\\end{aligned}\n$$\n同理为了求损失函数$\\ l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处的二阶展开，不妨先对$\\ l(y^{(i)},x)\\ $在$\\ \\hat{y}^{(i)}_{k-1} \\ $处进行二阶展开可得：\n$$\nl(y^{(i)},x)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+\\nabla_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})+\\dfrac{1}{2}\\nabla^2_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})^2\n$$\n令$\\ x=\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}) \\ $，且记$\\ \\nabla_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ g_i\\ $、$\\ \\nabla^2_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $为$\\ h_i\\ $则有：\n$$\nl\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\n$$\n其中，$g_i$为损失函数的一阶导数，$h_i$为损失函数的二阶导数，注意这里是对$\\hat{y}^{(t-1)}_{i}$求导。\n\n**以平方损失为例：**\n$$\nl(y^{(i)},\\hat{y}^{(t-1)}_i) = (y^{(i)},\\hat{y}^{(t-1)}_i)^2\n$$\n则：\n$$\n\\begin{aligned}\ng_i &= \\frac{\\partial{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{\\hat{y}^{(t-1)}_i}} = -2(y_i - \\hat{y}_i^{(t-1)}) \\\\\nh_i &= \\frac{\\partial ^2{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{(\\hat{y}^{(t-1)}_i})^2} = 2\n\\end{aligned}\n$$\n又因为在第$\\ k\\ $步$\\ \\hat{y}^{(i)}_{k-1} \\ $其实是已知的，所以$\\ l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\ $是一个常数函数，故对优化目标函数不会产生影响，将上述结论带入目标函数$\\ Obj^{(k)}\\ $可得：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\n$$\n\n##### 2.1  优化目标函数\n\n​\t以$\\ XGBoost\\ $算法的目标函数为例，对于任意决策树$\\ f_k \\ $，**假设其叶子结点个数$\\ T\\ $，该决策树是由所有结点对应的值组成的向量$\\ w\\in\\mathbb{R}^T\\ $，以及能够把特征向量映射到叶子结点的函数$\\ q(*):\\mathbb{R}^d\\rightarrow \\{1,2,\\cdots,T \\} \\ $构造而成的，且每个样本数据都存在唯一的叶子结点上。因此决策树$\\ f_k\\ $可以定义为$\\ f_k(x)=w_{q(x)} \\ $。**决策树的复杂度可以由正则项$\\ \\Omega(f_k)=\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2 \\ $来定义，该正则项表明决策树模型的复杂度可以由叶子结点的数量和叶子结点对应值向量$\\ w \\ $的$\\ L2\\ $范数决定。定义集合$\\ I_j=\\{i|q(x^{(i)})=j \\}\\ $为划分到叶子结点$\\ j \\ $的所有训练样本的集合，即之前训练样本的集合，现在都改写成叶子结点的集合，因此$\\ XGBoost\\ $算法的目标函数可以改写为：\n$$\n\\begin{aligned}\nObj^{(k)}&\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\\\\\n&=\\sum\\limits_{i=1}^m\\bigg[g_iw_{q(x^{(i)})}+\\dfrac{1}{2}h_jw^2_{q(x^{(i)})} \\bigg]+\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2\\\\\n&=\\sum\\limits_{j=1}^T\\bigg[(\\sum\\limits_{i\\in I_j}g_i)w_j+\\dfrac{1}{2}(\\sum\\limits_{i\\in I_j}h_i+\\lambda)w_j^2 \\bigg]+\\gamma T\n\\end{aligned}\n$$\n令$\\ G_j=\\sum\\limits_{i\\in I_j}g_i ,\\ H_j=\\sum\\limits_{i\\in I_j}h_i \\ $则有：\n$$\nObj^{(k)}\\simeq\\sum\\limits_{j=1}^T\\bigg[G_jw_j+\\dfrac{1}{2}(H_j+\\lambda)w_j^2 \\bigg]\n$$\n分析可知当更新到第$\\ k\\ $步时，此时**决策树结构固定的情况下**，每个叶子结点有哪些样本是已知的，那么$\\ q(*)\\ $和$\\ I_j\\ $也是已知的；又因为$\\ g_i\\ $和$\\ h_i\\ $是第$\\ k-1\\ $步的导数，那么也是已知的，因此$\\ G_j\\ $和$\\ H_j\\ $都是已知的。令目标函数$\\ Obj^{(k)}\\ $的一阶导数为$\\ 0\\ $，即可求得叶子结点$\\ j\\ $对应的值为：\n$$\nw^*_j=-\\dfrac{G_j}{H_j+\\lambda}\n$$\n因此针对于结构固定的决策树，最优的目标函数$\\ Obj\\ $为：\n$$\nObj=-\\dfrac{1}{2}\\sum\\limits_{j=1}^T\\dfrac{G_j^2}{H_j+\\lambda}+\\gamma T\n$$\n上面的推导是建立在决策树结构固定的情况下，然而决策树结构数量是无穷的，所以实际上并不能穷举所有可能的决策树结构，什么样的决策树结构是最优的呢？通常使用贪心策略来生成决策树的每个结点，$\\ XGBoost \\ $算法的在决策树的生成阶段就对过拟合的问题进行了处理，因此无需独立的剪枝阶段，具体步骤可以归纳为：\n\n1. 从深度为$\\ 0\\ $的树开始对每个叶子结点穷举所有的可用特征；\n2. 针对每一个特征，把属于该结点的训练样本的该特征升序排列，通过线性扫描的方式来决定该特征的**最佳分裂点**，并采用最佳分裂点时的**收益**；\n3. 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该结点生成出左右两个新的叶子结点，并为每个新结点关联新的样本集；\n4. 退回到第一步，继续递归操作直到满足特定条件。\n\n因为对某个结点采取的是二分策略，分别对应左子结点和右子结点，除了当前待处理的结点，其他结点对应的$\\ Obj \\ $值都不变，所以对于收益的计算只需要考虑当前结点的$\\ Obj \\ $值即可，分裂前针对该结点的最优目标函数为：\n$$\nObj^{(before)}=-\\dfrac{1}{2}\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda}+\\gamma\n$$\n分裂后的最优目标函数为：\n$$\nObj^{(later)}=-\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda} \\bigg]+2\\gamma\n$$\n那么对于该目标函数来说，分裂后的收益为：\n$$\n\\begin{aligned}\nGain&=Obj^{(before)}-Obj^{(later)}\\\\\n&=\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda}-\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda} \\bigg]-\\gamma\n\\end{aligned}\n$$\n故可以用上述公式来决定最有分裂特征和最优特征分裂点。\n\n##### 2.2  总结\n\n​\t$XGBoost  \\ $算法的过程可以归纳为：\n\n1. 前向分布算法的每一步都生成一棵决策树；\n2. 拟合该决策树之前，先计算损失函数在每个样本数据上的一阶导$\\ g_i \\ $和二阶导$\\ h_i \\ $；\n3. 通过贪心策略生成一棵决策树，计算每个叶子结点的$\\ G_j\\ $和$\\ H_j\\ $并计算预测值$\\ w\\ $；\n4. 把新生成的决策树$\\ f_k(x)\\ $加入$\\ \\hat{y}^{(i)}_k=\\hat{y}^{(i)}_{k-1}+\\epsilon f_k(x^{(i)}) \\ $，其中$\\ \\epsilon\\ $是学习率主要控制模型的过拟合。\n\n#### 3 $XGBoost\\ $的优缺点\n\n​\t相比于普通的$\\ GBDT \\ $算法$\\ XGBoost\\ $算法的主要优点在于：\n\n- 不仅支持决策树作为基分类器，还支持其它线性分类器；\n- 使用了损失函数的二阶泰勒展开，因此与损失函数更接近，收敛速度更快；\n-  在目标函数中加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、叶子节点权重的  范式。正则项降低了模型的方差，使学习出来的模型更加简单，有助于防止过拟合，这也是$XGBoost$优于传统GBDT的一个特性。；\n- $Shrinkage\\ $也就是之前说的$\\ \\epsilon\\ $，主要用于削弱每棵决策树的影响，让后面有更大的学习空间，实际应用中一般把$\\ \\epsilon\\ $设置的小点，迭代次数设置的大点；\n- 列抽样，$\\ XGBoost\\ $从随机森林算法中借鉴而来，支持列抽样可以降低过拟合，并且减少计算；\n- 支持对缺失值的处理，对于特征值缺失的样本，$\\ XGBoost\\ $可以学习这些缺失值的分裂方向；\n- 支持并行，boosting不是一种串行的结构吗?怎么并行的？注意$XGBoost$的并行不是tree粒度的并行，$XGBoost$也是一次迭代完才能进行下一次迭代的（第$t$次迭代的代价函数里包含了前面$t-1$次迭代的预测值）。$XGBoost$的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），$XGBoost$在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。；\n- 近似算法，决策树结点在分裂时需要穷举每个可能的分裂点，当数据没法全部加载到内存中时，这种方法会比较慢，$\\ XGBoost\\ $提出了一种近似的方法去高效的生成候选分割点。\n\n缺点\n\n- 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；\n- 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。\n\n#### 4. 关于XGBosst的若干问题\n\n##### 4.1 XGBoost与GBDT的联系和区别有哪些？\n\n1. GBDT是机器学习算法，XGBoost是该算法的工程实现。\n2. **正则项：** 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。\n3. **导数信息：** GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。\n4. **基分类器：** 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。\n5. **子采样：** 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。\n6. **缺失值处理：** 传统GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。\n7. **并行化：** 传统GBDT没有进行并行化设计，注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。\n\n##### 4.2 为什么XGBoost泰勒二阶展开后效果就比较好呢？\n\n（1）**从为什么会想到引入泰勒二阶的角度来说（可扩展性）：** XGBoost官网上有说，当目标函数是`MSE`时，展开是一阶项（残差）+二阶项的形式，而其它目标函数，如`logistic loss`的展开式就没有这样的形式。为了能有个统一的形式，所以采用泰勒展开来得到二阶项，这样就能把`MSE`推导的那套直接复用到其它自定义损失函数上。简短来说，就是为了统一损失函数求导的形式以支持自定义损失函数。至于为什么要在形式上与`MSE`统一？是因为`MSE`是最普遍且常用的损失函数，而且求导最容易，求导后的形式也十分简单。所以理论上只要损失函数形式与`MSE`统一了，那就只用推导`MSE`就好了。\n\n（2）**从二阶导本身的性质，也就是从为什么要用泰勒二阶展开的角度来说（精准性）：** 二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法中已经证实。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。简单来说，相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数。\n\n##### 4.3 XGBoost对缺失值是怎么处理的？\n\n在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。\n\n#### 5 代码实现\n\nscala代码实现地址如下：\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/XGBoost.scala\n\nScala代码实现参考了下面的python代码：\n\nhttps://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/xgboost.py\n\n#### 6 参考文献 \n\n- 陈天奇论文原文 XGBoost: A Scalable Tree Boosting System\n- 深入理解 XGBoost：Kaggle 最主流的集成算法\n\n"},{"title":"梯度提升树","url":"/2020/03/18/梯度提升树/","content":"提升树是分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。\n\n##### 1.1 提升树模型\n\n提升树的思想可以用一个通俗的例子解释，假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。\n\n提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。提升树模型可以表示成决策树的加法模型。\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n​\t其中，$T(x;\\Theta_m)$表示决策树；$\\Theta_m$表示决策树的参数；$M$为树的个数.\n\n##### 1.2 提升树算法\n\n提升树算法采取前向分步算法。首先确定初始提升树$f_0(x) = 0$,第$m$步的模型是\n$$\nf_m(x) = f_{m-1}(x)+T(x;\\Theta_m)\n$$\n其中$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$,\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n由于树的线性组合可以很好的拟合训练数据，即数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。\n\n下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的一般决策问题。\n\n对于二分类分类问题，提升树算法只需要将$Adaboost$算中基本分类器限制为二类分类树即可，可以说这时的提升树算法是$Adaboost$算法的特殊情况，这里不再详细叙述。下面重点叙述回归问题的提升树。\n\n已知一个训练数据集$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间。如果将输入空间$\\chi$划分成$J$个互不相交的区域$R_1,R_2,...,R_J$，并且在每个区域上确定输出的常量$c_j$,那么树可以表示为\n$$\nT(x;\\Theta) = \\sum_{j=1}^{J}c_jI(x \\in R_j)\n$$\n其中参数$\\Theta={(R_1,c_1),(R_2,c_2),...,(R_J,c_J)}$表示树的区域划分和各区域上常数，$J$是回归树的复杂度即叶子节点的个数。\n\n回归问题提升树使用以下前向分布算法：\n$$\n\\begin{aligned}\nf_0(x) &= 0\\\\\nf_m(x) &= f_{m-1}(x) + T(x;\\Theta_m),m = 1,2,...,M \\\\\nf_M(x) &= \\sum_{m=1}^{M}T(x;\\Theta_m) \\\\\n\\end{aligned}\n$$\n在前向分布算法的第$m$步，给定当前模型$f_{m-1}(x)$，需求解\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\n得到$\\hat{\\Theta}_m$,即第$m$棵树的参数。\n\n当采用平方损失函数时，\n$$\nL(y,f(x)) = (y - f(x))^{2}\n$$\n其损失变为\n$$\n\\begin{aligned}\nL(y,f_{m-1}(x) - T(x;\\Theta_m)) &=(y - f_{m-1}(x) - T(x;\\Theta_m))^{m}\\\\\n &= (r - T(x;\\Theta_m))^2\n\\end{aligned}\n$$\n这里,\n$$\nr = y - f_{m-1}(x)\n$$\n是当前模型拟合数据的**残差(residual)**,所以，对回归问题的提升树来说，只需要简单地拟合当前模型的残差。这样，算法是相当简单。 \n\n**算法1 回归问题的提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出：提升树$f_M(x)$\n\n(1) 初始化$f_0(x) = 0$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 按照式$r = y - f_{m-1}(x)$计算残差\n$$\nr_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N\n$$\n​     (b) 拟合残差$r_{mi}$学习一棵回归树，得到$T(x;\\Theta_m)$\n\n​\t (c) 更新$f_m(x) = f_{m-1}(x) + T(x;\\Theta_m)$\n\n(3) 得到回归问题提升树\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n\n##### 1.3 梯度提升\n\n梯度提升(Gradient Boosting）是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中。提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失时，每一步优化是很简单的。对于一般的损失函数而言，往往每一步优化并不是那么容易。针对这一问题，Freidman提出了梯度提升(gradient boosting)算法。这是利用最速下降法的近似方法，**其关键是利用损失函数的负梯度在当前模型的值**：\n$$\n-[\\frac{\\partial{L(y,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n**作为回归问题提升树算法中残差的近似值**，拟合一个回归树。\n\n**算法2 梯度提升树算法**\n\n输入：训练数据$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$为输入空间，$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$为输出空间\n\n输出: 回归树$\\hat{f}(x)$\n\n(1) 初始化$f_0(x) = \\arg \\min_{c}\\sum_{i=1}^{N}L(y_i,c)$\n\n(2) 对$m=1,2,3,...,M$\n\n​     (a) 对$i = 1,2, ...,N$计算残差\n$$\nr_{mi} = -[\\frac{\\partial{L(y_i,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n​     (b) 拟合$r_{mi}$学习一棵回归树，得到第$m$棵树的叶节点区域$R_{mj},j  = 1,2,...,J$\n\n​\t (c) 对于$j  = 1,2,...,J$，计算\n$$\nc_{mj} = \\arg \\min_{c}\\sum_{x_i \\in R_{mj}}L(y_i,f_{m-1}(x_i) +c)\n$$\n​    (d）更新$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^Jc_{mi}I(x \\in R_{mj})$\n\n(3) 得到回归问题提升树\n$$\n\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x\\in R_{mj})\n$$\n算法的第一步初始化，估计使损失函数极小化的常数值，它只有一个根节点的树，第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计，对于平方损失函数来说，它的负梯度其实就是常说的残差，对于一般的损失函数，它就是残差的近似值。第2（b)步，估计回归树叶子节点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶子终点区域的值，使损失函数极小化。第2（d）步更新回归树。第3步，得到输出的最终模型。\n\n##### 1.4 提升树主要损失函数\n\n下面我们对提升树所用的损失函数做一个总结：\n\n1) 对于分类算法来说：其损失函数一般有对数损失函数和指数损失函数：\n\na）**指数损失函数**\n$$\nL(y_i,f(x_i)) = exp(-y_if(x_i))\n$$\n其负梯度误差为：\n$$\n-y_i.exp(-f(x_i))\n$$\nb）**对数损失函数**\n$$\nL(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))\n$$\n其负梯度为：\n$$\n\\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}\n$$\n化简为：\n$$\n\\frac{y_i}{(1+exp(y_if(x_i)))}\n$$\n2) 回归算法：常见的有以下四种\n\n1. **均方差损失函数**\n   $$\n   L(y_i,f(x_i)) = (y_i - f(x_i))^2\n   $$\n   其负梯度为：\n   $$\n   y_i - f(x_i)\n   $$\n   p.s. 损失函数为$L(y,f(x))=(y-f(x))^2$,我们需要最小化$J= \\sum_iL(y_i,f(x_i))$通过调整$f(x_1),f(x_2),...,f(x_n)$.我们把$f(x_i)$当成参数并求导\n   $$\n   \\frac{\\partial}{\\partial f(x_i)} = \\frac{\\partial \\sum_iL(y_i,f(x_i))}{\\partial f(x_i)} = \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} = f(x_i) - y_i\n   $$\n   所以，**我们在均方差损失函数下，可以把残差理解成负梯度。**\n   $$\n   y_i-f(x_i) = - \\frac{\\partial}{\\partial f(x_i)}\n   $$\n   \n2. 绝对损失函数：\n   $$\n   L(y_i,f(x_i) = |y_i - f(x_i)|\n   $$\n   其对应的负梯度为：\n   $$\n   sign (y_i - f(x_i))\n   $$\n\n3. Huber损失函数：它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。损失函数如下：\n   $$\n   \\begin{aligned}\n   L(y_i,f(x_i)) = \n   \\begin{cases} \n   \\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta(|y_i -f(x_i)| - \\frac{\\delta}{2}), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n   其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   y_i-f(x_i), |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n\n4. 分位数损失。它对应的是分位数回归的损失函数，表达式为\n   $$\n   L(x_i,f(x_i)) = \\sum_{y_i \\geq f(x_i)}\\theta|y_i - f(x_i)| + \\sum_{y_i < f(x_i)}(1-\\theta)|y_i - f(x_i)| \n   $$\n   其中，$\\theta$为分位数，需要在回归钱设置，其对应的负梯度为：\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   \\theta,  y_i \\geq f(x_i)\\\\\n   \\theta - 1, y_i < f(x_i)\n   \\end{cases}\n   \\end{aligned}\n   $$\n   ![](.\\梯度提升树\\损失函数.png)\n\n#####    1.5 总结及优缺点\n\n本文介绍了boosting族的提升树算法和梯度提升树（GBDT)算法，提升树算法的每轮弱学习器是拟合上一轮的残差生成的，GBDT算法的每轮弱学习器是拟合上一轮损失函数的负梯度生成的。提升树算法和GBDT算法都是用CART回归树作为弱学习器，只要确定模型的损失函数，提升树和GBDT就可以通过前向分布算法进行构建。\n\n\n\n梯度提升树主要的优点有：\n\n1） 可以灵活处理各种类型的数据，包括连续值和离散值。\n\n2）在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。\n\n3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。\n\n梯度提升树的主要缺点有：\n\n1）由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。\n\n##### 1.6 代码实现\n\nscala代码地址:https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/ensemble/GBDT.scala\n\n##### 1.7 参考文献\n\n李航 《统计学习》\n\n"},{"title":"逻辑回归分类和softmax分类","url":"/2020/02/12/逻辑回归分类和softmax分类/","content":"### 逻辑回归分类和softmax分类\n\n#### 1.逻辑回归\n\n##### 1.1 算法原理\n\n一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。\n\n逻辑回归模型概率估算:\n$$\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdot x)$$\n\n逻辑函数：\n$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$\n\n预测模型：\n$$\\hat{y}=\n\\begin{cases}\n0 & (\\hat{p}<0.5)\\\\\n1 & (\\hat{p}\\geq0.5)\n\\end{cases}$$\n\n单个训练实例的损失函数:\n$$c(\\theta)=\n\\begin{cases}\n-log(\\hat{p}) & (y=1)\\\\\n-log(1-\\hat{p}) & (y=0)\n\\end{cases}$$\n\n我们可以看到，当$p$接近于$0$的时候，$-\\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。\n\n逻辑回归成本函数:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$$\n\n坏消息是，这个函数没有已知的闭式方程(也就是不尊在一个标准方差的等价方程)。好消息，这是个凸函数，通过梯度下降算法保证能够找出全局最小值。\n\nLogistic损失函数的偏导数:\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^T \\cdot x^{(i)})-y^{(i)})x_j^{(i)}$$\n\n$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^T\\cdot x^{i}-y^{i})x_j^{i}$$\n\n##### 1.2 scala 代码实现\n\n```scala\npackage ml.scrath.classification\n\nimport scala.collection.mutable.ArrayBuffer\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n\n\nobject LogitRegression{\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map{_.split(\",\").filter(_.length() > 0).map(_.toDouble)}\n      .toArray\n    val data = BDM(dataS:_*)\n\n    val features = data(0 to 98, 0 to 3)\n    val labels = data(0 to 98, 4)\n\n    val model = new LogitRegression\n    val w = model.fit(features,labels)\n    val predictions = model.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate)\n  }\n}\n\n\nclass LogitRegression (var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = (x_train * weights).map(sigmoid(_))\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def sigmoid(inX: Double) = {\n    1.0 / (1 + scala.math.exp(-inX))\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = (x_test * weights).map(sigmoid(_)).map(x => if(x > 0.5) 1.0 else 0.0)\n    output\n  }\n\n}\n```\n\n\n\n##### 1.3 python 代码实现\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # 为X增加一列特征x1，x1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # 梯度训练n_iterations轮\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n\n\nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X = data.data[data.target != 0]\n    y = data.target[data.target != 0]\n    y[y == 1] = 0\n    y[y == 2] = 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    Plot().plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n```\n\nPython结果展示如下【基于PCA将高维数据投影而得】：\n\n![](./逻辑回归分类和softmax分类/logistic.png)\n\n#### 2.softmax回归\n\n##### 2.1 算法原理和步骤\n\n对逻辑回归模型做推广，可以支持多个类别了。\n\n原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。\n\n1. 用零（或小的随机值）初始化权重矩阵和偏置值.\n\n2. 对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:\n   $$s_k(x)= x\\cdot w_k $$\n\n   向量化表示上式的话，可以写为\n\n   $$ socres = X \\cdot W $$\n\n   $X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.\n\n3. 应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:\n   $$\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(x))}$$\n\n4.  计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:\n   $$J(W)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}log(\\hat{p}_k^{(i)})$$\n\n5. 对于类别k的交叉熵梯度向量:\n   $$\\Delta_{w_k}J(W)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}$$\n\n6. 更新每个类的权重$W$\n\n   $$w_k = w_k - \\eta \\Delta_{w_k}J$$\n\n​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：\n$$H(p,q)=-\\sum_xp(x)logq(x)$$\n\n\n\n##### 2.2 scala 代码\n\n```scala\npackage ml.scrath.classification\n\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, _}\nimport breeze.numerics._\n\nobject softMax {\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map {\n        _.split(\",\").filter(_.length() > 0).map(_.toDouble)\n      }\n      .toArray\n    val data = BDM(dataS: _*)\n    val features = data(::, 0 to 3)\n    val labels = data(::, 4)\n\n    val soft = new SoftMaxRegression()\n    val w = soft.fit(features, labels)\n    println(w)\n    val predictions = soft.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate) // 正确率为0.9664\n\n  }\n}\n\nclass SoftMaxRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y: BDV[Double]): BDM[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n\n    val ncol = x_train.cols\n    val nclasses = y.toArray.distinct.length\n    var weights = BDM.ones[Double](ncol, nclasses) :* 1.0 / nclasses\n    val n_samples = x_train.rows\n\n    for (iterations <- 0 to num_iters) {\n      val logits = x_train * weights\n      val probs = softmax(logits)\n      val y_one_hot = one_hot(y)\n//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble\n      val error: BDM[Double] = probs - y_one_hot\n      val gradients = (x_train.t * error) :/ n_samples.toDouble\n\n      weights -= gradients :* lr\n    }\n    weights\n  }\n\n  def softmax(logits: BDM[Double]): BDM[Double] = {\n    val scores = exp(logits)\n    val divisor = sum(scores(*, ::))\n    for (i <- 0 to scores.cols - 1) {\n      scores(::, i) := scores(::, i) :/ divisor\n    }\n    scores\n  }\n\n  def one_hot(y: BDV[Double]): BDM[Double] = {\n    val n_samples = y.length\n    val n_classes = y.toArray.toSet.size\n    val one_hot = Array.ofDim[Double](n_samples, n_classes)\n    for (i <- 0 to n_samples - 1) {\n      one_hot(i)(y(i).toInt) = 1.0\n    }\n    BDM(one_hot: _*)\n  }\n\n  def predict(weights: BDM[Double], x: BDM[Double]): BDV[Int] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_test = BDM.horzcat(ones, x)\n    val predictions = argmax(x_test * weights, Axis._1)\n    predictions\n  }\n\n}\n```\n\n\n\n##### 2.3 python 代码\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 12 11:58:06 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\nclass SoftmaxRegressorII:\n\n    def __init__(self,learning_rate = 0.1,n_iters = 1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n\n    def train(self, X, y_true, n_classes):\n\n        x_train = np.column_stack((np.ones(len(X)),X))\n        \n        self.n_samples, n_features = x_train.shape\n        self.n_classes = n_classes\n        \n        self.weights = np.random.rand(n_features,self.n_classes)\n        all_losses = []\n        \n        for i in range(self.n_iters):\n            logits = np.dot(x_train, self.weights)\n            probs = self.softmax(logits)\n            y_one_hot = self.one_hot(y_true)\n            loss = self.cross_entropy(y_one_hot, probs)\n            all_losses.append(loss)\n\n            gradients = (1 / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))\n\n            self.weights = self.weights - self.learning_rate * gradients\n\n#            if i % 100 == 0:\n#                print(f'Iteration number: {i}, loss: {np.round(loss, 4)}')\n\n        return self.weights, all_losses\n\n    def predict(self, X):\n\n        x_test = np.column_stack((np.ones(len(X)), X))\n        scores = np.dot(x_test, self.weights)\n        probs = self.softmax(scores)\n        return np.argmax(probs, axis=1)[:, np.newaxis]\n\n    def softmax(self, logits):\n        exp = np.exp(logits)\n        sum_exp = np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        return exp / sum_exp\n\n    def cross_entropy(self, y_true, scores):\n        loss = - (1 / self.n_samples) * np.sum(y_true * np.log(scores))\n        return loss\n\n    def one_hot(self, y):\n        one_hot = np.zeros((self.n_samples, self.n_classes))\n        one_hot[np.arange(self.n_samples), y.T] = 1\n        return one_hot\n    \nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X= data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = SoftmaxRegressorII()\n    ll = clf.train(X_train, y_train,3)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Reduce dimension to two using PCA and plot the results\n    Plot().plot_in_2d(X_test, y_pred, title=\"SoftMax Regression\", accuracy=accuracy)\n```\n\n结果如图所示：\n\n![](./逻辑回归分类和softmax分类/softmax.png)"},{"title":"降维_线性判别分析","url":"/2020/01/16/降维_线性判别分析/","content":"#### 1. 算法概述\n\nLDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”，如下图所示。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。\n\n![](./降维_线性判别分析/lda_1.png)\n\n#### 2. 算法推导\n\nLDA多分类：假定存在$N$个类，且第$i$类示例树为$m_i$,我们定义全局散度矩阵：\n$$\nS_t = S_b + S_w= \\sum_{i= 1}^{m}(x_i - \\mu)(x_i - \\mu)^T\n$$\n其中$\\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即\n$$\nS_w = \\sum_{i=1}^{N}S_{w_i}\n$$\n其中\n$$\nS_{w_i} = \\sum_{x \\in X_i}(x - \\mu_i)(x - \\mu_i)^T\n$$\n由式$(1)-(3)$可得：\n$$\nS_b = S_t - S_w = \\sum_{i=1}^{N}(\\mu_i - \\mu)(\\mu_i - \\mu)^T\n$$\n多分类LDA有多种实现方式：使用$S_b,S_w,S_t$中的任意两即可。常见的一种实现是采用优化目标\n$$\n\\max_W \\dfrac{tr(W^TS_bW)}{tr(W^{T}S_wW)}\n$$\n其中$W \\in R^{d\\times(N-1)}$,$tr(.)$表示矩阵的迹。式$(5)$可以通过求解如下式的广义特征问题\n$$\nS_bW = \\lambda S_wW\n$$\n$W$的闭式解则是$S_w^{-1}S_b$的N-1个广义特征值所对应的特征向量组成的矩阵。\n\n​\t\t若将$W$视为一个投影矩阵，则多分类LDA将样本矩阵投影到$N-1$维空间。$N-1$通常原小于数据原来的维数，且投影过程中使用了类别信息，因此LDA也被视为一种数据降维的技术。\n\n#### 3. 实现步骤\n\n1. 对于每一类别，计算$d$维数据的均值向量；\n2. 构造类间散度矩阵$S_b$和类内散度矩阵$S_w$;\n3. 计算矩阵$S_w^{-1}S_b$的特征值及对应的特征向量；\n4. 选取前$k$特征值所对应的特征向量，构造$d \\times k$维的转换矩阵$W$,其中特征值以列的形式排列；\n5. 使用转换矩阵$W$将样本映射到新的特征子空间上。\n\n#### 4. LDA与PCA\n\nLDA和PCA都可以用作降维技术，下面比较一下相同点和不同点：\n\n##### 4.1 相同点\n\n​\t1）两者均可以对数据进行降维;\n\n​\t2）两者在降维时均使用了矩阵特征分解的思想;\n\n​\t3）两者都假设数据符合高斯分布.\n\n##### 4.2 不同点\n\n​\t1） LDA是有监督的降维技术，PCA是无监督的降维技术；\n\n​\t2） LDA还可以用于分类，后续在分类时，贴上分类的处理方法；\n\n​\t3） LDA最多可降低到$k$维（$k$是分类的个数),而PCA最多可降低到$n-1$维（$n$是数据的维数)；\n\n​\t4） LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。\n\n#### 5.  LDA的实现代码\n\n##### 5.1 Scala lda实现\n\n```scala\npackage ml.scrath.lda\n\nimport breeze.linalg._\nimport breeze.stats._\nimport org.apache.spark.rdd.RDD\n\nclass LinearDiscriminantAnalysis extends Serializable {\n\n  def fit(data: RDD[DenseVector[Double]], labels: RDD[Int],k:Int) = {\n    val sample = labels.zip(data)\n    computeLDA(sample,k)\n  }\n\n  def computeLDA(dataAndLabels: RDD[(Int, DenseVector[Double])],k:Int)= {\n\n    val featuresByClass = dataAndLabels.groupBy(_._1).values.map(x => rowsToMatrix(x.map(_._2)))\n    val meanByClass = featuresByClass.map(f => mean(f(::, *))) // 对行向量求平均值 each mean is a row vector, not col\n\n    //类内散度矩阵\n    val Sw = featuresByClass.zip(meanByClass).map(f => {\n      val featuresMinusMean: DenseMatrix[Double] = f._1(*, ::) - f._2.t // row vector, not column\n      featuresMinusMean.t * featuresMinusMean: DenseMatrix[Double]\n    }).reduce(_+_)\n\n    val numByClass = featuresByClass.map(_.rows : Double)\n    val features = rddToMatrix(dataAndLabels.map(_._2))\n    val totalMean = mean(features(::, *)) // A row-vector, not a column-vector\n\n    val Sb = meanByClass.zip(numByClass).map {\n      case (classMean, classNum) => {\n        val m = classMean - totalMean\n        (m.t * m : DenseMatrix[Double]) :* classNum : DenseMatrix[Double]\n      }\n    }.reduce(_+_)\n\n    val eigen = eig((inv(Sw): DenseMatrix[Double]) * Sb)\n\n    val eigenvectors = (0 until eigen.eigenvectors.cols).map(eigen.eigenvectors(::, _).toDenseMatrix.t)\n\n    val topEigenvectors = eigenvectors.zip(eigen.eigenvalues.toArray).sortBy(x => -scala.math.abs(x._2)).map(_._1).take(k)\n    val W = DenseMatrix.horzcat(topEigenvectors:_*)\n    (W,Sb,Sw)\n  }\n\n  def rowsToMatrix(in: TraversableOnce[DenseVector[Double]]): DenseMatrix[Double] = {\n    rowsToMatrix(in.toArray)\n  }\n\n  def rowsToMatrix(inArr: Array[DenseVector[Double]]): DenseMatrix[Double] = {\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n\n\n  def rddToMatrix(inArr1: RDD[DenseVector[Double]]): DenseMatrix[Double] = {\n    val inArr = inArr1.collect()\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n}\n```\n\n##### 5.2 scala 测试代码\n\n测试代码（其中数据iris.csv是由下面python代码生成）\n\n```scala\npackage ml.scrath.lda\n\nimport org.apache.spark.sql.SparkSession\nimport breeze.linalg.DenseVector\n\nobject TestLDA extends App {\n\n  val spark =\n    SparkSession.builder()\n      .appName(\"DataFrame-Basic\")\n      .master(\"local[4]\")\n      .config(\"spark.sql.warehouse.dir\", \"file:///E:/spark-warehouse\")\n      .getOrCreate()\n  val sc = spark.sparkContext\n  val irisData = sc.textFile(\"D:\\\\data\\\\iris.csv\")\n\n  val trainData = irisData.map {\n    _.split(\",\").dropRight(1).map(_.toDouble)\n  }.map(new DenseVector(_))\n\n  val labels = irisData.map {\n    _.split(\",\").apply(4).map(_.toInt).apply(0)\n  }\n\n  val start = System.currentTimeMillis()\n  val model = new LinearDiscriminantAnalysis\n  val k = 2\n  val res = model.fit(trainData, labels, k)\n\n  println(\"=====W======\")\n  println(res._1)\n  println(\"=======Sb====\")\n  println(res._2)\n  println(\"=======Sw====\")\n  println(res._3)\n\n}\n```\n\n##### 5.3 scala 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$\n\n![](./降维_线性判别分析/scala_lda_res.png)\n\n##### 5.4 Python lda 代码\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef LDA(X, y, k):\n    '''\n    X为数据集，y为label，k为目标维数\n    '''\n    label_ = np.unique(y)\n    mu = np.mean(X, axis=0)\n\n    Sw = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sw += np.dot((_X - _mean).T,\n                     _X - _mean)\n\n    print(Sw)\n    \n    Sb = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sb += len(_X) * np.dot(( _mean - mu).reshape(\n            (len(mu), 1)), (_mean - mu).reshape((1, len(mu))))\n        \n    print(Sb)\n\n    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # 计算Sw-1*Sb的特征值和特征矩阵\n\n    sorted_indices = np.argsort(eig_vals)\n    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]]  # 提取前k个特征向量\n    return topk_eig_vecs,Sb,Sw\n\n\nif '__main__' == __name__:\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n## 将数据写出到iris.csv文件，供scala使用，保证数据的一致性。\n    data_cat =  np.c_[X,y]\n    import pandas\n    iris_df = pandas.DataFrame(data_cat)\n    iris_df.to_csv(\"iris.csv\",index = 0,header = False)\n\n    W,Sb,Sw = LDA(X, y, 2)\n    X_new = np.dot((X), W)\n    plt.figure(1)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    \n    print(\"========W=========\")\n    print(W)\n    print(\"========Sb========\")\n    print(Sb)\n    print(\"========Sw========\")\n    print(Sw)\n    \n    \n    # 与sklearn中的LDA函数对比\n    lda = LinearDiscriminantAnalysis(n_components=2)\n    lda.fit(X, y)\n    X_new = lda.transform(X)\n#    print(X_new)\n    plt.figure(2)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    plt.show()\n\n```\n\n##### 5.5 Python 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$,可见scala和python得到结果是一致的。\n\n![](./降维_线性判别分析/python_lda_res.png)\n\n"},{"title":"线性回归","url":"/2020/01/09/线性回归/","content":"### 线性回归的代码实现\n\n#### 1. 原理\n\n多元线性回归的损失函数为：\n$$\nJ=\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2\n$$\n其中：$\\hat{y}^{(i)} = \\theta_{0} + \\theta_{1}X_{1}^{(i)} + \\theta_{2}X_{2}^{(i)} + ... + \\theta_{n}X_{n}^{(i)}$ 。\n\n对 $J$ 求导为：\n$$\n\\nabla J=(\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})\n$$\n其中：$\\frac{\\partial J}{\\partial \\theta_i}$ 为偏导数，与导数的求法一样。\n\n\n\n对 $\\nabla J$ 进一步计算：\n$$\n\\nabla J(\\theta) =  \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} =   \\begin{pmatrix} \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-1) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_1^{(i)}) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_2^{(i)}) \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_n^{(i)}) \\end{pmatrix} = 2·\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n\n其中：$X_b = \\begin{pmatrix}\n1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n1 & X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)}\n\\end{pmatrix}$\n\n\n\n​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：\n$$\n\\nabla J(\\theta)  = \\frac{2}{m}\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n​        此时，目标函数就成了使 $\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：\n$$\nJ(\\theta) = MSE(y, \\hat{y})\n$$\n​\t\t\n\n注1. 有时候目标函数也去 $\\frac{1}{\\boldsymbol{2}m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ ,其中的**2**是为了抵消梯度中的2，实际的效果有限。\n\n注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。\n\n\n\n#### 2.实现代码\n\n##### 2.1 python版本实现\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  8 17:25:39 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\n\nclass LinearRegression():\n    def __init__(self,lr=.01, num_iters=10000,tolerance=1e-8):\n        self.lr = lr\n        self.num_iters = num_iters\n        self.w = None\n        self.toterance = tolerance\n\n    def fit(self,x_train, y_train):\n        n_samples = len(x_train)\n        x_train = np.column_stack((np.ones(n_samples), x_train)) #也可以写成np.c_\n        \n        self.w = .01 * np.ones(x_train.shape[1])\n        self.loss_ = [0]\n        \n        # w_{i} = w_{i} - lr * (h(w_{i}) - y)*x_{i} 迭代公式\n        self.count = 0\n        for iteration in range(self.num_iters):\n            self.count += 1\n            raw_output = np.matmul(x_train, self.w)\n            errors =  raw_output - y_train\n            loss = 1/(2 * n_samples) * errors.dot(errors)\n            delta_loss = loss - self.loss_[-1]\n\n            self.loss_.append(loss)\n            if np.abs(delta_loss) < self.toterance:\n                break\n            else:\n                grad = (1.0 /n_samples) *np.matmul(x_train.T, np.array(errors))\n                self.w -= self.lr * grad\n\n    def predict(self,x_test):\n        x_test = np.column_stack((np.ones(len(x_test)), x_test))\n        \n        output = np.matmul(x_test, self.w)\n        return output\n    \nif __name__ == '__main__':\n    \n    num_inputs = 2\n    num_examples = 10000\n    true_w = [6.4, -3.2]\n    true_b = 2.3\n    features = np.random.random((num_examples, num_inputs))\n    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n    labels += np.random.normal(0, 0.1,size = len(labels))\n    \n    import sklearn.model_selection\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    print(\"回归系数:\",lr.w)\n    print(\"迭代次数:\",lr.count)\n    \n    y_pred = lr.predict(x_test)\n    from sklearn import metrics\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    print(\"MSE: %.4f\" % mse)\n\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    print(\"MAE: %.4f\" % mae)\n\n    R2 = metrics.r2_score(y_test,y_pred)\n    print(\"R2: %.4f\" % R2)\n```\n\n##### 2.2 Scala版本实现\n\n```scala\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n * Scala 版本的实现\n */\n\nobject LinearRegression {\n  def main(args: Array[String]): Unit = {\n    val num_inputs = 2\n    val num_examples = 1000\n    val x_train: BDM[Double] = BDM.rand(num_examples, num_inputs)\n    val ones = BDM.ones[Double](num_examples, 1)\n    val x_cat = BDM.horzcat(ones, x_train)\n    val y_train = x_cat * BDV(2.3, 6.4, -3.2)\n\n    val model = new LinearRegression(num_iters = 10000)\n    val weights = model.fit(x_train, y_train)\n    val predictions = model.predict(weights, x_train)\n    println(\"梯度下降求解的权重为：\" + weights)\n    println(predictions)\n  }\n}\n\nclass LinearRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = x_train * weights\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = x_test * weights\n    output\n  }\n}\n```\n\n参考文献：1. [机器学习之梯度下降法与线性回归](https://segmentfault.com/a/1190000017048213)\n\n\n\n"},{"title":"神经网络","url":"/2019/11/27/神经网络-1/","content":"\n### 神经网络的numpy实现和公式推导\n\n\n\n\n\n​       过去10多年是神经网络发展的黄金时期，神经网络(深度学习)成为了新时代的一种浪潮，所以今天借用国外一个小哥实现纯numpy的神经网络，来记录神经网络的实现过程。\n\n![](./神经网络-1/nn_architecture.png)\n\n\n\n#### 概览\n\n​        在开始编程之前，先让我们准备一份基本的路线图。我们的目标是创建一个特定架构（层数、层大小、激活函数）的密集连接神经网络。然后训练这一神经网络并做出预测。\n\n![](./神经网络-1/blueprint.gif)\n\n上面的示意图展示了训练网络【特别是正向传播和反向传播的操作】时进行的操作，以及单次迭代不同阶段需要更新和读取的参数。\n\n#### 初始化神经网络层\n\n  让我们从初始化每一层的权重矩阵$W$和偏置向量$b$开始。下图展示了网络层l的权重矩阵和偏置向量，其中，上标$[l]$表示当前层的索引，$n$表示给定层中的神经元数量。\n\n![](./神经网络-1/params_sizes.png)\n\n我们的程序也将以类似的列表形式描述神经网络架构。列表的每一项是一个字典，描述单个网络层的基本参数：`input_dim`是网络层输入的信号向量的大小，`output_dim`是网络层输出的激活向量的大小，`activation`是网络层所用的激活函数。\n\n```python\nnn_architecture = [\n    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n]\n```\n\n值得注意的是，一个网络层的输出向量同时也是下一层的输入。\n\n```python\ndef init_layers(nn_architecture, seed = 99):\n    np.random.seed(seed)\n    number_of_layers = len(nn_architecture)\n    params_values = {}\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        layer_input_size = layer[\"input_dim\"]\n        layer_output_size = layer[\"output_dim\"]\n\n        params_values['W' + str(layer_idx)] = np.random.randn(\n            layer_output_size, layer_input_size) * 0.1\n        params_values['b' + str(layer_idx)] = np.random.randn(\n            layer_output_size, 1) * 0.1\n\n    return params_values\n```\n\n​        上面的代码初始化了网络层的参数。注意我们用随机的小数字填充矩阵**W**和向量**b**。这并不是偶然的。权重值无法使用相同的数字初始化，否则会造成**破坏性的对称问题**。**基本上，如果权重都一样，不管输入X是什么，隐藏层的所有单元也都一样**。这样，我们就会陷入初始状态，不管训练多久，网络多深，都无望摆脱。线性代数不会原谅我们。\n\n[^初始化方法包含很多种，我们这里简便起见，使用随机初始化的方式生成权重]: \n\n​       小数值增加了算法的效率。我们可以看看下面的sigmoid函数图像，大数值处的函数图像几乎是扁平的，这会对神经网络的学习速度造成显著影响。所有参数使用小随机数是一个简单的方法，但它保证了算法有一个**足够好**的开始。\n\n![](./神经网络-1/activations.gif)\n\n#### 激活函数\n\n​        激活函数只需一行代码就可以定义，但它们给神经网络带来了非线性和所需的表达力。**“没有它们，神经网络将变成线性函数的组合，也就是单个线性函数。”**激活函数有很多种，但在这个项目中，我决定使用其中两种——sigmoid和ReLU。为了同时支持前向传播和反向传播，我们还需要准备好它们的导数。\n\n```python\ndef sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0,Z)\n\ndef sigmoid_backward(dA, Z):\n    sig = sigmoid(Z)\n    return dA * sig * (1 - sig)\n\ndef relu_backward(dA, Z):\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0;\n    return dZ;\n```\n\n#### 前向传播\n\n​    我们设计的神经网络有一个简单的架构。输入矩阵**X**传入网络，沿着隐藏单元传播，最终得到预测向量**Y_hat**。为了让代码更易读，我将前向传播拆分成两个函数——单层前向传播，和整个神经网络前向传播。\n\n​\t    **前向传播的过程是：输入$a^{[l-1]}$, 输出$a^{[l]}$, 缓存为$z^{[l]}$,  从方便实现的角度上看，$z^{[l]}$是$w^{[l]}$，$b^{[l]}$的函数。**\n\n```python\ndef single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n    Z_curr = np.dot(W_curr, A_prev) + b_curr\n    if activation is \"relu\":\n        activation_func = relu\n    elif activation is \"sigmoid\":\n        activation_func = sigmoid\n    else:\n        raise Exception('Non-supported activation function')\n\n    return activation_func(Z_curr), Z_curr\n```\n这部分代码大概是最直接，最容易理解的。给定来自上一层的输入信号，我们计算仿射变换**Z**，接着应用选中的激活函数。基于NumPy，我们可以对整个网络层和整批样本一下子进行矩阵操作，无需迭代，这大大加速了计算。除了计算结果外，函数还返回了一个反向传播时需要用到的中间值**Z**。\n\n![](./神经网络-1/matrix_sizes_2.png)\n\n基于单层前向传播函数，编写整个前向传播步骤很容易。这是一个略微复杂一点的函数，它的角色不仅是进行预测，还包括组织中间值。\n$$\n\\begin{aligned}\nz^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}  \\\\\n\na^{[l]} = g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n\n\n```python\ndef full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n\n        activ_function_curr = layer[\"activation\"]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        b_curr = params_values[\"b\" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\n        memory[\"A\" + str(idx)] = A_prev\n        memory[\"Z\" + str(layer_idx)] = Z_curr\n\n    return A_curr, memory\n```\n\n#### 损失函数\n\n​          损失函数可以监测进展，确保我们向着正确的方向移动。**“一般来说，损失函数是为了显示我们离‘理想’解答还有多远。”**损失函数根据我们计划解决的问题而选用，Keras之类的框架提供了很多选项。因为我计划将神经网络用于二元分类问题，我决定使用交叉熵：\n$$\nJ(W,b) = \\dfrac{1}{m}{\\sum}_{i=1}^{m}L(\\hat{y}^{i} - y^{i})  \\\\\nL(\\hat{y} - y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))\n$$\n为了取得更多关于学习过程的信息，我决定另外实现一个计算精确度的函数。\n\n```python\n'''\nJ(W,b) = 1/m sum_{i}^{m}L(y^{hat}_{i} - y_{i})\nL(y^{hat}_{i} - y_{i}) = -(ylogy^{hat} + (1-y)log(1-y^{hat}))\n'''\n\ndef get_cost_value(Y_hat, Y):\n    m = Y_hat.shape[1]\n    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n    return np.squeeze(cost)\n\n# an auxiliary function that converts probability into class\ndef convert_prob_into_class(probs):\n    probs_ = np.copy(probs)\n    probs_[probs_ > 0.5] = 1\n    probs_[probs_ <= 0.5] = 0\n    return probs_\n\ndef get_accuracy_value(Y_hat, Y):\n    Y_hat_ = convert_prob_into_class(Y_hat)\n    return (Y_hat_ == Y).all(axis=0).mean()\n```\n\n#### 反向传播\n\n不幸的是，很多缺乏经验的深度学习爱好者都觉得反向传播很吓人，难以理解。微积分和线性代数的组合经常会吓退那些没有经过扎实的数学训练的人。所以不要过于担心你现在还不能理解这一切。相信我，我们都经历过这个过程。\n\n```python\ndef single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n    # number of examples\n    m = A_prev.shape[1]\n    \n    # selection of activation function\n    if activation is \"relu\":\n        backward_activation_func = relu_backward\n    elif activation is \"sigmoid\":\n        backward_activation_func = sigmoid_backward\n    else:\n        raise Exception('Non-supported activation function')\n    \n    # calculation of the activation function derivative\n    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n    \n    # derivative of the matrix W\n    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n    # derivative of the vector b\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n    # derivative of the matrix A_prev\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n\n    return dA_prev, dW_curr, db_curr\n\n```\n\n​        人们经常搞混反向传播和梯度下降，但事实上它们不一样。前者是为了高效地计算梯度，后者则是为了基于计算出的梯度进行优化。在神经网络中，我们计算损失函数在参数上的梯度，但反向传播可以用来计算任何函数的导数。**反向传播算法的精髓在于递归地使用求导的链式法则，通过组合导数已知的函数，计算函数的导数**。下面的公式描述了单个网络层上的反向传播过程。由于本文的重点在实际实现，所以我将省略求导过程。从公式上我们可以很明显地看到，为什么我们需要在前向传播时记住中间层的**A**、**Z**矩阵的值。\n\n$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} =\\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}} \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{W}^{[l]}} =  \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n\n$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}}  = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}   \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{b}^{[l]}}= \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n\n$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} =  \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}  \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n\n$$\\boldsymbol{dZ}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}= \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l]}} \\frac{\\partial {A}^{[l]} }{\\partial \\boldsymbol{Z}^{[l]}}=\\boldsymbol{dA}^{[l]} * g^{[l]}{'}(\\boldsymbol{Z}^{[l]})$$\n\n![](./神经网络-1/640.webp)\n\n和前向传播一样，我决定将计算拆分成两个函数。之前给出的是单个网络层的反向传播函数，基本上就是以NumPy方式重写上面的数学公式。而定义完整反向传播过程的函数，主要是读取、更新三个字典中的值。\n\n```python\ndef full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n    grads_values = {}\n    \n    # number of examples\n    m = Y.shape[1]\n    # a hack ensuring the same shape of the prediction vector and labels vector\n    Y = Y.reshape(Y_hat.shape)\n    \n    # initiation of gradient descent algorithm\n    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n    \n    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n        # we number network layers from 1\n        layer_idx_curr = layer_idx_prev + 1\n        # extraction of the activation function for the current layer\n        activ_function_curr = layer[\"activation\"]\n        \n        dA_curr = dA_prev\n        \n        A_prev = memory[\"A\" + str(layer_idx_prev)]\n        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n        \n        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n        \n        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n    \n    return grads_values\n```\n\n基于单个网络层的反向传播函数，我们从最后一层开始迭代计算所有参数上的导数，并最终返回包含所需梯度的python字典。\n\n\n\n#### 更新参数值\n\n反向传播是为了计算梯度，以根据梯度进行优化，更新网络的参数值。为了完成这一任务，我们将使用两个字典作为函数参数：`params_values`，其中保存了当前参数值；`grads_values`，其中保存了用于更新参数值所需的梯度信息。现在我们只需在每个网络层上应用以下等式即可。这是一个非常简单的优化算法，但我决定使用它作为更高级的优化算法的起点（大概会是我下一篇文章的主题）。\n\n$$\\boldsymbol{W}^{[l]} = \\boldsymbol{W}^{[l]} - \\alpha \\boldsymbol{dW}^{[l]} $$\n\n$$\\boldsymbol{b}^{[l]} = \\boldsymbol{b}^{[l]} - \\alpha \\boldsymbol{b}^{[l]} $$\n\n```\ndef update(params_values, grads_values, nn_architecture, learning_rate):\n\n    # iteration over network layers\n    for layer_idx, layer in enumerate(nn_architecture, 1):\n        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n\n    return params_values;\n```\n\n#### 整合一切\n\n万事俱备只欠东风。最困难的部分已经完成了——我们已经准备好了所需的函数，现在只需以正确的顺序把它们放到一起。\n\n```python\ndef train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n    # initiation of neural net parameters\n    params_values = init_layers(nn_architecture, 2)\n    # initiation of lists storing the history \n    # of metrics calculated during the learning process \n    cost_history = []\n    accuracy_history = []\n    \n    # performing calculations for subsequent iterations\n    for i in range(epochs):\n        # step forward\n        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n        \n        # calculating metrics and saving them in history\n        cost = get_cost_value(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        # step backward - calculating gradient\n        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n        # updating model state\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        if(i % 50 == 0):\n            if(verbose):\n                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n            if(callback is not None):\n                callback(i, params_values)\n            \n    return params_values\n```\n\n\n"}]