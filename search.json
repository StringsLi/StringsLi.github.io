[{"title":"leetcodeç®—æ³•æ€»ç»“ç³»åˆ—1-å †","url":"/2020/06/29/leetcodeç®—æ³•æ€»ç»“ç³»åˆ—1-å †/","content":"\n\n### Leetcodeç®—æ³•æ€»ç»“ç³»åˆ—1-å †\n\n[TOC]\n\n##### 1.0 å †çš„ä»‹ç»\n\nå †ï¼ˆHeapï¼‰æ˜¯ä¸€ä¸ªå¯ä»¥è¢«çœ‹æˆè¿‘ä¼¼å®Œå…¨äºŒå‰æ ‘çš„æ•°ç»„ã€‚æ ‘ä¸Šçš„æ¯ä¸€ä¸ªç»“ç‚¹å¯¹åº”æ•°ç»„çš„ä¸€ä¸ªå…ƒç´ ã€‚é™¤äº†æœ€åº•å±‚å¤–ï¼Œè¯¥æ ‘æ˜¯å®Œå…¨å……æ»¡çš„ï¼Œè€Œä¸”æ˜¯ä»å·¦åˆ°å³å¡«å……ã€‚â€”â€” æ¥è‡ªï¼šã€Šç®—æ³•å¯¼è®ºã€‹\n\nå †åŒ…æ‹¬æœ€å¤§å †å’Œæœ€å°å †ï¼šæœ€å¤§å †çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆé™¤äº†æ ¹ç»“ç‚¹ï¼‰çš„å€¼ä¸å¤§äºå…¶çˆ¶èŠ‚ç‚¹ï¼›æœ€å°å †çš„æ¯ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆé™¤äº†æ ¹ç»“ç‚¹ï¼‰çš„å€¼ä¸å°äºå…¶çˆ¶èŠ‚ç‚¹ã€‚\n\nå †å¸¸è§çš„æ“ä½œï¼š\n\nHEAPIFY å»ºå †ï¼šæŠŠä¸€ä¸ªä¹±åºçš„æ•°ç»„å˜æˆå †ç»“æ„çš„æ•°ç»„ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(n)$ã€‚\nHEAPPUSHï¼šæŠŠä¸€ä¸ªæ•°å€¼æ”¾è¿›å·²ç»æ˜¯å †ç»“æ„çš„æ•°ç»„ä¸­ï¼Œå¹¶ä¿æŒå †ç»“æ„ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(log\\ n)$ã€‚\nHEAPPOPï¼šä»æœ€å¤§å †ä¸­å–å‡ºæœ€å¤§å€¼æˆ–ä»æœ€å°å †ä¸­å–å‡ºæœ€å°å€¼ï¼Œå¹¶å°†å‰©ä½™çš„æ•°ç»„ä¿æŒå †ç»“æ„ï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(log\\ n)$ã€‚\nHEAPSORTï¼šå€Ÿç”± HEAPFY å»ºå †å’Œ HEAPPOP å †æ•°ç»„è¿›è¡Œæ’åºï¼Œæ—¶é—´å¤æ‚åº¦ä¸º $O(n\\ log\\ n)$ï¼Œç©ºé—´å¤æ‚åº¦ä¸º $O(1)$ã€‚\nå †ç»“æ„çš„ä¸€ä¸ªå¸¸è§åº”ç”¨æ˜¯å»ºç«‹ä¼˜å…ˆé˜Ÿåˆ—ï¼ˆPriority Queueï¼‰ã€‚\n\n##### 1.1 å‰Kä¸ªé«˜é¢‘å•è¯\n\nidï¼š692\n\nurlï¼šhttps://leetcode-cn.com/problems/top-k-frequent-words/\n\næè¿°ï¼š\n\nç»™ä¸€éç©ºçš„å•è¯åˆ—è¡¨ï¼Œè¿”å›å‰ k ä¸ªå‡ºç°æ¬¡æ•°æœ€å¤šçš„å•è¯ã€‚\n\nè¿”å›çš„ç­”æ¡ˆåº”è¯¥æŒ‰å•è¯å‡ºç°é¢‘ç‡ç”±é«˜åˆ°ä½æ’åºã€‚å¦‚æœä¸åŒçš„å•è¯æœ‰ç›¸åŒå‡ºç°é¢‘ç‡ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åºã€‚\n\nç¤ºä¾‹ 1ï¼š\n\nè¾“å…¥: [\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"], k = 2\nè¾“å‡º: [\"i\", \"love\"]\nè§£æ: \"i\" å’Œ \"love\" ä¸ºå‡ºç°æ¬¡æ•°æœ€å¤šçš„ä¸¤ä¸ªå•è¯ï¼Œå‡ä¸º2æ¬¡ã€‚\n    æ³¨æ„ï¼ŒæŒ‰å­—æ¯é¡ºåº \"i\" åœ¨ \"love\" ä¹‹å‰ã€‚\n\n\n\nç®—æ³•ï¼š\n\n1. è®¡ç®—æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼Œç„¶åå°†å…¶æ·»åŠ åˆ°å­˜å‚¨åˆ°å¤§å°ä¸º k çš„å°æ ¹å †ä¸­ã€‚å®ƒå°†é¢‘ç‡æœ€å°çš„å€™é€‰é¡¹æ”¾åœ¨å †çš„é¡¶éƒ¨ã€‚æœ€åï¼Œæˆ‘ä»¬ä»å †ä¸­å¼¹å‡ºæœ€å¤š k æ¬¡ï¼Œå¹¶åè½¬ç»“æœï¼Œå°±å¯ä»¥å¾—åˆ°å‰ k ä¸ªé«˜é¢‘å•è¯ã€‚\n2. åœ¨ Python ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ heapq\\heapifyï¼Œå®ƒå¯ä»¥åœ¨çº¿æ€§æ—¶é—´å†…å°†åˆ—è¡¨è½¬æ¢ä¸ºå †ï¼Œä»è€Œç®€åŒ–äº†æˆ‘ä»¬çš„å·¥ä½œã€‚\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_692_topKFrequent {\n    public static List<String> topKFrequent(String[] words, int k){\n        Map<String,Integer> countMap = new HashMap<String, Integer>();\n        for(String word:words){\n            countMap.put(word,countMap.getOrDefault(word,0)+1);\n        }\n\n        PriorityQueue<String> queue = new PriorityQueue<String>(\n                (o1, o2) -> countMap.get(o1) == countMap.get(o2)?\n                        o2.compareTo(o1):countMap.get(o1) - countMap.get(o2)\n        );\n\n        for(String word:countMap.keySet()){\n            queue.offer(word);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<String> ans = new ArrayList<>();\n        while (!queue.isEmpty()){\n            ans.add(queue.poll());\n        }\n        Collections.reverse(ans);\n        return ans;\n\n    }\n\n    public static void main(String[] args) {\n        String[] words = {\"i\", \"love\", \"leetcode\", \"i\", \"love\", \"coding\"};\n        topKFrequent(words,2);\n    }\n}\n\n```\n\nå¤æ‚åº¦åˆ†æ\n\næ—¶é—´å¤æ‚åº¦ï¼š $O(N \\log{k})$ã€‚å…¶ä¸­ $N$ æ˜¯ words çš„é•¿åº¦ã€‚æˆ‘ä»¬ç”¨ $O(N)$çš„æ—¶é—´è®¡ç®—æ¯ä¸ªå•è¯çš„é¢‘ç‡ï¼Œç„¶åå°† $N$ ä¸ªå•è¯æ·»åŠ åˆ°å †ä¸­ï¼Œæ·»åŠ æ¯ä¸ªå•è¯çš„æ—¶é—´ä¸º $O(\\log {k})$ ã€‚æœ€åï¼Œæˆ‘ä»¬ä»å †ä¸­å¼¹å‡ºæœ€å¤š $k$ æ¬¡ã€‚å› ä¸º $k \\leq N$ çš„å€¼ï¼Œæ€»å…±æ˜¯ $O(N \\log{k})$\nç©ºé—´å¤æ‚åº¦ï¼š$O(N)$,ç”¨äºå­˜å‚¨æˆ‘ä»¬è®¡æ•°çš„ç©ºé—´\n\n\n\n```python\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, words: List[str], k: int) -> List[str]:\n        count = Counter(words)\n        return heapq.nsmallest(k,count,lambda i: (-count[i], i))\n```\n\n\n\n##### 1.2 [å‰ K ä¸ªé«˜é¢‘å…ƒç´ ](https://leetcode-cn.com/problems/top-k-frequent-elements/)\n\nid:347\n\nç»™å®šä¸€ä¸ªéç©ºçš„æ•´æ•°æ•°ç»„ï¼Œè¿”å›å…¶ä¸­å‡ºç°é¢‘ç‡å‰ **k** é«˜çš„å…ƒç´ ã€‚\n\n**ç¤ºä¾‹ 1:**\n\n```text\nè¾“å…¥: nums = [1,1,1,2,2,3], k = 2\nè¾“å‡º: [1,2]\n```\n\n- ä½ å¯ä»¥å‡è®¾ç»™å®šçš„ *k* æ€»æ˜¯åˆç†çš„ï¼Œä¸” 1 â‰¤ k â‰¤ æ•°ç»„ä¸­ä¸ç›¸åŒçš„å…ƒç´ çš„ä¸ªæ•°ã€‚\n- ä½ çš„ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦**å¿…é¡»**ä¼˜äº O(*n* log *n*) , *n* æ˜¯æ•°ç»„çš„å¤§å°ã€‚\n\n\n\n```java\nclass Solution {\n    public List<Integer> topKFrequent(int[] nums, int k) {\n        Map<Integer,Integer> count = new HashMap<>();\n        for(int n:nums){\n            count.put(n,count.getOrDefault(n,0) + 1);\n        }\n\n        PriorityQueue<Integer> queue = new PriorityQueue<>((o1,o2) -> count.get(o1) - count.get(o2));\n\n        for(Integer i: count.keySet()){\n            queue.add(i);\n            while (queue.size() > k){\n                queue.poll();\n            }\n        }\n\n        List<Integer> res = new ArrayList<>();\n        while (!queue.isEmpty()){\n            res.add(queue.poll());\n        }\n        Collections.reverse(res);\n        return res;\n    }\n}\n```\n\n```python\nfrom typing import List\nfrom collections import Counter\nimport heapq\nclass Solution:\n    def topKFrequent(self, nums: List[int], k: int) -> List[int]:\n        count = Counter(nums)\n        return heapq.nlargest(k, count.keys(), count.get)\n```\n\n##### 1.3 [ æ•°ç»„ä¸­çš„ç¬¬Kä¸ªæœ€å¤§å…ƒç´ ](https://leetcode-cn.com/problems/kth-largest-element-in-an-array/)\n\nid:215\n\nåœ¨æœªæ’åºçš„æ•°ç»„ä¸­æ‰¾åˆ°ç¬¬ k ä¸ªæœ€å¤§çš„å…ƒç´ ã€‚è¯·æ³¨æ„ï¼Œä½ éœ€è¦æ‰¾çš„æ˜¯æ•°ç»„æ’åºåçš„ç¬¬ k ä¸ªæœ€å¤§çš„å…ƒç´ ï¼Œè€Œä¸æ˜¯ç¬¬ k ä¸ªä¸åŒçš„å…ƒç´ ã€‚\n\nç¤ºä¾‹ 1:\n\nè¾“å…¥: [3,2,1,5,6,4] å’Œ k = 2\nè¾“å‡º: 5\n\n```java\nclass Solution {\n    public int findKthLargest(int[] nums, int k) {\n        PriorityQueue<Integer> queue = new PriorityQueue<>(Comparator.comparingInt(n -> n));\n\n        for(int n:nums){\n            queue.add(n);\n            if(queue.size() > k){\n                queue.poll();\n            }\n        }\n        return queue.poll();\n    }\n}\n```\n\n\n\n```python\nclass Solution:\n    def findKthLargest(self, nums: List[int], k: int) -> int:\n        return heapq.nlargest(k,nums)[-1]\n```\n\n##### 1.4  [é¢è¯•é¢˜40. æœ€å°çš„kä¸ªæ•°](https://leetcode-cn.com/problems/zui-xiao-de-kge-shu-lcof/)\n\nè¾“å…¥æ•´æ•°æ•°ç»„ arr ï¼Œæ‰¾å‡ºå…¶ä¸­æœ€å°çš„ k ä¸ªæ•°ã€‚ä¾‹å¦‚ï¼Œè¾“å…¥4ã€5ã€1ã€6ã€2ã€7ã€3ã€8è¿™8ä¸ªæ•°å­—ï¼Œåˆ™æœ€å°çš„4ä¸ªæ•°å­—æ˜¯1ã€2ã€3ã€4ã€‚\n\n \n\nç¤ºä¾‹ 1ï¼š\n\nè¾“å…¥ï¼šarr = [3,2,1], k = 2\nè¾“å‡ºï¼š[1,2] æˆ–è€… [2,1]\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_40_mian_getLeastNumbers {\n\n    public static int[]  getLeastNumbers(int[] arr, int k) {\n        if(k == 0) return new int[0];\n        PriorityQueue<Integer> priorityQueue = new PriorityQueue<>((a, b) -> (b -a));\n        for(int i: arr){\n            if(priorityQueue.size() < k){\n                priorityQueue.add(i);\n            }else{\n                if(priorityQueue.peek() > i){\n                    priorityQueue.remove();\n                    priorityQueue.add(i);\n                }\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (priorityQueue.size() > 0){\n            res[count++] = priorityQueue.remove();\n        }\n        return res;\n    }\n\n    public static int[] getLeastNumbers2(int[] arr, int k) {\n        if(k == 0 || arr.length == 0){\n            return new int[0];\n        }\n        PriorityQueue<Integer> heap = new PriorityQueue<>((o1, o2) -> o2 - o1);\n\n        for(int i:arr){\n            heap.offer(i);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n\n        int[] res = new int[k];\n        int count = 0;\n        while (!heap.isEmpty()){\n            res[count++] = heap.poll();\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {3,2,1};\n        System.out.println(Arrays.toString(getLeastNumbers2(arr,2)));\n        System.out.println(Arrays.toString(getLeastNumbers(arr,2)));\n    }\n}\n\n```\n\n```scala\n    def getLeastNumbers(arr: Array[Int], k: Int): Array[Int] = {\n        val heap = scala.collection.mutable.PriorityQueue.empty[Int].reverse\n        arr.foreach(x => heap.enqueue(x))\n        (0 until k).map(_ => heap.dequeue()).toArray\n    }\n```\n\n##### 1.5 [æœ€æ¥è¿‘åŸç‚¹çš„ K ä¸ªç‚¹](https://leetcode-cn.com/problems/k-closest-points-to-origin/)\n\nidï¼š973\n\næˆ‘ä»¬æœ‰ä¸€ä¸ªç”±å¹³é¢ä¸Šçš„ç‚¹ç»„æˆçš„åˆ—è¡¨ `points`ã€‚éœ€è¦ä»ä¸­æ‰¾å‡º `K` ä¸ªè·ç¦»åŸç‚¹ `(0, 0)` æœ€è¿‘çš„ç‚¹ã€‚\n\nï¼ˆè¿™é‡Œï¼Œå¹³é¢ä¸Šä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»æ˜¯æ¬§å‡ é‡Œå¾·è·ç¦»ã€‚ï¼‰\n\nä½ å¯ä»¥æŒ‰ä»»ä½•é¡ºåºè¿”å›ç­”æ¡ˆã€‚é™¤äº†ç‚¹åæ ‡çš„é¡ºåºä¹‹å¤–ï¼Œç­”æ¡ˆç¡®ä¿æ˜¯å”¯ä¸€çš„ã€‚\n\n \n\n**ç¤ºä¾‹ 1ï¼š**\n\n```\nè¾“å…¥ï¼špoints = [[1,3],[-2,2]], K = 1\nè¾“å‡ºï¼š[[-2,2]]\nè§£é‡Šï¼š \n(1, 3) å’ŒåŸç‚¹ä¹‹é—´çš„è·ç¦»ä¸º sqrt(10)ï¼Œ\n(-2, 2) å’ŒåŸç‚¹ä¹‹é—´çš„è·ç¦»ä¸º sqrt(8)ï¼Œ\nç”±äº sqrt(8) < sqrt(10)ï¼Œ(-2, 2) ç¦»åŸç‚¹æ›´è¿‘ã€‚\næˆ‘ä»¬åªéœ€è¦è·ç¦»åŸç‚¹æœ€è¿‘çš„ K = 1 ä¸ªç‚¹ï¼Œæ‰€ä»¥ç­”æ¡ˆå°±æ˜¯ [[-2,2]]ã€‚\n```\n\näºŒã€å¤§æ ¹å †(å‰ K å°) / å°æ ¹å †ï¼ˆå‰ K å¤§),Javaä¸­æœ‰ç°æˆçš„ PriorityQueueï¼Œå®ç°èµ·æ¥æœ€ç®€å•ï¼š$O(NlogK)$\næœ¬é¢˜æ˜¯æ±‚å‰ K å°ï¼Œå› æ­¤ç”¨ä¸€ä¸ªå®¹é‡ä¸º K çš„å¤§æ ¹å †ï¼Œæ¯æ¬¡ poll å‡ºæœ€å¤§çš„æ•°ï¼Œé‚£å †ä¸­ä¿ç•™çš„å°±æ˜¯å‰ K å°å•¦ï¼ˆæ³¨æ„ä¸æ˜¯å°æ ¹å †ï¼å°æ ¹å †çš„è¯éœ€è¦æŠŠå…¨éƒ¨çš„å…ƒç´ éƒ½å…¥å †ï¼Œé‚£æ˜¯ O(NlogN)O(NlogN)ğŸ˜‚ï¼Œå°±ä¸æ˜¯ O(NlogK)O(NlogK)å•¦ï½ï½ï¼‰\nè¿™ä¸ªæ–¹æ³•æ¯”å¿«æ’æ…¢ï¼Œä½†æ˜¯å› ä¸º Java ä¸­æä¾›äº†ç°æˆçš„ PriorityQueueï¼ˆé»˜è®¤å°æ ¹å †ï¼‰ï¼Œæ‰€ä»¥å®ç°èµ·æ¥æœ€ç®€å•ï¼Œæ²¡å‡ è¡Œä»£ç \n\n```java\n\npackage com.strings.leetcode.heap;\n\nimport scala.actors.threadpool.Arrays;\n\nimport java.util.PriorityQueue;\n\npublic class Problem_973j_kClosest {\n    public static int[][] kClosest(int[][] points, int K) {\n        if(K == 0 || points.length == 0){\n            return new int[0][0];\n        }\n        PriorityQueue<int[]> heap = new PriorityQueue<>((o1, o2) ->\n                o2[0]*o2[0] + o2[1]*o2[1] - o1[0]*o1[0] - o1[1]*o1[1]\n        );\n\n        for(int[] p:points){\n            heap.offer(p);\n            if(heap.size() > K){\n                heap.poll();\n            }\n        }\n        int[][] res = new int[K][2];\n        int count = 0;\n        for(int[] i:heap){\n            res[count++] = i;\n        }\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[][] points = {{1,3},{-2,2}};\n        int K = 1;\n        System.out.println(Arrays.deepToString(kClosest(points,1)));\n    }\n}\n```\n\n```python\nclass Solution:\n    def kClosest(self, points: List[List[int]], K: int) -> List[List[int]]:\n        return heapq.nsmallest(K, points, key = lambda point: point[0] ** 2 + point[1] ** 2 )\n```\n\n```scala\nobject Solution {\n  def diff(arr: Array[Int]) = -arr(0)*arr(0) - arr(1)*arr(1)\n  def kClosest(points: Array[Array[Int]], K: Int): Array[Array[Int]] = {\n    val heap = scala.collection.mutable.PriorityQueue[Array[Int]]()(Ordering.by(diff))\n    points.foreach(x => heap.enqueue(x))\n    (0 until K).map(_ => heap.dequeue()).toArray\n  }\n}\n```\n\n\n\n##### 1.6 [æŸ¥æ‰¾å’Œæœ€å°çš„Kå¯¹æ•°å­—](https://leetcode-cn.com/problems/find-k-pairs-with-smallest-sums/)\n\nidï¼š373.\n\nç»™å®šä¸¤ä¸ªä»¥å‡åºæ’åˆ—çš„æ•´å½¢æ•°ç»„ **nums1** å’Œ **nums2**, ä»¥åŠä¸€ä¸ªæ•´æ•° **k**ã€‚\n\nå®šä¹‰ä¸€å¯¹å€¼ **(u,v)**ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ æ¥è‡ª **nums1**ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ¥è‡ª **nums2**ã€‚\n\næ‰¾åˆ°å’Œæœ€å°çš„ k å¯¹æ•°å­— **(u1,v1), (u2,v2) ... (uk,vk)**ã€‚\n\n**ç¤ºä¾‹ 1:**\n\n```\nè¾“å…¥: nums1 = [1,7,11], nums2 = [2,4,6], k = 3\nè¾“å‡º: [1,2],[1,4],[1,6]\nè§£é‡Š: è¿”å›åºåˆ—ä¸­çš„å‰ 3 å¯¹æ•°ï¼š\n     [1,2],[1,4],[1,6],[7,2],[7,4],[11,2],[7,6],[11,4],[11,6]\n```\n\n```scala\nclass Solution {\n    public List<List<Integer>> kSmallestPairs(int[] nums1, int[] nums2, int k) {\n      List<List<Integer>> pair = new ArrayList<>();\n        for (int i = 0; i < nums1.length ; i++) {\n            for (int j = 0; j < nums2.length; j++) {\n                List<Integer> res = new ArrayList<>();\n                res.add(nums1[i]);\n                res.add(nums2[j]);\n                pair.add(res);\n            }\n        }\n\n        PriorityQueue<List<Integer>> heap = new PriorityQueue<>((o1, o2) ->\n                o2.get(0) + o2.get(1) - o1.get(0) - o1.get(1)\n                );\n\n        for(List<Integer> lst:pair){\n            heap.offer(lst);\n            if(heap.size() > k){\n                heap.poll();\n            }\n        }\n        List<List<Integer>> ans = new ArrayList<>();\n        if(k==0 || nums1.length==0 || nums2.length == 0){\n            return ans;\n        }\n        for(List<Integer> lst:heap){\n            ans.add(lst);\n        }\n        return ans;\n\n    }\n}\n```\n\n```scala\nimport scala.collection.mutable.ArrayBuffer\n\nobject Solution {\n    def diff(num: List[Int]) = {-num(0) - num(1)}\n    def kSmallestPairs(nums1: Array[Int], nums2: Array[Int], k: Int): List[List[Int]] = {\n        if(k==0||nums1.length== 0){ List()}\n        else{\n        val pair:ArrayBuffer[List[Int]] = new ArrayBuffer[List[Int]]()\n        for(i <- 0 until nums1.length){\n            for(j <- 0 until nums2.length){\n                pair.append(List(nums1(i),nums2(j)))\n            }\n        }\n        val heap = scala.collection.mutable.PriorityQueue[List[Int]]()(Ordering.by(diff))\n        pair.toArray.foreach(x => heap.enqueue(x))\n        var tmpk= k\n        if(heap.size < k){\n            tmpk = heap.size\n        }\n        (0 until tmpk).map(_ => heap.dequeue()).toList\n        }\n\n    }\n}\n```\n\n##### 1.7  [æ•°æ®æµä¸­çš„ä¸­ä½æ•°](https://leetcode-cn.com/problems/shu-ju-liu-zhong-de-zhong-wei-shu-lcof/)\n\n###### [æ•°æ®æµçš„ä¸­ä½æ•°](https://leetcode-cn.com/problems/find-median-from-data-stream/)\n\n###### [è¿ç»­ä¸­å€¼](https://leetcode-cn.com/problems/continuous-median-lcci/)\n\nè¿™ä¸‰ä¸ªé¢˜éƒ½æ˜¯ä¸€æ ·çš„ã€‚\n\nå°†è¾“å…¥çš„æ•°åˆ†æˆä¸¤éƒ¨åˆ†ï¼šè¾ƒå°çš„ä¸€éƒ¨åˆ†å’Œè¾ƒå¤§çš„ä¸€éƒ¨åˆ†\n\n1. lowPart ï¼šå®šä¹‰ä¸ºè¾ƒå°çš„ä¸€éƒ¨åˆ†ï¼Œç”¨æœ€å¤§å †\n   å…è®¸lowPartçš„å¤§å°æ¯”highPartå¤š1\n2. highPart ï¼š å®šä¹‰ä¸ºè¾ƒå¤§çš„ä¸€éƒ¨åˆ†ï¼Œç”¨æœ€å°å †\n   å¦‚æœsizeæ˜¯å¥‡æ•°ï¼Œé‚£ä¹ˆä¸­ä½æ•°å°±æ˜¯lowPartçš„æœ€å¤§å€¼ï¼Œä¹Ÿå°±æ˜¯å †é¡¶\n\nå¦åˆ™ï¼Œæœ€å¤§å€¼æ˜¯lowPartå’ŒhighPartçš„å †é¡¶å¹³å‡å€¼\n\nç»´æŠ¤\n\næ¯è¿›å…¥ä¸€ä¸ªæ•°ï¼Œå…ˆåŠ å…¥lowPartï¼Œç„¶åå°†lowPartçš„æœ€å¤§å€¼ï¼ˆå †é¡¶ï¼‰ç§»å‡ºåˆ°highPart\n\nå¦‚æœè¿™æ—¶sizeæ˜¯å¥‡æ•°ï¼Œæ­¤æ—¶highPartå°†æœ€å°å€¼ç§»å‡ºåˆ°lowPart\n\n\n\n```java\nclass MedianFinder {\n\n        private PriorityQueue<Integer> lowPart;\n    private PriorityQueue<Integer> highPart;\n    int size;\n    /** initialize your data structure here. */\n    public MedianFinder() {\n        lowPart = new PriorityQueue<Integer>((x, y) -> y - x);  //æœ€å¤§å †\n        highPart = new PriorityQueue<Integer>();\n        size = 0;\n    }\n\n    public void addNum(int num) {\n        size++;\n        lowPart.offer(num);\n        highPart.offer(lowPart.poll());\n        if((size & 1) == 1){\n            lowPart.offer(highPart.poll());\n        }\n    }\n\n    public double findMedian() {\n        if((size & 1) == 1){\n            return (double) lowPart.peek();\n        }else{\n            return (double) (lowPart.peek() + highPart.peek()) / 2;\n        }\n    }\n}\n\n/**\n * Your MedianFinder object will be instantiated and called as such:\n * MedianFinder obj = new MedianFinder();\n * obj.addNum(num);\n * double param_2 = obj.findMedian();\n */\n```\n\n```python\nclass MedianFinder:\n\n    def __init__(self):\n        # å½“å‰å¤§é¡¶å †å’Œå°é¡¶å †çš„å…ƒç´ ä¸ªæ•°ä¹‹å’Œ\n        self.count = 0\n        self.max_heap = []\n        self.min_heap = []\n\n    def addNum(self, num: int) -> None:\n        self.count += 1\n        # å› ä¸º Python ä¸­çš„å †é»˜è®¤æ˜¯å°é¡¶å †ï¼Œæ‰€ä»¥è¦ä¼ å…¥ä¸€ä¸ª tupleï¼Œç”¨äºæ¯”è¾ƒçš„å…ƒç´ éœ€æ˜¯ç›¸åæ•°ï¼Œ\n        # æ‰èƒ½æ¨¡æ‹Ÿå‡ºå¤§é¡¶å †çš„æ•ˆæœ\n        heapq.heappush(self.max_heap, (-num, num))\n        _, max_heap_top = heapq.heappop(self.max_heap)\n        heapq.heappush(self.min_heap, max_heap_top)\n        if self.count & 1:\n            min_heap_top = heapq.heappop(self.min_heap)\n            heapq.heappush(self.max_heap, (-min_heap_top, min_heap_top))\n\n    def findMedian(self) -> float:\n        if self.count & 1:\n            # å¦‚æœä¸¤ä¸ªå †åˆèµ·æ¥çš„å…ƒç´ ä¸ªæ•°æ˜¯å¥‡æ•°ï¼Œæ•°æ®æµçš„ä¸­ä½æ•°å¤§é¡¶å †çš„å †é¡¶å…ƒç´ \n            return self.max_heap[0][1]\n        else:\n            # å¦‚æœä¸¤ä¸ªå †åˆèµ·æ¥çš„å…ƒç´ ä¸ªæ•°æ˜¯å¶æ•°ï¼Œæ•°æ®æµçš„ä¸­ä½æ•°å°±æ˜¯å„è‡ªå †é¡¶å…ƒç´ çš„å¹³å‡å€¼\n            return (self.min_heap[0] + self.max_heap[0][1]) / 2\n\n        \n\n\n# Your MedianFinder object will be instantiated and called as such:\n# obj = MedianFinder()\n# obj.addNum(num)\n# param_2 = obj.findMedian()\n```\n\n##### 1.8 [è·ç¦»ç›¸ç­‰çš„æ¡å½¢ç ](https://leetcode-cn.com/problems/distant-barcodes/)\n\nidï¼š1054\n\nåœ¨ä¸€ä¸ªä»“åº“é‡Œï¼Œæœ‰ä¸€æ’æ¡å½¢ç ï¼Œå…¶ä¸­ç¬¬ `i` ä¸ªæ¡å½¢ç ä¸º `barcodes[i]`ã€‚\n\nè¯·ä½ é‡æ–°æ’åˆ—è¿™äº›æ¡å½¢ç ï¼Œä½¿å…¶ä¸­ä¸¤ä¸ªç›¸é‚»çš„æ¡å½¢ç  **ä¸èƒ½** ç›¸ç­‰ã€‚ ä½ å¯ä»¥è¿”å›ä»»ä½•æ»¡è¶³è¯¥è¦æ±‚çš„ç­”æ¡ˆï¼Œæ­¤é¢˜ä¿è¯å­˜åœ¨ç­”æ¡ˆã€‚\n\n \n\n**ç¤ºä¾‹ 1ï¼š**\n\n```\nè¾“å…¥ï¼š[1,1,1,2,2,2]\nè¾“å‡ºï¼š[2,1,2,1,2,1]\n```\n\n**ç¤ºä¾‹ 2ï¼š**\n\n```\nè¾“å…¥ï¼š[1,1,1,1,2,2,3,3]\nè¾“å‡ºï¼š[1,3,1,3,2,1,2,1]\n```\n\nè´ªå¿ƒå †\n\n```java\npackage com.strings.leetcode.heap;\n\n\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\nclass Problem_1054j_rearrangeBarcodes {\n    public static int[] rearrangeBarcodes2(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2){\n            return barcodes;\n        }\n        Map<Integer,Integer> countMap = new HashMap<>();\n        for(int b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(int i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1){\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = countMap.get(a);\n            int freqB = countMap.get(b);\n            if(freqA > 1){\n                countMap.put(a,freqA-1);\n                maxHeap.offer(a);\n            }\n            if(freqB > 1){\n                countMap.put(b,freqB-1);\n                maxHeap.add(b);\n            }\n        }\n        if(maxHeap.size() > 0){\n            res[idx] = maxHeap.poll();\n        }\n\n        return res;\n    }\n\n    public static int[] rearrangeBarcodes(int[] barcodes) {\n        if(barcodes == null || barcodes.length < 2) return barcodes;\n        Map<Integer, Integer> map = new HashMap<>();\n        for(int i : barcodes) {\n            map.put(i, map.getOrDefault(i, 0) + 1);\n        }\n        //å¤§é¡¶å †\n        PriorityQueue<Integer> maxHeap = new PriorityQueue<>((a, b) -> map.get(b) - map.get(a));\n        for(int i : map.keySet()) {\n            maxHeap.offer(i);\n        }\n        int[] res = new int[barcodes.length];\n        int idx = 0;\n        while(maxHeap.size() > 1) {\n            int a = maxHeap.poll();\n            int b = maxHeap.poll();\n            res[idx++] = a;\n            res[idx++] = b;\n            int freqA = map.get(a);\n            int freqB = map.get(b);\n            if(freqA > 1) {\n                map.put(a, freqA - 1);\n                maxHeap.add(a);\n            }\n            if(freqB > 1) {\n                map.put(b, freqB - 1);\n                maxHeap.add(b);\n            }\n        }\n        //æ”¶å°¾\n        if(maxHeap.size() > 0) res[idx] = maxHeap.poll();\n        return res;\n    }\n\n    public static void main(String[] args) {\n        int[] bor = {1,1,2};\n        System.out.println(Arrays.toString(rearrangeBarcodes(bor)));\n        System.out.println(Arrays.toString(rearrangeBarcodes2(bor)));\n    }\n}\n\n```\n\n##### 1.9 [ é‡æ„å­—ç¬¦ä¸²](https://leetcode-cn.com/problems/reorganize-string/)\n\nç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²Sï¼Œæ£€æŸ¥æ˜¯å¦èƒ½é‡æ–°æ’å¸ƒå…¶ä¸­çš„å­—æ¯ï¼Œä½¿å¾—ä¸¤ç›¸é‚»çš„å­—ç¬¦ä¸åŒã€‚\n\nè‹¥å¯è¡Œï¼Œè¾“å‡ºä»»æ„å¯è¡Œçš„ç»“æœã€‚è‹¥ä¸å¯è¡Œï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚\n\nç¤ºä¾‹ 1:\n\nè¾“å…¥: S = \"aab\"\nè¾“å‡º: \"aba\"\nç¤ºä¾‹ 2:\n\nè¾“å…¥: S = \"aaab\"\nè¾“å‡º: \"\"\n\n\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.PriorityQueue;\n\npublic class Problem_767_reorganizeString {\n\n    public static String reorganizeString(String S) {\n        if(S == null || S.length() < 2){\n            return S;\n        }\n        char[] barcodes = S.toCharArray();\n            Map<Character,Integer> countMap = new HashMap<>();\n            for(char b:barcodes){\n                countMap.put(b,countMap.getOrDefault(b,0)+1);\n            }\n            if(Collections.max(countMap.values()) > (S.length()+1)/2){\n                return \"\";\n            }\n            PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n            for(char i:countMap.keySet()){\n                maxHeap.add(i);\n            }\n            char[] res = new char[barcodes.length];\n            int idx = 0;\n            while(maxHeap.size() > 1){\n                char a = maxHeap.poll();\n                char b = maxHeap.poll();\n                res[idx++] = a;\n                res[idx++] = b;\n                int freqA = countMap.get(a);\n                int freqB = countMap.get(b);\n                if(freqA > 1){\n                    countMap.put(a,freqA-1);\n                    maxHeap.offer(a);\n                }\n                if(freqB > 1){\n                    countMap.put(b,freqB-1);\n                    maxHeap.add(b);\n                }\n            }\n            if(maxHeap.size() > 0){\n                res[idx] = maxHeap.poll();\n            }\n            return String.valueOf(res);\n    }\n\n    public static void main(String[] args) {\n        String S1 = \"aab\";\n        String S2 = \"aaab\";\n\n        System.out.println(reorganizeString(S1));\n        System.out.println(reorganizeString(S2));\n    }\n}\n\n```\n\n##### 1.10 [æœ‰åºçŸ©é˜µä¸­ç¬¬Kå°çš„å…ƒç´ ](https://leetcode-cn.com/problems/kth-smallest-element-in-a-sorted-matrix/)\n\nid:378\n\nç»™å®šä¸€ä¸ª n x n çŸ©é˜µï¼Œå…¶ä¸­æ¯è¡Œå’Œæ¯åˆ—å…ƒç´ å‡æŒ‰å‡åºæ’åºï¼Œæ‰¾åˆ°çŸ©é˜µä¸­ç¬¬kå°çš„å…ƒç´ ã€‚\nè¯·æ³¨æ„ï¼Œå®ƒæ˜¯æ’åºåçš„ç¬¬ k å°å…ƒç´ ï¼Œè€Œä¸æ˜¯ç¬¬ k ä¸ªä¸åŒçš„å…ƒç´ ã€‚\n\n \n\nç¤ºä¾‹:\n\nmatrix = [\n   [ 1,  5,  9],\n   [10, 11, 13],\n   [12, 13, 15]\n],\nk = 8,\n\nè¿”å› 13ã€‚\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.Comparator;\nimport java.util.PriorityQueue;\n\npublic class Problem_378_kthSmallest {\n    public static int kthSmallest(int[][] matrix, int k) {\n        PriorityQueue<Integer> heap = new PriorityQueue<>(Comparator.reverseOrder());\n        for(int[] arr:matrix){\n            for(int i: arr){\n                heap.offer(i);\n                if(heap.size() > k){\n                    heap.poll();\n                }\n            }\n        }\n        return heap.peek();\n    }\n\n    public static void main(String[] args) {\n        int[][] matrix = {{1,  5,  9},{10, 11, 13},{12, 13, 15}};\n        System.out.println(kthSmallest(matrix,8));\n    }\n\n}\n```\n\n##### 1.11 [æ ¹æ®å­—ç¬¦å‡ºç°é¢‘ç‡æ’åº](https://leetcode-cn.com/problems/sort-characters-by-frequency/)\n\nid:451\n\nç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¯·å°†å­—ç¬¦ä¸²é‡Œçš„å­—ç¬¦æŒ‰ç…§å‡ºç°çš„é¢‘ç‡é™åºæ’åˆ—ã€‚\n\nç¤ºä¾‹ 1:\n\nè¾“å…¥:\n\"tree\"\n\nè¾“å‡º:\n\"eert\"\n\nè§£é‡Š:\n'e'å‡ºç°ä¸¤æ¬¡ï¼Œ'r'å’Œ't'éƒ½åªå‡ºç°ä¸€æ¬¡ã€‚\nå› æ­¤'e'å¿…é¡»å‡ºç°åœ¨'r'å’Œ't'ä¹‹å‰ã€‚æ­¤å¤–ï¼Œ\"eetr\"ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç­”æ¡ˆã€‚\n\n```java\npackage com.strings.leetcode.heap;\n\nimport java.util.*;\n\npublic class Problem_451j_frequencySort {\n\n    public static String frequencySort(String s){\n        if(s == null || s.length() < 2){\n            return s;\n        }\n        char[] barcodes = s.toCharArray();\n        Map<Character,Integer> countMap = new HashMap<>();\n        for(char b:barcodes){\n            countMap.put(b,countMap.getOrDefault(b,0)+1);\n        }\n\n        PriorityQueue<Character> maxHeap = new PriorityQueue<>((a, b) -> countMap.get(b) - countMap.get(a));\n\n        for(char i:countMap.keySet()){\n            maxHeap.add(i);\n        }\n\n        StringBuilder sb = new StringBuilder();\n        while (!maxHeap.isEmpty()){  // æ³¨æ„ä¸èƒ½ä½¿ç”¨for(char c:maxHeap)\n            char a = maxHeap.poll();\n            int freq = countMap.get(a);\n            for (int i = 0; i < freq; i++) {\n                sb.append(a);\n            }\n        }\n\n        return sb.toString();\n    }\n\n    public static void main(String[] args) {\n        String s  = \"Aabb\";\n        System.out.println(frequencySort(s));\n    }\n}\n\n```\n\n"},{"title":"èšç±»ç®—æ³•æ€»ç»“","url":"/2020/05/19/èšç±»ç®—æ³•æ€»ç»“/","content":"### èšç±»ç®—æ³•æ€»ç»“\n\nèšç±»æ˜¯ä¸€ç§ç»å…¸çš„æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œæ— ç›‘ç£å­¦ä¹ çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹æ— æ ‡è®°è®­ç»ƒæ ·æœ¬çš„å­¦ä¹ ï¼Œå‘æ˜å’Œæ­ç¤ºæ•°æ®é›†æœ¬èº«æ½œåœ¨çš„ç»“æ„ä¸è§„å¾‹ï¼Œå³ä¸ä¾èµ–äºè®­ç»ƒæ•°æ®é›†çš„ç±»æ ‡è®°ä¿¡æ¯ã€‚èšç±»åˆ™æ˜¯è¯•å›¾å°†æ•°æ®é›†çš„æ ·æœ¬åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªäº’ä¸ç›¸äº¤çš„ç±»ç°‡ï¼Œä»è€Œæ¯ä¸ªç°‡å¯¹åº”ä¸€ä¸ªæ½œåœ¨çš„ç±»åˆ«ã€‚\n\nèšç±»ç›´è§‚ä¸Šæ¥è¯´æ˜¯å°†ç›¸ä¼¼çš„æ ·æœ¬èšåœ¨ä¸€èµ·ï¼Œä»è€Œå½¢æˆä¸€ä¸ªç±»ç°‡ï¼ˆclusterï¼‰ã€‚é‚£é¦–å…ˆçš„é—®é¢˜æ˜¯å¦‚ä½•æ¥åº¦é‡ç›¸ä¼¼æ€§ï¼ˆsimilarity measureï¼‰å‘¢ï¼Ÿè¿™ä¾¿æ˜¯è·ç¦»åº¦é‡ï¼Œåœ¨ç”Ÿæ´»ä¸­æˆ‘ä»¬è¯´å·®åˆ«å°åˆ™ç›¸ä¼¼ï¼Œå¯¹åº”åˆ°å¤šç»´æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å¯ä»¥å¯¹åº”äºé«˜ç»´ç©ºé—´ä¸­çš„ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè‹¥å®ƒä»¬çš„è·ç¦»ç›¸è¿‘ï¼Œæˆ‘ä»¬ä¾¿å¯ä»¥ç§°å®ƒä»¬ç›¸ä¼¼ã€‚é‚£æ¥ç€å¦‚ä½•æ¥è¯„ä»·èšç±»ç»“æœçš„å¥½åå‘¢ï¼Ÿè¿™ä¾¿æ˜¯æ€§èƒ½åº¦é‡ï¼Œæ€§èƒ½åº¦é‡ä¸ºè¯„ä»·èšç±»ç»“æœçš„å¥½åæä¾›äº†ä¸€ç³»åˆ—æœ‰æ•ˆæ€§æŒ‡æ ‡ã€‚\n\n#### 1 Kmeansèšç±»\n\nK-Meansçš„æ€æƒ³ååˆ†ç®€å•ï¼Œé¦–å…ˆéšæœºæŒ‡å®šç±»ä¸­å¿ƒï¼Œæ ¹æ®æ ·æœ¬ä¸ç±»ä¸­å¿ƒçš„è¿œè¿‘åˆ’åˆ†ç±»ç°‡ï¼Œæ¥ç€é‡æ–°è®¡ç®—ç±»ä¸­å¿ƒï¼Œè¿­ä»£ç›´è‡³æ”¶æ•›ï¼š\n\nå‡è®¾è¾“å…¥ç©ºé—´ $\\cal X \\in \\R^n \\ $ ä¸º$\\ n\\ $ç»´å‘é‡çš„é›†åˆï¼Œ$\\ \\cal{X}=\\{x^{(1)} ,x^{(2)},\\cdots,x^{(m)} \\} \\ $ï¼Œ$ \\ \\mathcal  C\\ $ä¸ºè¾“å…¥ç©ºé—´$\\ \\cal X\\ $çš„ä¸€ä¸ªåˆ’åˆ†ï¼Œä¸å¦¨ä»¤$\\ \\mathcal C=\\{ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\} \\ $ï¼Œå› æ­¤å¯ä»¥å®šä¹‰$\\ k\\text{-}means\\ $ç®—æ³•çš„æŸå¤±å‡½æ•°ä¸º\n$$\nJ(\\mathcal C)=\\sum\\limits_{k=1}^K\\sum\\limits_{x^{(i)}\\in \\mathbb C_k}\\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2  \\tag{1}\n$$\nå…¶ä¸­$\\ \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)}  \\ $æ˜¯ç°‡$\\ \\mathbb C_k\\ $çš„èšç±»ä¸­å¿ƒã€‚\n\näº‹å®ä¸Šï¼Œè‹¥å°†æ ·æœ¬çš„ç±»åˆ«çœ‹åšä¸ºâ€œéšå˜é‡â€ï¼ˆlatent variableï¼‰ï¼Œç±»ä¸­å¿ƒçœ‹ä½œæ ·æœ¬çš„åˆ†å¸ƒå‚æ•°ï¼Œè¿™ä¸€è¿‡ç¨‹æ­£æ˜¯é€šè¿‡EMç®—æ³•çš„ä¸¤æ­¥èµ°ç­–ç•¥è€Œè®¡ç®—å‡ºï¼Œå…¶æ ¹æœ¬çš„ç›®çš„æ˜¯ä¸ºäº†æœ€å°åŒ–å¹³æ–¹è¯¯å·®å‡½æ•°$J(\\mathcal C)$\n\n##### 1.1 ç®—æ³•æµç¨‹\n\n1. é¦–å…ˆéšæœºåˆå§‹åŒ–$\\ K\\ $ä¸ªèšç±»ä¸­å¿ƒï¼Œ$\\ \\mu^{(1)},\\mu^{(2)},\\cdots,\\mu^{(K)} \\ $ï¼›\n\n2. ç„¶åæ ¹æ®è¿™$\\ K\\ $ä¸ªèšç±»ä¸­å¿ƒç»™å‡ºè¾“å…¥ç©ºé—´$\\ \\mathcal X \\ $çš„ä¸€ä¸ªåˆ’åˆ†ï¼Œ$\\ \\mathbb C_1,\\mathbb C_2,\\cdots,\\mathbb C_K \\ $ï¼›\n\n   - æ ·æœ¬ç¦»å“ªä¸ªç°‡çš„èšç±»ä¸­å¿ƒæœ€è¿‘ï¼Œåˆ™è¯¥æ ·æœ¬å°±åˆ’å½’åˆ°é‚£ä¸ªç°‡\n     $$\n     \\mathop{\\arg\\min}_{k}\\ \\Vert x^{(i)}-\\mu^{(k)} \\Vert_2^2 \\tag{2}\n     $$\n\n3. å†æ ¹æ®è¿™ä¸ªåˆ’åˆ†æ¥æ›´æ–°è¿™$\\ K\\ $ä¸ªèšç±»ä¸­å¿ƒ\n   $$\n   \\mu^{(k)}=\\frac{1}{\\vert \\mathbb C_k \\vert}\\sum\\limits_{x^{(i)}\\in\\mathbb C_k}x^{(i)} \\tag{3}\n   $$\n\n4. é‡å¤2ã€3æ­¥éª¤ç›´è‡³æ”¶æ•›\n\n   - å³$\\ K\\ $ä¸ªèšç±»ä¸­å¿ƒä¸å†å˜åŒ–\n\n##### 1.2  ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.model.Model\nimport com.strings.data.Data\n\nimport scala.collection.mutable.ListBuffer\nimport scala.util.Random\n\nclass Kmeans(val k:Int = 3,\n             val max_iter:Int = 100,\n             val seed:Long = 1234L,\n             val tolerance: Double = 1e-4) extends Model{\n\n  private var centroids = List[DenseVector[Double]]()\n  private var cluster = ListBuffer[(Int,DenseVector[Double])]()\n\n  var iterations:Int = 0\n\n  def _init_random_centroids(data : List[DenseVector[Double]]):List[DenseVector[Double]] = {\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closest_centroid2(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n        var close_i = 0\n        var closest_dist = -1.0\n        centroids.zipWithIndex.foreach(centroid => {\n          val distance = squaredDistance(centroid._1,row)\n          if(closest_dist>distance || closest_dist == -1.0){\n            closest_dist = distance\n            close_i = centroid._2\n          }\n        })\n    (close_i,row)\n  }\n\n  def _closest_centroid(centroids:List[DenseVector[Double]],row:DenseVector[Double]):(Int,DenseVector[Double]) = {\n      val distWithIndex =  centroids.zipWithIndex.map(x =>\n                          (squaredDistance(x._1,row),x._2)\n                          ).minBy(_._1)\n      (distWithIndex._2,row)\n  }\n\n  def train(data:List[DenseVector[Double]]):Unit = {\n    centroids = _init_random_centroids(data)\n    var flag = true\n    for(_ <- Range(0,max_iter) if flag){\n      iterations += 1\n      data.foreach{d =>\n        val b = _closest_centroid(centroids, d)\n        cluster.append(b)\n      }\n      val prev_centroid = centroids\n      centroids = _calculate_centroids(cluster)\n      cluster = ListBuffer[(Int,DenseVector[Double])]()\n      val diff = prev_centroid.zip(centroids).map(x => squaredDistance(x._2,x._1))\n      if( diff.sum < tolerance){\n        flag = false\n      }\n    }\n\n  }\n\n  def predit(data:List[DenseVector[Double]]):List[(Int,DenseVector[Double])]= {\n    data.map(x => _closest_centroid(centroids,x))\n  }\n\n  def _calculate_centroids(cluster:ListBuffer[(Int,DenseVector[Double])]):List[DenseVector[Double]]= {\n    cluster.groupBy(_._1).map { x =>\n      val temp = x._2.map(_._2)\n      temp.reduce((a, b) => a :+ b).map(_ / temp.length)\n    }.toList\n  }\n\n  /**\n   * @param x input matrix\n   * @return predict vector value\n   */\n  override def predict(x: DenseMatrix[Double]): DenseVector[Double] = {\n    DenseVector[Double]()\n  }\n}\n\nobject Kmeans{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).map(DenseVector(_)).toList\n    val kmeans = new Kmeans(max_iter = 100)\n    kmeans.train(data)\n    println(kmeans.centroids)\n\n  }\n}\n```\n\nè¯¦ç»†è¯·å‚è€ƒï¼šhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/Kmeans.scala \n\n#### 2 GMM èšç±»\n\nGMMèšç±»åˆç§°é«˜æ–¯æ··åˆèšç±»ï¼Œå³é‡‡ç”¨é«˜æ–¯åˆ†å¸ƒæ¥æè¿°åŸå‹ã€‚ç°å‡è®¾æ¯ä¸ªç±»ç°‡ä¸­çš„æ ·æœ¬éƒ½æœä»ä¸€ä¸ªå¤šç»´é«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆç©ºé—´ä¸­çš„æ ·æœ¬å¯ä»¥çœ‹ä½œç”±kä¸ªå¤šç»´é«˜æ–¯åˆ†å¸ƒæ··åˆè€Œæˆã€‚\n\n##### 2.1 ç®—æ³•è¿‡ç¨‹\n\né«˜æ–¯åˆ†å¸ƒçš„å®šä¹‰ï¼Œå¯¹äº$n$ç»´æ ·æœ¬ç©ºé—´$\\mathcal X$ä¸­çš„éšæœºå‘é‡$x$ï¼Œè‹¥$x$æœä»é«˜æ–¯åˆ†å¸ƒï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º\n$$\np(x) = \\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}  \\tag{4}\n$$\nå…¶ä¸­$\\mu$æ˜¯$n$ç»´å‡å€¼å‘é‡ï¼Œ$\\Sigma$æ˜¯$n\\times n$çš„åæ–¹å·®çŸ©é˜µ,ç”±(4)å¯ä»¥çœ‹å‡ºï¼Œé«˜æ–¯åˆ†å¸ƒå®Œå…¨ç”±å‡å€¼å‘é‡å’Œåæ–¹å·®çŸ©é˜µ$\\Sigma$è¿™ä¸¤ä¸ªå‚æ•°ç¡®å®šã€‚ä¸ºäº†æ˜ç¡®é«˜æ–¯åˆ†å¸ƒä¸ç›¸åº”çš„å‚æ•°çš„ä¾èµ–å…³ç³»ï¼Œå°†æ¦‚ç‡å¯†åº¦å‡½æ•°è®°ä¸º$p(x|\\mu,\\Sigma)$.\n\næˆ‘ä»¬å¯ä»¥å®šä¹‰é«˜æ–¯æ··åˆåˆ†å¸ƒ\n$$\np_{\\mathcal M}(x) = \\sum_{i=1}{k}\\alpha_i.p(x|\\mu_i,\\Sigma_i) \\tag{5}\n$$\n$Î±_i$ç§°ä¸ºæ··åˆç³»æ•°,æ»¡è¶³$\\sum_{i=1}^{k}\\alpha_i=1$ï¼Œå‡è®¾æ ·æœ¬çš„ç”Ÿæˆè¿‡ç¨‹ç”±é«˜æ–¯æ··åˆåˆ†å¸ƒç»™å‡ºï¼šé¦–å…ˆï¼Œæ ¹æ®$\\alpha_1,\\alpha_2,...,\\alpha_k$å®šä¹‰çš„å…ˆéªŒåˆ†å¸ƒé€‰æ‹©é«˜æ–¯æ··åˆæˆåˆ†ï¼Œå…¶ä¸­$Î±_i$ä¸ºé€‰æ‹©ç¬¬$i$æ··åˆæˆåˆ†çš„æ¦‚ç‡ï¼Œç„¶åæ ¹æ®è¢«é€‰æ‹©çš„æ··åˆæˆåˆ†çš„æ¦‚ç‡å¯†åº¦å‡½æ•°è¿›è¡Œé‡‡æ ·ï¼Œä»è€Œç”Ÿæˆç›¸åº”çš„æ ·æœ¬ã€‚\n\nè‹¥è®­ç»ƒé›†$D={x_1,x_2,...,x_m}$ç”±ä¸Šè¿°è¿‡ç¨‹ç”Ÿæˆï¼Œä»¤éšæœºå˜é‡$z_j \\in{1,2,...,k}$è¡¨ç¤ºç”Ÿæˆæ ·æœ¬çš„é«˜æ–¯æ··åˆæˆåˆ†ï¼Œæ ¹æ®è´å¶æ–¯å®šç†ï¼Œ$z_j$çš„åéªŒåˆ†å¸ƒå¯¹åº”äº\n$$\np_{\\mathcal M}(z_j = i | x_j) = \\frac{P(z_j=i).p_{\\mathcal M}(x_j|z_j=i)}{p_{\\mathcal M}(x_j)} \\\\\n=\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}   \\tag{6}\n$$\nå½“é«˜æ–¯æ··åˆåˆ†å¸ƒ(5)å·²çŸ¥æ—¶ï¼Œé«˜æ–¯æ··åˆèšç±»å°†æ ·æœ¬é›†$D$åˆ’åˆ†ä¸º$k$ä¸ªç°‡$\\mathcal C = {C_1,C_2,...,C_k}$,å°†æ¯ä¸ªæ ·æœ¬$x_j$çš„ç°‡æ ‡è®°ä¸º$\\lambda_j$å¦‚ä¸‹ç¡®å®šï¼š\n$$\n\\lambda_j = \\arg \\max _{i\\in{1,2...,k}}\\gamma_{ji} \\tag{7}\n$$\nå¯¹äº(5)å¼ï¼Œæ¨¡å‹å‚æ•°$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$å¦‚ä½•æ±‚è§£ï¼Ÿæ˜¾ç„¶ç»™å®šæ ·æœ¬é›†$D$ï¼Œå¯ä»¥é‡‡ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡ï¼Œå³æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ï¼š\n$$\nLL(D) = \\ln(\\prod_{j}^mp_{\\mathcal M}(x_j)) \\\\\n=\\sum_{j=1}^{m}ln(\\sum_{i=1}^{k}\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)) \\tag{8}\n$$\né‡‡ç”¨EMç®—æ³•è¿›è¡Œè¿­ä»£ä¼˜åŒ–æ±‚è§£ï¼Œä¸‹é¢åšä¸ªç®€å•åœ°æ¨å¯¼ï¼š\n\nè‹¥å‚æ•°$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$èƒ½ä½¿(8)å¼æœ€å¤§åŒ–ï¼Œåˆ™ç”±$\\frac{\\partial LL(D)}{\\partial \\mu_i} = 0$æœ‰ï¼š\n$$\n\\sum_{j=1}^{m}\\frac{\\alpha_i.p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{l=1}^{k}\\alpha_l.p(x_j|\\mu_l,\\Sigma_l)}(x_j - \\mu_i) \\tag{9}\n$$\nç”±(6)å¼ä»¥åŠ$\\gamma_{ji} = p_{\\mathcal M}(z_j = i |x_j)$æœ‰\n$$\n\\mu_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}x_j}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{10}\n$$\nå³æ··åˆæˆåˆ†çš„å‡å€¼å¯ä»¥é€šè¿‡æ ·æœ¬çš„åŠ æƒå¹³å‡æ¥ä¼°è®¡ï¼Œæ ·æœ¬æƒé‡æ˜¯æ¯ä¸ªæ ·æœ¬å±äºè¯¥æˆåˆ†çš„åéªŒæ¦‚ç‡ï¼Œç±»ä¼¼çš„ï¼Œç”±$\\frac{\\partial LL(D)}{\\partial \\Sigma_i} = 0$å¯ä»¥å¾—åˆ°ï¼š\n$$\n\\Sigma_i = \\frac{\\sum_{j=1}^{m}\\gamma_{ji}(x_j-\\mu_i)(x_j-\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}} \\tag{11}\n$$\nå¯¹äºæ··åˆç³»æ•°ï¼Œä½¿ç”¨æ‹‰æ ¼æœ—æ—¥æ³•å¯ä»¥å¾—åˆ°ï¼š\n$$\n\\alpha_i = \\frac{1}{m}\\sum_{j=1}^{m}\\gamma_{ji} \\tag{12}\n$$\nå³æ¯ä¸ªé«˜æ–¯æˆåˆ†çš„æ··åˆç³»æ•°ç”±æ ·æœ¬å±äºè¯¥æˆåˆ†çš„å¹³å‡åéªŒæ¦‚ç‡ç¡®å®šã€‚\n\nç”±ä¸Šè¿°æ¨å¯¼å¯å¾—é«˜æ–¯æ··åˆæ¨¡å‹çš„EMç®—æ³•ï¼šåœ¨æ¯æ­¥è¿­ä»£ä¸­ï¼Œå…ˆæ ¹æ®å½“å‰å‚æ•°æ¥è®¡ç®—æ¯ä¸ªæ ·æœ¬å±äºé«˜æ–¯æˆåˆ†çš„åéªŒæ¦‚ç‡$\\gamma_{ji}$(**Eæ­¥**), å†æ ¹æ®å¼(10)-(12)æ›´æ–°å‚æ•°æ¨¡å‹$\\{(\\alpha_i,\\mu_i,\\Sigma_i)|1\\leq i \\leq k\\}$ï¼ˆ**Mæ­¥**ï¼‰.\n\n##### 2.2  ç®—æ³•æµç¨‹\n\nGMMç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š\n\n![](./èšç±»ç®—æ³•æ€»ç»“/gmm_ç®—æ³•æ­¥éª¤.png)\n\n##### 2.3 ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{*, Axis, DenseMatrix, DenseVector, argmax, det, max, norm, pinv, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport com.strings.utils.MatrixUtils\nimport org.slf4j.LoggerFactory\n\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass GMM(k:Int = 3,\n          max_iterations:Int = 2000,\n          tolerance:Double = 1e-8) {\n\n  private val logger = LoggerFactory.getLogger(classOf[GMM])\n\n  var means:Array[DenseVector[Double]] = new Array[DenseVector[Double]](k)\n  var vars:Array[DenseMatrix[Double]] = new Array[DenseMatrix[Double]](k)\n  var sample_assignments:DenseVector[Int] = _\n  var priors:DenseVector[Double] = _\n  var responsibility:DenseMatrix[Double] = _\n  var responsibilities:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n\n  def _initialize(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val X_lst = (0 until n_samples).map(X.t(::,_))\n    val rng =  new scala.util.Random()\n    priors = DenseVector.ones[Double](k) :/ k.toDouble\n    means = rng.shuffle(X_lst).take(k).toArray\n//    means = X_lst.take(k).toArray\n    for(i <- 0 until k){\n      vars(i) = MatrixUtils.calculate_covariance_matrix(X)\n    }\n  }\n\n  def multivariate_gaussian(X:DenseMatrix[Double],i:Int):DenseVector[Double]={\n    val n_features = X.cols\n    val mean = means(i)\n    val covar = vars(i)\n    val determinant = det(covar)\n    val likelihoods = DenseVector.zeros[Double](X.rows)\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n\n    for((sample,index) <- X_arr.zipWithIndex){\n      val d = n_features\n      val coeff = 1.0 / (math.pow(2 * Math.PI,d/2) * math.sqrt(determinant))\n      val gram = (sample :- mean).t * pinv(covar) * (sample :- mean)\n      val exponent = math.exp(-0.5 * gram)\n      likelihoods(index) = coeff * exponent\n    }\n    likelihoods\n  }\n\n  def _get_likelihoods(X:DenseMatrix[Double]):DenseMatrix[Double] = {\n    val n_samples = X.rows\n    val likelihoods = DenseMatrix.zeros[Double](n_samples,k)\n    for(i <- 0 until k){\n      likelihoods(::,i) := multivariate_gaussian(X,i)\n    }\n    likelihoods\n  }\n\n  def _expectation(X:DenseMatrix[Double]): Unit ={\n    val weighted_likelihoods = _get_likelihoods(X)(*,::).map(x => x :* priors)\n    val sum_likelihoods = sum(weighted_likelihoods,Axis._1)\n    responsibility = weighted_likelihoods(::,*).map(x => x :/ sum_likelihoods) // åˆ—é™¤\n    sample_assignments = argmax(responsibility,Axis._1)\n    responsibilities.append(max(responsibility,Axis._1))\n  }\n\n  def _maximization(X:DenseMatrix[Double]): Unit ={\n    for(i <- 0 until k){\n      val resp = responsibility(::,i)\n      val mean = sum(X(::,*).map(f => resp :* f),Axis._0) :/ sum(resp)\n      means(i) = mean.t\n      val diff = X(*,::).map(f => f :- mean.t)\n      val covariance = diff.t * diff(::,*).map(f => f :* resp) :/sum(resp) // æ³¨æ„diff(::,*)æ˜¯å–åˆ—è¿ç®—\n      vars(i) = covariance\n    }\n    val n_samples = X.rows\n    priors = sum(responsibility,Axis._0).t :/ n_samples.toDouble\n  }\n\n  def predict(X:DenseMatrix[Double]): DenseVector[Double] = {\n    _initialize(X)\n    var iter = 0\n    var flag = true\n    for (_ <- 0 until max_iterations if flag) {\n      iter += 1\n      _expectation(X)\n      _maximization(X)\n      breakable {\n        if (responsibilities.length < 2) {\n          break()\n        }else{\n          val n = responsibilities.length\n          val diff = norm(responsibilities(n-1) - responsibilities(n-2), 2)\n          println(diff)\n          if (diff <= tolerance) flag = false\n        }\n    }\n  }\n    logger.info(s\"$iter ä¹‹åæ”¶æ•›\")\n    _expectation(X)\n    sample_assignments.map(_.toDouble)\n  }\n\n}\n\nobject GMM{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val gmm = new GMM(max_iterations = 100)\n    gmm._initialize(DenseMatrix(data:_*))\n\n    val pred = gmm.predict(DenseMatrix(data:_*))\n    println(pred)\n\n  }\n}\n```\n\nè¯¦ç»†è¯·å‚è€ƒ:https://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/GMM.scala\n\n#### 3 DBSCANèšç±»\n\nå¯†åº¦èšç±»åˆ™æ˜¯åŸºäºå¯†åº¦çš„èšç±»ï¼Œå®ƒä»æ ·æœ¬åˆ†å¸ƒçš„è§’åº¦æ¥è€ƒå¯Ÿæ ·æœ¬ä¹‹é—´çš„å¯è¿æ¥æ€§ï¼Œå¹¶åŸºäºå¯è¿æ¥æ€§ï¼ˆå¯†åº¦å¯è¾¾ï¼‰ä¸æ–­æ‹“å±•ç–†åŸŸï¼ˆç±»ç°‡ï¼‰ã€‚\n\nDBSCANæ˜¯ä¸€ç§è‘—åçš„å¯†åº¦èšç±»ç®—æ³•ï¼Œå®ƒæ˜¯ä¸€ç»„â€œé‚»åŸŸâ€ï¼ˆneighbhoodï¼‰å‚æ•°($\\epsilon $,MinPts)æ¥åˆ»ç”»æ ·æœ¬åˆ†å¸ƒçš„ç´§å¯†ç¨‹åº¦.ç»™å®šæ•°æ®é›†$D = \\{x_1,x_2,...,x_m\\}$,å®šä¹‰ä¸‹é¢å‡ ä¸ªæ¦‚å¿µï¼š\n\n1. $\\epsilon$-é‚»åŸŸï¼šå¯¹äº$x_j \\in D$,å…¶$\\epsilon$-é‚»åŸŸåŒ…å«æ ·æœ¬é›†$D$ä¸­ä¸$x_j$çš„è·ç¦»ä¸å¤§äº$\\epsilon$çš„æ ·æœ¬ï¼Œå³$N_{\\epsilon}(x_j) = {x_i \\in D | dist(x_i,x_j) \\leq \\epsilon}$;\n2. æ ¸å¿ƒå¯¹è±¡(core object):è‹¥$x_j$çš„$\\epsilon$-é‚»åŸŸè‡³å°‘åŒ…å«MinPtsä¸ªæ ·æœ¬ï¼Œå³$|N_{\\epsilon}(x_j)| \\geq MinPts$,åˆ™$x_j$æ˜¯ä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡ï¼›\n3. å¯†åº¦ç›´è¾¾(directly density-reachable):è‹¥$x_j$ä½äº$x_i$çš„$\\epsilon$-é‚»åŸŸä¸­ï¼Œä¸”$x_i$æ˜¯æ ¸å¿ƒå¯¹è±¡ï¼Œåˆ™ç§°$x_j$æ˜¯ç”±$x_i$å¯†åº¦ç›´è¾¾ï¼›\n4. å¯†åº¦å¯è¾¾(density-reachable)ï¼šå¯¹$x_i$ä¸$x_j$ï¼Œè‹¥å­˜åœ¨æ ·æœ¬åºåˆ—$p_1,p_2,...,p_n$,å…¶ä¸­$p_1 = x_i,p_n = x_j$,ä¸”$p_{i+1}$ç”±$p_i$å¯†åº¦ç›´è¾¾ï¼Œåˆ™ç§°$x_j$ç”±$x_i$å¯†åº¦å¯è¾¾ï¼›\n5. å¯†åº¦ç›¸è¿(density-connected):å¯¹$x_i$ä¸$x_j$ï¼Œè‹¥å­˜åœ¨$x_k$ä½¿å¾—$x_i$ä¸$x_j$å‡ç”±$x_k$å¯†åº¦å¯è¾¾ï¼Œåˆ™ç§°$x_i$ä¸$x_j$å¯†åº¦ç›¸è¿ï¼›\n\nä¸‹å›¾ç»™å‡ºäº†ä¸Šè¿°æ¦‚å¿µçš„ç›´è§‚æ˜¾ç¤ºï¼š\n\n![](./èšç±»ç®—æ³•æ€»ç»“/dbscan_å¯†åº¦ç›´è¾¾.png)\n\n##### 3.1 ç®—æ³•æ­¥éª¤\n\nDBSCANçš„ç®—æ³•çš„æ€æƒ³æ˜¯æ‰¾å‡ºä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡æ‰€æœ‰å¯†åº¦å¯è¾¾çš„æ ·æœ¬é›†åˆå½¢æˆç°‡ã€‚é¦–å…ˆä»æ•°æ®é›†ä¸­ä»»é€‰ä¸€ä¸ªæ ¸å¿ƒå¯¹è±¡$A$ï¼Œæ‰¾å‡ºæ‰€æœ‰$A$å¯†åº¦å¯è¾¾çš„æ ·æœ¬é›†åˆï¼Œå°†è¿™äº›æ ·æœ¬å½¢æˆä¸€ä¸ªå¯†åº¦ç›¸è¿çš„ç±»ç°‡ï¼Œç›´åˆ°æ‰€æœ‰çš„æ ¸å¿ƒå¯¹è±¡éƒ½éå†å®Œã€‚DBSCANç®—æ³•çš„æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n\n![](./èšç±»ç®—æ³•æ€»ç»“/dbscan_ç®—æ³•æ­¥éª¤.png)\n\n##### 3.2 ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, sum}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\nimport scala.collection.mutable.ArrayBuffer\n\nclass DBSCAN(eps:Double = 1.0,\n             min_samples:Int = 5) {\n\n  var visited_samples:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n  var neighbors: Map[Int, Array[Int]] = Map()\n  var X:DenseMatrix[Double] = _\n  var clusters:ArrayBuffer[Array[Int]] = new ArrayBuffer[Array[Int]]()\n\n  def euclidean_distance(x1:DenseVector[Double],x2:DenseVector[Double]): Double ={\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def _get_neighbors(sample_i:Int): Array[Int] ={\n    val neighbors:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    val X_arr = (0 until X.rows).map(X.t(::,_))\n    val X_arr2 =  X_arr.indices.filter(i => i != sample_i).map(X_arr(_))\n    for((_sample,inx) <- X_arr2.zipWithIndex){\n      val dist = euclidean_distance(_sample,X_arr(sample_i))\n      if(dist < eps){\n        neighbors.append(inx)\n      }\n    }\n    neighbors.toArray\n  }\n\n  def _expand_cluster(sample_i:Int, neighbor:Array[Int]): Array[Int] ={\n    val cluster:ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    cluster.append(sample_i)\n    for(neighbor_i <- neighbor){\n      if(!visited_samples.contains(neighbor_i)){\n        visited_samples.append(neighbor_i)\n        neighbors += (neighbor_i -> _get_neighbors(neighbor_i) )\n        if (neighbors(neighbor_i).length >= min_samples){   //        neighbors.get(neighbor_i).size ç»“æœæ˜¯1\n          val expanded_cluster = _expand_cluster(neighbor_i,neighbors(neighbor_i))\n          cluster.append(expanded_cluster:_*)\n        }else{\n          cluster.append(neighbor_i)\n        }\n      }\n    }\n    cluster.toArray\n  }\n\n  def _get_cluster_labels(): Array[Int] ={\n    val labels = Array.fill(X.rows)(clusters.length)\n    for((cluster,cluster_i) <- clusters.zipWithIndex){\n      for(sample_i <- cluster){\n        labels(sample_i) = cluster_i\n      }\n    }\n    labels\n  }\n\n  def predict(XX:DenseMatrix[Double]): Array[Int] ={\n    X = XX\n    visited_samples = new ArrayBuffer[Int]()\n    neighbors = Map()\n    val n_samples = X.rows\n    for(sample_i <- 0 until n_samples){\n      breakable {\n        if (visited_samples.contains(sample_i)) {\n          break()\n        }else{\n          neighbors += (sample_i -> _get_neighbors(sample_i))\n          if(neighbors.get(sample_i).size >= min_samples){\n            visited_samples.append(sample_i)\n          }\n          val new_cluster = _expand_cluster(sample_i,neighbors(sample_i))\n          clusters.append(new_cluster)\n        }\n      }\n    }\n    _get_cluster_labels()\n  }\n\n}\n\nobject DBSCAN{\n  def main(args: Array[String]): Unit = {\n\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val dbscan = new DBSCAN(eps = .7,min_samples = 5)\n\n    val pred = dbscan.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n  }\n}\n```\n\nè¯¦ç»†è¯·å‚è€ƒï¼šhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/DBSCAN.scala\n\n#### 4 PAMèšç±»ç®—æ³•\n\nK-meansæ˜¯æ¯æ¬¡é€‰**ç°‡çš„**å‡å€¼**ä½œä¸ºæ–°çš„ä¸­å¿ƒï¼Œè¿­ä»£ç›´åˆ°ç°‡ä¸­å¯¹è±¡åˆ†å¸ƒä¸å†å˜åŒ–ã€‚å…¶ç¼ºç‚¹æ˜¯å¯¹äºç¦»ç¾¤ç‚¹æ˜¯æ•æ„Ÿçš„ï¼Œå› ä¸ºä¸€ä¸ªå…·æœ‰å¾ˆå¤§æç«¯å€¼çš„å¯¹è±¡ä¼šæ‰­æ›²æ•°æ®åˆ†å¸ƒã€‚é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è€ƒè™‘æ–°çš„ç°‡ä¸­å¿ƒä¸é€‰æ‹©å‡å€¼è€Œæ˜¯é€‰æ‹©**ç°‡å†…çš„æŸä¸ªå¯¹è±¡**ï¼Œåªè¦ä½¿æ€»çš„ä»£ä»·é™ä½å°±å¯ä»¥ã€‚\n\nPAMï¼ˆpartitioning around medoidï¼Œå›´ç»•ä¸­å¿ƒç‚¹çš„åˆ’åˆ†ï¼‰æ˜¯å…·æœ‰ä»£è¡¨æ€§çš„k-medoidsç®—æ³•ã€‚\n\nå®ƒæœ€åˆéšæœºé€‰æ‹©kä¸ªå¯¹è±¡ä½œä¸ºä¸­å¿ƒç‚¹ï¼Œè¯¥ç®—æ³•åå¤çš„ç”¨éä»£è¡¨å¯¹è±¡ï¼ˆéä¸­å¿ƒç‚¹ï¼‰ä»£æ›¿ä»£è¡¨å¯¹è±¡ï¼Œè¯•å›¾æ‰¾å‡ºæ›´å¥½çš„ä¸­å¿ƒç‚¹ï¼Œä»¥æ”¹è¿›èšç±»çš„è´¨é‡ã€‚ Kå‡å€¼èšç±»ä¸€èˆ¬ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ï¼Œè€ŒPAMå¯ä»¥ä½¿ç”¨ä»»æ„çš„è·ç¦»æ¥è®¡ç®—ã€‚å› æ­¤ï¼Œ PAMå¯ä»¥å®¹çº³æ··åˆæ•°æ®ç±»å‹ï¼Œå¹¶ä¸”ä¸ä»…é™äºè¿ç»­å˜é‡ã€‚\n\n##### 4.1  ç®—æ³•æ­¥éª¤\n\nâ€‹       PAMç®—æ³•å¦‚ä¸‹ï¼š\nâ€‹       (1) éšæœºé€‰æ‹©Kä¸ªè§‚æµ‹å€¼ï¼ˆæ¯ä¸ªéƒ½ç§°ä¸ºä¸­å¿ƒç‚¹ï¼‰ï¼›\nâ€‹       (2) è®¡ç®—è§‚æµ‹å€¼åˆ°å„ä¸ªä¸­å¿ƒçš„è·ç¦»/ç›¸å¼‚æ€§ï¼›\nâ€‹       (3) æŠŠæ¯ä¸ªè§‚æµ‹å€¼åˆ†é…åˆ°æœ€è¿‘çš„ä¸­å¿ƒç‚¹ï¼›\nâ€‹       (4) è®¡ç®—æ¯ä¸ªä¸­å¿ƒç‚¹åˆ°æ¯ä¸ªè§‚æµ‹å€¼çš„è·ç¦»çš„æ€»å’Œï¼ˆæ€»æˆæœ¬ï¼‰ï¼›\nâ€‹       (5) é€‰æ‹©ä¸€ä¸ªè¯¥ç±»ä¸­ä¸æ˜¯ä¸­å¿ƒçš„ç‚¹ï¼Œå¹¶å’Œä¸­å¿ƒç‚¹äº’æ¢ï¼›\nâ€‹       (6) é‡æ–°æŠŠæ¯ä¸ªç‚¹åˆ†é…åˆ°è·å®ƒæœ€è¿‘çš„ä¸­å¿ƒç‚¹ï¼›\nâ€‹       (7) å†æ¬¡è®¡ç®—æ€»æˆæœ¬ï¼›\nâ€‹       (8) å¦‚æœæ€»æˆæœ¬æ¯”æ­¥éª¤(4)è®¡ç®—çš„æ€»æˆæœ¬å°‘ï¼ŒæŠŠæ–°çš„ç‚¹ä½œä¸ºä¸­å¿ƒç‚¹ï¼›\nâ€‹       (9) é‡å¤æ­¥éª¤(5)ï½(8)ç›´åˆ°ä¸­å¿ƒç‚¹ä¸å†æ”¹å˜ã€‚\n\n##### 4.2 ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, squaredDistance}\nimport com.strings.data.Data\nimport com.strings.model.metric.Metric\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\nimport util.control.Breaks.breakable\nimport util.control.Breaks.break\n\n/**\n *  Partitioning (clustering) of the data into k clusters â€œaround medoidsâ€, a more robust version of K-means.\n *\n * @param k nums of cluster\n */\n\n\nclass PAM(k:Int = 2,seed:Long = 1234L) {\n\n  def _init_random_medoids(X: DenseMatrix[Double]): IndexedSeq[DenseVector[Double]] ={\n    val n_samples = X.rows\n    val data = (0 until n_samples).map(X.t(::,_))\n    val rng  = new Random(seed)\n    rng.shuffle(data).take(k)\n  }\n\n  def _closet_medoid(sample:DenseVector[Double],medoids:IndexedSeq[DenseVector[Double]]): Int ={\n    val distWithIndex =  medoids.zipWithIndex.map(x =>\n      (squaredDistance(x._1,sample),x._2)\n    ).minBy(_._1)\n    distWithIndex._2\n  }\n\n  def _create_clusters(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[Array[Int]] ={\n    val clusterss = new Array[Int](X.rows)\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((sample,inx) <- data.zipWithIndex){\n      val medoid_i = _closet_medoid(sample,medoids)\n      clusterss(inx) = medoid_i\n    }\n    clusterss.zipWithIndex.groupBy(_._1).toArray.sortBy(_._1).map(_._2.map(_._2))\n  }\n\n  def _calculate_cost(X:DenseMatrix[Double],clusters:Array[Array[Int]],medoids:IndexedSeq[DenseVector[Double]]):Double={\n    var cost = 0.0\n    val data = (0 until X.rows).map(X.t(::,_))\n    for((cluster,i) <- clusters.zipWithIndex){\n      val medoid = medoids(i)\n      for(sample_i <- cluster){\n        cost += squaredDistance(data(sample_i),medoid)\n      }\n    }\n    cost\n  }\n\n  def _get_cluster_labels(clusters:Array[Array[Int]],X:DenseMatrix[Double]): Array[Int] ={\n    val y_pred = Array.fill(X.rows)(0)\n    for(cluster_i <- 0 until clusters.length){\n      val cluster = clusters(cluster_i)\n      for(sample_i <- cluster){\n        y_pred(sample_i) = cluster_i\n      }\n    }\n    y_pred\n  }\n  def _get_no_medoids(X:DenseMatrix[Double],medoids:IndexedSeq[DenseVector[Double]]): Array[DenseVector[Double]] ={\n    val non_medoids:ArrayBuffer[DenseVector[Double]] = new ArrayBuffer[DenseVector[Double]]()\n    val data = (0 until X.rows).map(X.t(::,_))\n\n    for(sample <- data){\n      if(!medoids.contains(sample)) non_medoids.append(sample)\n    }\n    non_medoids.toArray\n  }\n\n  def predict(X:DenseMatrix[Double]): Array[Int] ={\n    var medoids = _init_random_medoids(X)\n    val clusters = _create_clusters(X,medoids)\n    var cost = _calculate_cost(X, clusters, medoids)\n    breakable {\n      while (true) {\n        var best_medoids = medoids\n        var lowest_cost = cost\n        for (medoid <- medoids) {\n          val non_medoids = _get_no_medoids(X, medoids)\n          for (sample <- non_medoids) {\n            val new_medoids = new Array[DenseVector[Double]](medoids.length)\n            for (i <- 0 until medoids.length) {\n              new_medoids(i) = medoids(i)\n            }\n            val inx: IndexedSeq[Int] = medoids.indices.filter(i => medoids(i) == medoid)\n            inx.foreach(i => new_medoids(i) = sample)\n\n            val new_clusters = _create_clusters(X, new_medoids)\n            val new_cost = _calculate_cost(X, new_clusters, new_medoids)\n\n            if (new_cost < lowest_cost) {\n              lowest_cost = new_cost\n              best_medoids = new_medoids\n            }\n          }\n        }\n        if (lowest_cost < cost) {\n          cost = lowest_cost\n          medoids = best_medoids\n        } else {\n          break()\n        }\n      }\n    }\n    val finaly_clusters = _create_clusters(X,medoids)\n    _get_cluster_labels(finaly_clusters,X)\n  }\n\n}\n\nobject PAM{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4)).toList\n    val target = irisData.map(_.apply(4))\n\n    val pam = new PAM(k = 3)\n\n    val pred = pam.predict(DenseMatrix(data:_*))\n    println(pred.toList)\n    val acc =  Metric.accuracy(pred.map(_.toDouble),target) * 100\n    println(f\"å‡†ç¡®ç‡ä¸º: $acc%-5.2f%%\")\n  }\n}\n\n```\n\nè¯¦ç»†è¯·å‚è€ƒï¼šhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/PAM.scala\n\n#### 5. LVQèšç±»\n\nLVQåˆç§°â€œå­¦ä¹ å‘é‡é‡åŒ–â€(Learning Vector Quantization)ä¹Ÿæ˜¯è¯•å›¾æ‰¾åˆ°ä¸€ç»„åŸå‹å‘é‡æ¥åˆ»ç”»èšç±»ç»“æ„ï¼Œä½†ä¸ä¸€èˆ¬èšç±»ç®—æ³•ä¸åŒçš„æ˜¯ï¼ŒLVQå‡è®¾æ ·æœ¬å¸¦æœ‰ç±»åˆ«æ ‡è®°ï¼Œå­¦ä¹ è¿‡ç¨‹åˆ©ç”¨æ ·æœ¬çš„è¿™äº›ç›‘ç£ä¿¡æ¯æ¥è¾…åŠ©èšç±»ã€‚\n\nç»™å®šæ ·æœ¬é›†$\\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\\}$,æ¯ä¸ªæ ·æœ¬$x_j$æ˜¯ç”±$n$ä¸ªå±æ€§æè¿°çš„ç‰¹å¾å‘é‡$(x_{j1};x_{j2};...;x_{jn})$,$y_j \\in \\mathcal Y$æ˜¯æ ·æœ¬$x_j$çš„ç±»åˆ«æ ‡è®°. LVQçš„ç›®æ ‡æ˜¯å­¦å¾—ä¸€ç»„$n$ç»´åŸå‹å‘é‡$\\{p_1,p_2,...,p_q\\}$,æ¯ä¸ªåŸå‹å‘é‡ä»£è¡¨ä¸€ä¸ªèšç±»ç°‡ï¼Œç°‡æ ‡è®°ä¸º$t_i \\in \\mathcal Y$.\n\n##### 5.1 ç®—æ³•æ­¥éª¤\n\nLVQçš„ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š\n\n![](./èšç±»ç®—æ³•æ€»ç»“/lvq_ç®—æ³•æ­¥éª¤.png)\n\n##### 5.2 ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector, argmin, sum}\n\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.Random\n\nclass LVQ(t:Array[Int],\n          lr:Double = 0.1,\n          nums_iters:Int = 400) {\n\n  val c = t.distinct.length\n  val q = t.length\n  var C: Map[Int, ArrayBuffer[Int]] = Map()\n  var p: DenseMatrix[Double] = _\n  var labels: DenseVector[Int] = _\n\n  def euclidean_distance(x1: DenseVector[Double], x2: DenseVector[Double]): Double = {\n    require(x1.length == x2.length)\n    math.sqrt(sum((x1 :- x2) :* (x1 :- x2)))\n  }\n\n  def fit(X: DenseMatrix[Double], y: DenseVector[Int]) = {\n    p = DenseMatrix.zeros[Double](q, X.cols)\n    for (i <- 0 until q) {\n      C += (i -> ArrayBuffer[Int]())\n      val candidate_indices = y.toArray.indices.filter(f => y(f) == t(i))\n      val target_indice = Random.shuffle(candidate_indices.toList).take(1).apply(0)\n      p(i, ::) := X(target_indice, ::)\n    }\n\n\n    var p_arr = (0 until p.rows).map(p.t(::, _))\n    for (_ <- 0 until nums_iters) {\n      val j = Random.shuffle(Range(0, y.length).toList).take(1).apply(0)\n      val x_j = X(j, ::).t\n      val d = p_arr.map(f => euclidean_distance(f, x_j))\n      val idx: Int = argmin(d.toArray)\n      if (y(j) == t(idx)) {\n        p(idx, ::) := p(idx, ::) :+ ((X(j, ::) :- p(idx, ::)) :* lr)  // :+ å’Œ :* è¿ç®—ä¼˜å…ˆçº§ä¸€è‡´\n      } else {\n        p(idx, ::) := p(idx, ::) :- ((X(j, ::) :- p(idx, ::)) :* lr)\n      }\n\n    }\n    p_arr = (0 until p.rows).map(p.t(::, _))\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      C(idx).append(j)\n    }\n\n    labels = DenseVector.zeros[Int](X.rows)\n    for (i <- 0 until q) {\n      for (j <- C(i)) {\n        labels(j) = i\n      }\n    }\n  }\n\n  def predict(X: DenseMatrix[Double]): DenseVector[Int] = {\n    val p_arr = (0 until p.rows).map(p.t(::, _))\n    val preds_y: ArrayBuffer[Int] = new ArrayBuffer[Int]()\n    for (j <- 0 until X.rows) {\n      val d = p_arr.map(f => euclidean_distance(f, X(j, ::).t))\n      val idx: Int = argmin(DenseVector(d.toArray))\n      preds_y.append(t(idx))\n    }\n    DenseVector(preds_y.toArray)\n  }\n}\n\nobject LVQ{\n  def main(args: Array[String]): Unit = {\n\n    val X = Array(Array(0.697,0.460),Array(0.774,0.376),Array(0.634,0.264),Array(0.608,0.318),Array(0.556,0.215),\n                  Array(0.403,0.237),Array(0.481,0.149),Array(0.437,0.211),Array(0.666,0.091),Array(0.243,0.267),\n                  Array(0.245,0.057),Array(0.343,0.099),Array(0.639,0.161),Array(0.657,0.198),Array(0.360,0.370),\n                  Array(0.593,0.042),Array(0.719,0.103),Array(0.359,0.188),Array(0.339,0.241),Array(0.282,0.257),\n                  Array(0.748,0.232),Array(0.714,0.346),Array(0.483,0.312),Array(0.478,0.437),Array(0.525,0.369),\n                  Array(0.751,0.489),Array(0.532,0.472),Array(0.473,0.376),Array(0.725,0.445),Array(0.446,0.459))\n\n   val XX = DenseMatrix(X:_*)\n   val y = DenseVector.zeros[Int](XX.rows)\n\n    for(i <- 9 until 21){\n      y(i) = 1\n    }\n\n    val t = Array(0,1,1,0,0)\n    println(y)\n    val lvq = new LVQ(t)\n    lvq.fit(XX,y)\n\n    println(lvq.C)\n    println(lvq.labels)\n    println(lvq.predict(XX))\n\n  }\n}\n```\n\nè¯¦ç»†ä»£ç è¯·å‚è€ƒï¼š\n\nhttps://github.com/StringsLi/ml_scratch_scala/blob/master/src/main/scala/com/strings/model/cluster/LVQ.scala\n\nä»£ç å®ç°å‚è€ƒäº†pythonä»£ç ï¼š\n\nhttps://github.com/fengyang95/tiny_ml/blob/master/tinyml/cluster/LVQ.py\n\n#### 6 å±‚æ¬¡èšç±»\n\nå±‚æ¬¡èšç±»(hierarchical clustering)æ˜¯ä¸€ç§åŸºäºæ ‘å½¢ç»“æ„çš„èšç±»æ–¹æ³•ï¼Œå¸¸ç”¨çš„æ˜¯**è‡ªåº•å‘ä¸Š**çš„ç»“åˆç­–ç•¥ï¼ˆ**AGNESç®—æ³•**ï¼‰ï¼Œå®ƒå°†æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬çœ‹ä½œä¸€ä¸ªåˆå§‹èšç±»ç°‡ï¼Œç„¶åå†ç®—æ³•è¿è¡Œçš„æ¯ä¸€æ­¥ä¸­æ‰¾åˆ°è·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªèšç±»ç°‡è¿›è¡Œåˆå¹¶ï¼Œè¯¥è¿‡ç¨‹ä¸æ–­é‡å¤ï¼Œç›´è‡³è¾¾åˆ°é¢„è®¾çš„èšç±»ç°‡ä¸ªæ•°ã€‚è¿™é‡Œçš„å…³é”®æ˜¯å¦‚ä½•è®¡ç®—èšç±»ç°‡ä¹‹é—´çš„è·ç¦»ã€‚å®é™…ä¸Šï¼Œæ¯ä¸ªç°‡æ˜¯ä¸€ä¸ªæ ·æœ¬é›†åˆï¼Œåªéœ€è¦é‡‡ç”¨é›†åˆçš„æŸç§è·ç¦»å³å¯ã€‚ä¾‹å¦‚ï¼Œç»™å®šèšç±»ç°‡$C_i$ä¸$C_j$,å¯ä»¥é€šè¿‡ä¸‹é¢çš„å¼å­æ¥è®¡ç®—è·ç¦»ï¼š\n\næœ€å°è·ç¦»ï¼š\n$$\nd_{\\min}(C_i,C_j) = \\min _{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{15}\n$$\næœ€å¤§è·ç¦»ï¼š\n$$\nd_{\\max}(C_i,C_j) = \\max_{x \\in C_i,z \\in C_j}dist(x,z)  \\tag{16}\n$$\nå¹³å‡è·ç¦»ï¼š\n$$\nd_{avg}(C_i,C_j) = \\frac{1}{|C_i||C_j|}dist(x,z)  \\tag{17}\n$$\næ˜¾ç„¶ï¼Œæœ€å°è·ç¦»ç”±ä¸¤ä¸ªç°‡çš„æœ€è¿‘çš„æ ·æœ¬å†³å®šï¼›æœ€å¤§è·ç¦»ç”±ä¸¤ä¸ªç°‡çš„æœ€è¿œæ ·æœ¬å†³å®šï¼Œè€Œå¹³å‡è·ç¦»åˆ™ç”±ä¸¤ä¸ªç°‡çš„æ‰€æœ‰æ ·æœ¬å†³å®šã€‚\n\n##### 6.1  ç®—æ³•æ­¥éª¤\n\nAGNES ç®—æ³•æ­¥éª¤å¦‚ä¸‹,åœ¨1-9è¡Œï¼Œç®—æ³•å…ˆå¯¹ä»…å«ä¸€ä¸ªæ ·æœ¬çš„åˆå§‹èšç±»ç°‡å’Œç›¸åº”çš„è·ç¦»è¿›è¡Œåˆå§‹åŒ–ï¼Œç„¶ååœ¨11-23è¡Œï¼ŒAGNESä¸æ–­åˆå¹¶è·ç¦»æœ€è¿‘çš„èšç±»ç°‡ï¼Œå¹¶å¯¹åˆå¹¶å¾—åˆ°çš„èšç±»ç°‡çš„è·ç¦»çŸ©é˜µè¿›è¡Œæ›´æ–°ï¼›ä¸Šè¿°è¿‡ç¨‹ä¸æ–­é‡å¤ï¼Œç›´è‡³è¾¾åˆ°é¢„è®¾çš„èšç±»ç°‡æ•°ã€‚\n\n![](./èšç±»ç®—æ³•æ€»ç»“/hc_ç®—æ³•æ­¥éª¤.png)\n\n##### 6.2 ç®—æ³•å®ç°\n\n```scala\npackage com.strings.model.cluster\n\nimport breeze.linalg.{DenseMatrix, DenseVector}\nimport com.strings.data.Data\nimport com.strings.utils.MatrixUtils\n\nimport scala.collection.mutable.ArrayBuffer\n\n\ncase class ClusterNode(vector:DenseVector[Double],\n                       id:Int,\n                       left:ClusterNode = null,\n                       right:ClusterNode = null,\n                       distance:Double = -1.0,\n                       count:Int = 1)\n\nclass HierarchicalCluster(k:Int) {\n\n  var labels:DenseVector[Int] = _\n\n  def fit(X:DenseMatrix[Double]): Unit ={\n    val n_samples = X.rows\n    val n_features = X.cols\n    val X_arr = (0 until n_samples).map(X.t(::,_))\n    val nodes:ArrayBuffer[ClusterNode] = new ArrayBuffer[ClusterNode]()\n    for((sample,inx) <- X_arr.zipWithIndex){\n      nodes.append(ClusterNode(sample,inx))\n    }\n    labels = DenseVector.ones[Int](n_samples) :* (-1)\n    var distances: Map[(Int, Int), Double] = Map()\n    var curret_cluster_id = -1\n    while (nodes.length > k){\n      var min_dist = Double.MaxValue\n      val nodes_len = nodes.length\n      var closest_part:(Int, Int) = 0 -> 0\n      for(i <- 0 until nodes_len - 1){\n        for(j <- i+1 until nodes_len){\n          val d_key = nodes(i).id -> nodes(j).id\n          if(!distances.contains(d_key)){\n            distances += (d_key -> MatrixUtils.euclidean_distance(nodes(i).vector,nodes(j).vector))\n          }\n          val d = distances(d_key)\n          if(d < min_dist){\n            min_dist = d\n            closest_part = i -> j\n          }\n        }\n      }\n\n      val part1 = closest_part._1\n      val part2 = closest_part._2\n      val node1 = nodes(part1)\n      val node2 = nodes(part2)\n      val new_vec = DenseVector.ones[Double](n_features)\n      for(i <- 0 until n_features){\n        new_vec(i) = (node1.vector(i) * node1.count + node2.vector(i) * node2.count)/\n          (node1.count + node2.count)\n      }\n      val new_count = node1.count + node2.count\n      val new_node = ClusterNode(new_vec,curret_cluster_id,node1,node2, min_dist,new_count)\n\n      curret_cluster_id -= 1\n      nodes.remove(part2)\n      nodes.remove(part1)\n      nodes.append(new_node)\n    }\n    calc_label(nodes)\n\n  }\n\n  def calc_label(nodes:ArrayBuffer[ClusterNode]): Unit ={\n    for((node,inx) <- nodes.zipWithIndex){\n      leaf_traveral(node,inx)\n    }\n  }\n\n  def leaf_traveral(node:ClusterNode,label:Int): Unit ={\n    if(node.left == null && node.right == null){\n      labels(node.id) = label\n    }\n    if(node.left != null){\n      leaf_traveral(node.left,label)\n    }\n    if(node.right != null){\n      leaf_traveral(node.right,label)\n    }\n  }\n}\n\nobject HierarchicalCluster{\n  def main(args: Array[String]): Unit = {\n    val irisData = Data.irisData\n    val data = irisData.map(_.slice(0,4))\n    val dd = DenseMatrix(data:_*)\n    val hc = new HierarchicalCluster(k=3)\n    hc.fit(dd)\n    println(hc.labels)\n  }\n}\n\n```\n\nç®—æ³•å®ç°å‚è€ƒäº†ï¼š\n\nhttps://zhuanlan.zhihu.com/p/32438294\n\n\n\n#### å‚è€ƒæ–‡çŒ®\n\n1. å‘¨å¿—å æœºå™¨å­¦ä¹  - èšç±»éƒ¨åˆ†\n\np.s. è¯¥æ€»ç»“ä¸»è¦ä»‹ç»æ¯ä¸ªèšç±»ç®—æ³•çš„ç®—æ³•æ­¥éª¤ï¼Œä»¥åŠscalaçš„ç®€å•å®ç°ï¼Œå¹¶æ²¡æœ‰å¤šæ³¨é‡æ•ˆç‡ï¼Œä»…ä¾›è‡ªå·±å­¦ä¹ ä½¿ç”¨ã€‚"},{"title":"adaboostç®—æ³•","url":"/2020/05/08/adaboostç®—æ³•/","content":"Boosting, ä¹Ÿç§°ä¸ºå¢å¼ºå­¦ä¹ æˆ–æå‡æ³•ï¼Œæ˜¯ä¸€ç§é‡è¦çš„é›†æˆå­¦ä¹ æŠ€æœ¯ï¼Œ èƒ½å¤Ÿå°†é¢„æµ‹ç²¾åº¦ä»…æ¯”éšæœºçŒœåº¦ç•¥é«˜çš„å¼±å­¦ä¹ å™¨å¢å¼ºä¸ºé¢„æµ‹ç²¾åº¦é«˜çš„å¼ºå­¦ä¹ å™¨ï¼Œè¿™åœ¨ç›´æ¥æ„é€ å¼ºå­¦ä¹ å™¨éå¸¸å›°éš¾çš„æƒ…å†µä¸‹ï¼Œä¸ºå­¦ä¹ ç®—æ³•çš„è®¾è®¡æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ–°æ€è·¯å’Œæ–°æ–¹æ³•ã€‚å…¶ä¸­æœ€ä¸ºæˆåŠŸåº”ç”¨çš„æ˜¯ï¼ŒYoav Freundå’ŒRobert Schapireåœ¨1995å¹´æå‡ºçš„AdaBoostç®—æ³•ã€‚\n\nâ€‹      AdaBoostæ˜¯è‹±æ–‡\"Adaptive Boosting\"ï¼ˆè‡ªé€‚åº”å¢å¼ºï¼‰çš„ç¼©å†™ï¼Œå®ƒçš„è‡ªé€‚åº”åœ¨äºï¼šå‰ä¸€ä¸ªåŸºæœ¬åˆ†ç±»å™¨è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬çš„æƒå€¼ä¼šå¢å¤§ï¼Œè€Œæ­£ç¡®åˆ†ç±»çš„æ ·æœ¬çš„æƒå€¼ä¼šå‡å°ï¼Œå¹¶å†æ¬¡ç”¨æ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºæœ¬åˆ†ç±»å™¨ã€‚åŒæ—¶ï¼Œåœ¨æ¯ä¸€è½®è¿­ä»£ä¸­ï¼ŒåŠ å…¥ä¸€ä¸ªæ–°çš„å¼±åˆ†ç±»å™¨ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªé¢„å®šçš„è¶³å¤Ÿå°çš„é”™è¯¯ç‡æˆ–è¾¾åˆ°é¢„å…ˆæŒ‡å®šçš„æœ€å¤§è¿­ä»£æ¬¡æ•°æ‰ç¡®å®šæœ€ç»ˆçš„å¼ºåˆ†ç±»å™¨ã€‚\n\n##### ç®—æ³•è¿‡ç¨‹\n\nç»™å®šè®­ç»ƒæ•°æ®é›†ï¼š $(x_1,y_1),...,(x_N,y_N)$ï¼Œå…¶ä¸­ $y_i \\in \\{1,-1\\}$ï¼Œç”¨äºè¡¨ç¤ºè®­ç»ƒæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾$i=1,...,N$ã€‚Adaboostçš„ç›®çš„å°±æ˜¯ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ä¸€ç³»åˆ—å¼±åˆ†ç±»å™¨æˆ–åŸºæœ¬åˆ†ç±»å™¨ï¼Œç„¶åå°†è¿™äº›å¼±åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªå¼ºåˆ†ç±»å™¨ã€‚\n\n##### ç®—æ³•è¯¦ç»†æ­¥éª¤\n\n1. åˆå§‹åŒ–æ•°æ®çš„æƒå€¼åˆ†å¸ƒ\n   $$\n   D_1 = (w_{11},...,w_{1i},...,w_{1N}),w_{1i} = \\frac{1}{N},i=1,2,...,N\n   $$\n\n2. å¯¹$m=1,2,...,M$\n\n   (a) ä½¿ç”¨å…·æœ‰æƒå€¼åˆ†å¸ƒ$D_m$çš„è®­ç»ƒæ•°æ®é›†å­¦ä¹ ï¼Œå¾—åˆ°åŸºæœ¬åˆ†ç±»å™¨\n   $$\n   G_m(x):f \\rightarrow  \\{-1,+1\\}\n   $$\n   (b) è®¡ç®—$G_m(x)$åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„åˆ†ç±»è¯¯å·®ç‡\n   $$\n   e_m = P(G_m(x_i) \\neq y_i) = \\sum_{i=1}^Nw_{mi}I(G_m(x_i)\\neq y_i)\n   $$\n   (c) è®¡ç®—$G_m(x)$çš„ç³»æ•°\n   $$\n   \\alpha_m = \\frac{1}{2}\\log\\frac{1-e_m}{e_m}\n   $$\n   è¿™é‡Œçš„å¯¹æ•°æ˜¯è‡ªç„¶å¯¹æ•°\n\n   (d) æ›´æ–°è®­ç»ƒæ•°æ®é›†çš„æƒå€¼åˆ†å¸ƒ\n   $$\n   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})\\\\\n   w_{m+1,i} = \\frac{w_{mi}}{Z_m}\\exp(-\\alpha_my_iG_m(x_i)),i=1,2,..,N\n   $$\n   è¿™é‡Œï¼Œ$Z_m$æ˜¯è§„èŒƒåŒ–å› å­\n   $$\n   Z_m = \\sum_{i=1}^Nw_{mi}\\exp(-\\alpha_my_iG_m(x_i))\n   $$\n   å®ƒä½¿$D_{m+1}$æˆä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ\n\n3. æ„å»ºåŸºæœ¬åˆ†ç±»å™¨çš„çº¿æ€§ç»„åˆ\n   $$\n   f(x) = \\sum_{m=1}^{M}\\alpha_mG_m(x)\n   $$\n   å¾—åˆ°æœ€ç»ˆçš„åˆ†ç±»å™¨\n   $$\n   G(x) = sign(f(x)) = sign(\\sum_{m=1}^M\\alpha_mG_m(x))\n   $$\n   å¯¹AdaBoostç®—æ³•åšä¸‹é¢çš„è¯´æ˜ï¼š\n\n    ï¼ˆ1ï¼‰é¦–å…ˆï¼Œæ˜¯åˆå§‹åŒ–è®­ç»ƒæ•°æ®çš„æƒå€¼åˆ†å¸ƒ$D_1$ã€‚å‡è®¾æœ‰$N$ä¸ªè®­ç»ƒæ ·æœ¬æ•°æ®ï¼Œåˆ™æ¯ä¸€ä¸ªè®­ç»ƒæ ·æœ¬æœ€å¼€å§‹æ—¶ï¼Œéƒ½è¢«èµ‹äºˆç›¸åŒçš„æƒå€¼$w_1 = \\frac{1}{N}$ã€‚\n\n    ï¼ˆ2ï¼‰ç„¶åï¼Œè®­ç»ƒå¼±åˆ†ç±»å™¨$h_i$ã€‚å…·ä½“è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯ï¼šå¦‚æœæŸä¸ªè®­ç»ƒæ ·æœ¬ç‚¹ï¼Œè¢«å¼±åˆ†ç±»å™¨$h_i$å‡†ç¡®åœ°åˆ†ç±»ï¼Œé‚£ä¹ˆåœ¨æ„é€ ä¸‹ä¸€ä¸ªè®­ç»ƒé›†ä¸­ï¼Œå®ƒå¯¹åº”çš„æƒå€¼è¦å‡å°ï¼›ç›¸åï¼Œå¦‚æœæŸä¸ªè®­ç»ƒæ ·æœ¬ç‚¹è¢«é”™è¯¯åˆ†ç±»ï¼Œé‚£ä¹ˆå®ƒçš„æƒå€¼å°±åº”è¯¥å¢å¤§ã€‚æƒå€¼æ›´æ–°è¿‡çš„æ ·æœ¬é›†è¢«ç”¨äºè®­ç»ƒä¸‹ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹å¦‚æ­¤è¿­ä»£åœ°è¿›è¡Œä¸‹å»ã€‚\n\n    ï¼ˆ3ï¼‰æœ€åï¼Œå°†å„ä¸ªè®­ç»ƒå¾—åˆ°çš„å¼±åˆ†ç±»å™¨ç»„åˆæˆä¸€ä¸ªå¼ºåˆ†ç±»å™¨ã€‚å„ä¸ªå¼±åˆ†ç±»å™¨çš„è®­ç»ƒè¿‡ç¨‹ç»“æŸåï¼ŒåŠ å¤§åˆ†ç±»è¯¯å·®ç‡å°çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ï¼Œä½¿å…¶åœ¨æœ€ç»ˆçš„åˆ†ç±»å‡½æ•°ä¸­èµ·ç€è¾ƒå¤§çš„å†³å®šä½œç”¨ï¼Œè€Œé™ä½åˆ†ç±»è¯¯å·®ç‡å¤§çš„å¼±åˆ†ç±»å™¨çš„æƒé‡ï¼Œä½¿å…¶åœ¨æœ€ç»ˆçš„åˆ†ç±»å‡½æ•°ä¸­èµ·ç€è¾ƒå°çš„å†³å®šä½œç”¨ã€‚\n\n     æ¢è€Œè¨€ä¹‹ï¼Œè¯¯å·®ç‡ä½çš„å¼±åˆ†ç±»å™¨åœ¨æœ€ç»ˆåˆ†ç±»å™¨ä¸­å çš„æƒé‡è¾ƒå¤§ï¼Œå¦åˆ™è¾ƒå°ã€‚"},{"title":"é™ç»´æ–¹æ³•-æ€»ç»“","url":"/2020/05/06/é™ç»´æ–¹æ³•-æ€»ç»“/","content":"#### 1. é™ç»´æ¦‚è¿°\n\næ ·æœ¬çš„ç‰¹å¾æ•°ç§°ä¸ºç»´æ•°ï¼ˆdimensionalityï¼‰ï¼Œå½“ç»´æ•°éå¸¸å¤§æ—¶ï¼Œä¹Ÿå°±æ˜¯ç°åœ¨æ‰€è¯´çš„â€œç»´æ•°ç¾éš¾â€ï¼Œå…·ä½“è¡¨ç°åœ¨ï¼šåœ¨é«˜ç»´æƒ…å½¢ä¸‹ï¼Œæ•°æ®æ ·æœ¬å°†å˜å¾—ååˆ†ç¨€ç–ï¼Œå› ä¸ºæ­¤æ—¶è¦æ»¡è¶³è®­ç»ƒæ ·æœ¬ä¸ºâ€œå¯†é‡‡æ ·â€çš„æ€»ä½“æ ·æœ¬æ•°ç›®æ˜¯ä¸€ä¸ªè§¦ä¸å¯åŠçš„å¤©æ–‡æ•°å­—ï¼Œè°“å¯è¿œè§‚è€Œä¸å¯äºµç©ç„‰...è®­ç»ƒæ ·æœ¬çš„ç¨€ç–ä½¿å¾—å…¶ä»£è¡¨æ€»ä½“åˆ†å¸ƒçš„èƒ½åŠ›å¤§å¤§å‡å¼±ï¼Œä»è€Œæ¶ˆå‡äº†å­¦ä¹ å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼›åŒæ—¶å½“ç»´æ•°å¾ˆé«˜æ—¶ï¼Œè®¡ç®—è·ç¦»ä¹Ÿå˜å¾—ååˆ†å¤æ‚ï¼Œç”šè‡³è¿è®¡ç®—å†…ç§¯éƒ½ä¸å†å®¹æ˜“ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ä½¿ç”¨æ ¸å‡½æ•°â€œä½ç»´è®¡ç®—ï¼Œé«˜ç»´è¡¨ç°â€çš„åŸå› ã€‚\n\nç¼“è§£ç»´æ•°ç¾éš¾çš„ä¸€ä¸ªé‡è¦é€”å¾„å°±æ˜¯é™ç»´ï¼Œå³é€šè¿‡æŸç§æ•°å­¦å˜æ¢å°†åŸå§‹é«˜ç»´ç©ºé—´è½¬å˜åˆ°ä¸€ä¸ªä½ç»´çš„å­ç©ºé—´ã€‚åœ¨è¿™ä¸ªå­ç©ºé—´ä¸­ï¼Œæ ·æœ¬çš„å¯†åº¦å°†å¤§å¹…æé«˜ï¼ŒåŒæ—¶è·ç¦»è®¡ç®—ä¹Ÿå˜å¾—å®¹æ˜“ã€‚è¿™æ—¶ä¹Ÿè®¸ä¼šæœ‰ç–‘é—®ï¼Œè¿™æ ·é™ç»´ä¹‹åä¸æ˜¯ä¼šä¸¢å¤±åŸå§‹æ•°æ®çš„ä¸€éƒ¨åˆ†ä¿¡æ¯å—ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šå®é™…çš„é—®é¢˜ä¸­ï¼Œè™½ç„¶è®­ç»ƒæ•°æ®æ˜¯é«˜ç»´çš„ï¼Œä½†æ˜¯ä¸å­¦ä¹ ä»»åŠ¡ç›¸å…³ä¹Ÿè®¸ä»…ä»…æ˜¯å…¶ä¸­çš„ä¸€ä¸ªä½ç»´å­ç©ºé—´ï¼Œä¹Ÿç§°ä¸ºä¸€ä¸ªä½ç»´åµŒå…¥ï¼Œä¾‹å¦‚ï¼šæ•°æ®å±æ€§ä¸­å­˜åœ¨å™ªå£°å±æ€§ã€ç›¸ä¼¼å±æ€§æˆ–å†—ä½™å±æ€§ç­‰ï¼Œå¯¹é«˜ç»´æ•°æ®è¿›è¡Œé™ç»´èƒ½åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¾¾åˆ°æç‚¼ä½ç»´ä¼˜è´¨å±æ€§æˆ–é™å™ªçš„æ•ˆæœã€‚\n\n#### 2.é™ç»´æ–¹æ³•åˆ†ç±»\n\n![è¿™é‡Œå†™å›¾ç‰‡æè¿°](http://img.blog.csdn.net/20150522194801297)\n\n#### 3 çº¿æ€§æ–¹æ³•\n\n##### 3.1 PCAä¸»æˆåˆ†åˆ†æ\n\nä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ç›´æ¥é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œå°†åŸå§‹ç©ºé—´ä¸­çš„æ ·æœ¬æŠ•å½±åˆ°æ–°çš„ä½ç»´ç©ºé—´ä¸­ã€‚ç®€å•æ¥ç†è§£è¿™ä¸€è¿‡ç¨‹ä¾¿æ˜¯ï¼šPCAé‡‡ç”¨ä¸€ç»„æ–°çš„åŸºæ¥è¡¨ç¤ºæ ·æœ¬ç‚¹ï¼Œå…¶ä¸­æ¯ä¸€ä¸ªåŸºå‘é‡éƒ½æ˜¯åŸæ¥åŸºå‘é‡çš„çº¿æ€§ç»„åˆï¼Œé€šè¿‡ä½¿ç”¨å°½å¯èƒ½å°‘çš„æ–°åŸºå‘é‡æ¥è¡¨å‡ºæ ·æœ¬ï¼Œä»è€Œè¾¾åˆ°é™ç»´çš„ç›®çš„ã€‚\n\nå‡è®¾ä½¿ç”¨$d^{â€™}$ä¸ªæ–°åŸºå‘é‡æ¥è¡¨ç¤ºåŸæ¥æ ·æœ¬ï¼Œå®è´¨ä¸Šæ˜¯å°†æ ·æœ¬æŠ•å½±åˆ°ä¸€ä¸ªç”±$d^{â€™}$ä¸ªåŸºå‘é‡ç¡®å®šçš„ä¸€ä¸ªè¶…å¹³é¢ä¸Šï¼ˆå³èˆå¼ƒäº†ä¸€äº›ç»´åº¦ï¼‰ï¼Œè¦ç”¨ä¸€ä¸ªè¶…å¹³é¢å¯¹ç©ºé—´ä¸­æ‰€æœ‰é«˜ç»´æ ·æœ¬è¿›è¡Œæ°å½“çš„è¡¨è¾¾ï¼Œæœ€ç†æƒ³çš„æƒ…å½¢æ˜¯ï¼šè‹¥è¿™äº›æ ·æœ¬ç‚¹éƒ½èƒ½åœ¨è¶…å¹³é¢ä¸Šè¡¨å‡ºä¸”è¿™äº›è¡¨å‡ºåœ¨è¶…å¹³é¢ä¸Šéƒ½èƒ½å¤Ÿå¾ˆå¥½åœ°åˆ†æ•£å¼€æ¥ã€‚ä½†æ˜¯ä¸€èˆ¬ä½¿ç”¨è¾ƒåŸç©ºé—´ä½ä¸€äº›ç»´åº¦çš„è¶…å¹³é¢æ¥åšåˆ°è¿™ä¸¤ç‚¹ååˆ†ä¸å®¹æ˜“ï¼Œå› æ­¤æˆ‘ä»¬é€€ä¸€æ­¥æµ·é˜”å¤©ç©ºï¼Œè¦æ±‚è¿™ä¸ªè¶…å¹³é¢åº”å…·æœ‰å¦‚ä¸‹ä¸¤ä¸ªæ€§è´¨ï¼š\n\n> **æœ€è¿‘é‡æ„æ€§**ï¼šæ ·æœ¬ç‚¹åˆ°è¶…å¹³é¢çš„è·ç¦»è¶³å¤Ÿè¿‘ï¼Œå³å°½å¯èƒ½åœ¨è¶…å¹³é¢é™„è¿‘ï¼›\n> **æœ€å¤§å¯åˆ†æ€§**ï¼šæ ·æœ¬ç‚¹åœ¨è¶…å¹³é¢ä¸Šçš„æŠ•å½±å°½å¯èƒ½åœ°åˆ†æ•£å¼€æ¥ï¼Œå³æŠ•å½±åçš„åæ ‡å…·æœ‰åŒºåˆ†æ€§ã€‚\n\nPCAçš„ç®—æ³•æè¿°å¦‚ä¸‹ï¼š\n\n![](.\\é™ç»´æ–¹æ³•-æ€»ç»“\\pca_ç®—æ³•æ­¥éª¤.png)\n\n##### 3.2 LDA çº¿æ€§åˆ¤åˆ«åˆ†æ\n\nå‚è€ƒä¹‹å‰çš„åšæ–‡\n\n#### 4 éçº¿æ€§æ–¹æ³•\n\n##### 4.1 MDS \n\nä¸ç®¡æ˜¯ä½¿ç”¨æ ¸å‡½æ•°å‡ç»´è¿˜æ˜¯å¯¹æ•°æ®é™ç»´ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›**åŸå§‹ç©ºé—´æ ·æœ¬ç‚¹ä¹‹é—´çš„è·ç¦»åœ¨æ–°ç©ºé—´ä¸­åŸºæœ¬ä¿æŒä¸å˜**ï¼Œè¿™æ ·æ‰ä¸ä¼šä½¿å¾—åŸå§‹ç©ºé—´æ ·æœ¬ä¹‹é—´çš„å…³ç³»åŠæ€»ä½“åˆ†å¸ƒå‘ç”Ÿè¾ƒå¤§çš„æ”¹å˜ã€‚**â€œå¤šç»´ç¼©æ”¾â€ï¼ˆMDSï¼‰**æ­£æ˜¯åŸºäºè¿™æ ·çš„æ€æƒ³ï¼Œ**MDSè¦æ±‚åŸå§‹ç©ºé—´æ ·æœ¬ä¹‹é—´çš„è·ç¦»åœ¨é™ç»´åçš„ä½ç»´ç©ºé—´ä¸­å¾—ä»¥ä¿æŒ**ã€‚\n\nå‡å®š$m$ä¸ªæ ·æœ¬åœ¨åŸå§‹ç©ºé—´ä¸­ä»»æ„ä¸¤ä¸¤æ ·æœ¬ä¹‹é—´çš„è·ç¦»çŸ©é˜µä¸º$D \\in R ^{m \\times m}$ï¼Œå…¶ä¸­ç¬¬$i$è¡Œ$j$åˆ—çš„å…ƒç´ $dist_{ij}$ä¸ºæ ·æœ¬$x_i$åˆ°$x_j$çš„è·ç¦»ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä¾¿æ˜¯è·å¾—æ ·æœ¬åœ¨ä½ç»´ç©ºé—´ä¸­çš„è¡¨ç¤º$Z \\in R^{d^{â€™} \\times m} $, $d'< d$ï¼Œä¸”ä»»æ„ä¸¤ä¸ªæ ·æœ¬åœ¨ä½ç»´ç©ºé—´ä¸­çš„æ¬§å¼è·ç¦»ç­‰äºåŸå§‹ç©ºé—´ä¸­çš„è·ç¦»ï¼Œå³$||zi-zj||=dist_{ij}$ã€‚å› æ­¤æ¥ä¸‹æ¥æˆ‘ä»¬è¦åšçš„å°±æ˜¯æ ¹æ®å·²æœ‰çš„è·ç¦»çŸ©é˜µ$D$æ¥æ±‚è§£å‡ºé™ç»´åçš„åæ ‡çŸ©é˜µ$Z$ã€‚\n\nä»¤é™ç»´åçš„æ ·æœ¬åæ ‡çŸ©é˜µZè¢«ä¸­å¿ƒåŒ–ï¼Œ**ä¸­å¿ƒåŒ–æ˜¯æŒ‡å°†æ¯ä¸ªæ ·æœ¬å‘é‡å‡å»æ•´ä¸ªæ ·æœ¬é›†çš„å‡å€¼å‘é‡ï¼Œæ•…æ‰€æœ‰æ ·æœ¬å‘é‡æ±‚å’Œå¾—åˆ°ä¸€ä¸ªé›¶å‘é‡**ã€‚è¿™æ ·æ˜“çŸ¥ï¼šçŸ©é˜µBçš„æ¯ä¸€åˆ—ä»¥åŠæ¯ä¸€åˆ—æ±‚å’Œå‡ä¸º0ï¼Œå› ä¸ºæå–å…¬å› å­åéƒ½æœ‰ä¸€é¡¹ä¸ºæ‰€æœ‰æ ·æœ¬å‘é‡çš„å’Œå‘é‡ã€‚\n\n![4.png](https://i.loli.net/2018/10/18/5bc851a4a4ee2.png)\n\nä»¤$B = Z^TZ \\in R^{m \\times m}$,å…¶ä¸­$B$ä¸ºé™ç»´åçš„æ ·æœ¬çš„å…§ç§¯çŸ©é˜µï¼Œ$b_{ij} = z_i^Tz_j$,æœ‰ï¼š\n$$\ndist_{ij}^{2} = ||z_i||^2 + ||z_j||^2 -2z_i^Tz_j\n= b_{ii} + b_{jj} - 2b_{ij}\n$$\nä¸ºäº†æ–¹ä¾¿è®¨è®ºï¼Œä»¤é™ç»´åçš„æ ·æœ¬$Z$è¢«ä¸­å¿ƒåŒ–ï¼Œå³$\\sum_{i=1}^{m}z_i = 0$,æ˜¾ç„¶$B$çš„è¡Œä¸åˆ—ä¹‹å’Œå‡ä¸º0ï¼Œå³$\\sum_{i=1}^{m}b_{ij} =\\sum_{j=1}^{m}b_{ij} =  0$,å®¹æ˜“çŸ¥é“ï¼š\n$$\n\\sum_{i=1}^{m}dist_{ij}^2 = tr(B) +mb_{jj}\n$$\n\n$$\n\\sum_{j=1}^{m}dist_{ij}^2 = tr(B) +mb_{ii}\n$$\n\n$$\n\\sum_{i=1}^m\\sum_{i=1}^{m}dist_{ij}^2 = 2m * tr(B)\n$$\n\nå…¶ä¸­$tr(.)$è¡¨ç¤ºçŸ©é˜µçš„è¿¹(trace),$tr(B) = \\sum_{i=1}^{m}||z_i||^2$,ä»¤\n$$\ndist_{i.}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{.j}^2=\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\n$$\ndist_{..}^2=\\frac{1}{m^2}\\sum_{i=1}^{m}\\sum_{i=1}^{m}dist_{ij}^2\n$$\n\nç”±ä¸Šé¢ä¸‰ä¸ªç­‰å¼å¯çŸ¥ï¼š\n$$\nb_{ij} = -0.5(dist_{ij}^2 - dist_{i.}^2 -dist_{.j}^2 + dist_{..}^2)\n$$\né€šè¿‡é™ç»´å‰åä¿æŒä¸å˜çš„è·ç¦»çŸ©é˜µ$D$æ±‚å–å…§ç§¯çŸ©é˜µ$B$.\n\nå¯¹$B$åšç‰¹å¾å€¼åˆ†è§£(**eigenvalue decompostion**), $B = VUV^T$,å…¶ä¸­$U=diag(\\lambda _1,\\lambda _2,...,\\lambda _d)$ä¸ºç‰¹å¾å€¼æ„æˆçš„å¯¹è§’çŸ©é˜µï¼Œ$\\lambda _1 \\geq \\lambda _2 \\geq ... \\geq \\lambda _d$,$V$ä¸ºç‰¹å¾å‘é‡çŸ©é˜µã€‚å‡å®šå…¶ä¸­æœ‰$d^*$ä¸ªéé›¶ç‰¹å¾å€¼ï¼Œä»–ä»¬æ„æˆçš„å¯¹è§’çŸ©é˜µ $U_* = diag(\\lambda _1,\\lambda _2,...,\\lambda _{d^{*}})$ ,å…¶ä¸­$V_*$è¡¨ç¤ºå¯¹åº”çš„ç‰¹å¾å‘é‡çŸ©é˜µï¼Œåˆ™$Z$å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n$$\nZ = U_*^{\\frac{1}{2}}V_*^T \\in R^{d^*\\times m}\n$$\nMDS çš„ç®—æ³•æè¿°å¦‚ä¸‹ï¼š\n\n![](.\\é™ç»´æ–¹æ³•-æ€»ç»“\\mds_ç®—æ³•æ­¥éª¤.png)\n\n##### 4.2 Isomap\n\nç­‰åº¦é‡æ˜ å°„ï¼ˆIsomap)å±äºæµè¡Œå­¦ä¹ ï¼Œæµå½¢å­¦ä¹ ï¼ˆmanifold learningï¼‰æ˜¯ä¸€ç§å€ŸåŠ©æ‹“æ‰‘æµå½¢æ¦‚å¿µçš„é™ç»´æ–¹æ³•**ï¼Œ**æµå½¢æ˜¯æŒ‡åœ¨å±€éƒ¨ä¸æ¬§å¼ç©ºé—´åŒèƒšçš„ç©ºé—´ï¼Œå³åœ¨å±€éƒ¨ä¸æ¬§å¼ç©ºé—´å…·æœ‰ç›¸åŒçš„æ€§è´¨ï¼Œèƒ½ç”¨æ¬§æ°è·ç¦»è®¡ç®—æ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚è¿™æ ·å³ä½¿é«˜ç»´ç©ºé—´çš„åˆ†å¸ƒååˆ†å¤æ‚ï¼Œä½†æ˜¯åœ¨å±€éƒ¨ä¸Šä¾ç„¶æ»¡è¶³æ¬§å¼ç©ºé—´çš„æ€§è´¨ï¼ŒåŸºäºæµå½¢å­¦ä¹ çš„é™ç»´æ­£æ˜¯è¿™ç§**â€œé‚»åŸŸä¿æŒâ€**çš„æ€æƒ³ ï¼Œç­‰åº¦é‡æ˜ å°„ï¼ˆIsomapï¼‰è¯•å›¾åœ¨é™ç»´å‰åä¿æŒé‚»åŸŸå†…æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼Œè€Œå±€éƒ¨çº¿æ€§åµŒå…¥ï¼ˆLLEï¼‰åˆ™æ˜¯ä¿æŒé‚»åŸŸå†…æ ·æœ¬ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚\n\nç­‰åº¦é‡æ˜ å°„çš„åŸºæœ¬å‡ºå‘ç‚¹æ˜¯ï¼šé«˜ç»´ç©ºé—´ä¸­çš„ç›´çº¿è·ç¦»å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› ä¸ºæœ‰æ—¶é«˜ç»´ç©ºé—´ä¸­çš„ç›´çº¿è·ç¦»åœ¨ä½ç»´ç©ºé—´ä¸­æ˜¯ä¸å¯è¾¾çš„ã€‚**å› æ­¤åˆ©ç”¨æµå½¢åœ¨å±€éƒ¨ä¸Šä¸æ¬§å¼ç©ºé—´åŒèƒšçš„æ€§è´¨ï¼Œå¯ä»¥ä½¿ç”¨è¿‘é‚»è·ç¦»æ¥é€¼è¿‘æµ‹åœ°çº¿è·ç¦»**ï¼Œå³å¯¹äºä¸€ä¸ªæ ·æœ¬ç‚¹ï¼Œå®ƒä¸è¿‘é‚»å†…çš„æ ·æœ¬ç‚¹ä¹‹é—´æ˜¯å¯è¾¾çš„ï¼Œä¸”è·ç¦»ä½¿ç”¨æ¬§å¼è·ç¦»è®¡ç®—ï¼Œè¿™æ ·æ•´ä¸ªæ ·æœ¬ç©ºé—´å°±å½¢æˆäº†ä¸€å¼ è¿‘é‚»å›¾ï¼Œé«˜ç»´ç©ºé—´ä¸­ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´çš„è·ç¦»å°±è½¬ä¸ºæœ€çŸ­è·¯å¾„é—®é¢˜ã€‚å¯é‡‡ç”¨è‘—åçš„**Dijkstraç®—æ³•**æˆ–**Floydç®—æ³•**è®¡ç®—æœ€çŸ­è·ç¦»ï¼Œå¾—åˆ°é«˜ç»´ç©ºé—´ä¸­ä»»æ„ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»åä¾¿å¯ä»¥ä½¿ç”¨MDSç®—æ³•æ¥å…¶è®¡ç®—ä½ç»´ç©ºé—´ä¸­çš„åæ ‡ã€‚\n\n![](./é™ç»´æ–¹æ³•-æ€»ç»“/æµ‹åœ°è·ç¦».png)\n\nä»**MDS**ç®—æ³•çš„æè¿°ä¸­æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼šMDSå…ˆæ±‚å‡ºäº†ä½ç»´ç©ºé—´çš„å†…ç§¯çŸ©é˜µ$B$ï¼Œæ¥ç€ä½¿ç”¨ç‰¹å¾å€¼åˆ†è§£è®¡ç®—å‡ºäº†æ ·æœ¬åœ¨ä½ç»´ç©ºé—´ä¸­çš„åæ ‡ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰ç»™å‡ºé€šç”¨çš„æŠ•å½±å‘é‡$w$ï¼Œå› æ­¤å¯¹äºéœ€è¦é™ç»´çš„æ–°æ ·æœ¬æ— ä»ä¸‹æ‰‹ï¼Œä¹¦ä¸­ç»™å‡ºçš„æƒå®œä¹‹è®¡æ˜¯åˆ©ç”¨å·²çŸ¥é«˜/ä½ç»´åæ ‡çš„æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†å­¦ä¹ å‡ºä¸€ä¸ªâ€œæŠ•å½±å™¨â€ï¼Œä¾¿å¯ä»¥ç”¨é«˜ç»´åæ ‡é¢„æµ‹å‡ºä½ç»´åæ ‡ã€‚\n\nISOMAPçš„ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š\n\n![](./é™ç»´æ–¹æ³•-æ€»ç»“/isomap_ç®—æ³•æ­¥éª¤.png)\n\nå¯¹äºè¿‘é‚»å›¾çš„æ„å»ºï¼Œå¸¸ç”¨çš„æœ‰ä¸¤ç§æ–¹æ³•ï¼š**ä¸€ç§æ˜¯æŒ‡å®šè¿‘é‚»ç‚¹ä¸ªæ•°**ï¼Œåƒ**KNN**ä¸€æ ·é€‰å–kä¸ªæœ€è¿‘çš„é‚»å±…ï¼›**å¦ä¸€ç§æ˜¯æŒ‡å®šé‚»åŸŸåŠå¾„**ï¼Œè·ç¦»å°äºè¯¥é˜ˆå€¼çš„è¢«è®¤ä¸ºæ˜¯å®ƒçš„è¿‘é‚»ç‚¹ã€‚ä½†ä¸¤ç§æ–¹æ³•å‡ä¼šå‡ºç°ä¸‹é¢çš„é—®é¢˜ï¼š\n\n> è‹¥**é‚»åŸŸèŒƒå›´æŒ‡å®šè¿‡å¤§ï¼Œåˆ™ä¼šé€ æˆâ€œçŸ­è·¯é—®é¢˜â€**ï¼Œå³æœ¬èº«è·ç¦»å¾ˆè¿œå´æˆäº†è¿‘é‚»ï¼Œå°†è·ç¦»è¿‘çš„é‚£äº›æ ·æœ¬æ‰¼æ€åœ¨æ‘‡ç¯®ã€‚\n> è‹¥**é‚»åŸŸèŒƒå›´æŒ‡å®šè¿‡å°ï¼Œåˆ™ä¼šé€ æˆâ€œæ–­è·¯é—®é¢˜â€**ï¼Œå³æœ‰äº›æ ·æœ¬ç‚¹æ— æ³•å¯è¾¾äº†ï¼Œæ•´ä¸ªä¸–ç•Œæ‘è¢«åˆ’åˆ†ä¸ºäº’ä¸å¯è¾¾çš„å°éƒ¨è½ã€‚\n\n##### 4.3 LLE \n\nä¸åŒäºIsomapç®—æ³•å»ä¿æŒé‚»åŸŸè·ç¦»ï¼ŒLLEç®—æ³•è¯•å›¾å»ä¿æŒé‚»åŸŸå†…çš„çº¿æ€§å…³ç³»ï¼Œå‡å®šæ ·æœ¬$x_i$çš„åæ ‡å¯ä»¥é€šè¿‡å®ƒçš„é‚»åŸŸæ ·æœ¬çº¿æ€§è¡¨å‡ºï¼š\n$$\nx_i = w_{ij}x_j + w_{ik}x_k + w_{il}x_l\n$$\nLLEå¸Œæœ›ä¸Šå¼çš„å…³ç³»åœ¨ä½ç»´ç©ºé—´ä¸­å¾—ä»¥ä¿æŒã€‚\n\n![](./é™ç»´æ–¹æ³•-æ€»ç»“/LLEç©ºé—´ä¿æŒ.png)\n\nLLEå…ˆä¸ºæ¯ä¸ªæ ·æœ¬$x_i$æ‰¾åˆ°å…¶è¿‘é‚»ä¸‹æ ‡é›†åˆ$Q_i$,ç„¶åè®¡ç®—åŸºäº$Q_i$ä¸­çš„æ ·æœ¬ç‚¹å¯¹$x_i$è¿›è¡Œçº¿æ€§é‡æ„çš„ç³»æ•°$w_i$.\n$$\n\\min_{w_1,w_2,...,w_m}\\sum_{i=1}^m||x_i - \\sum_{j \\in Q_i}w_{ij}{x_j}||_2^2\n$$\n\n$$\ns.t. \\sum_{j\\in Q_i}w_{ij} = 1\n$$\n\nå…¶ä¸­$x_i$å’Œ$x_j$å‡ä¸ºå·²çŸ¥ï¼Œä»¤$C_{jk} = (x_i- x_j)^T(x_i-x_j)$,$w_{ij}$æœ‰é—­å¼è§£\n$$\nw_{ij} = \\frac{\\sum_{k \\in Q_i}C_{jk}^{-1}}{\\sum_{l,s \\in Q_i}C_{ls}^{-1}}\n$$\n**LLE**åœ¨ä½ç»´ç©ºé—´ä¸­ä¿æŒ$w_i$ä¸å˜ï¼Œäºæ˜¯$x_i$å¯¹åº”çš„ä½ç»´ç©ºé—´åæ ‡$z_i$å¯é€šè¿‡ä¸‹å¼æ±‚è§£ï¼š\n$$\n\\min_{z_1,z_2,...,z_m}\\sum_{i=1}^m||z_i - \\sum_{j \\in Q_i}w_{ij}{z_j}||_2^2\n$$\nä¸Šå¼éœ€è¦ç¡®å®šçš„æ˜¯$x_i$å¯¹åº”çš„ä½ç»´ç©ºé—´åæ ‡$z_i$.\n\nä»¤$Z =(z_1,z_2,...,z_m) \\in R^{d'\\times m}$,$(W)_{ij} = w_{ij}$,\n$$\nM = (I-W)^T(I-W)\n$$\nåˆ™ä¼˜åŒ–ç›®æ ‡å¯ä»¥é‡å†™ä¸ºä¸‹å¼ï¼š\n$$\n\\begin{equation}\n\\min_z tr(ZMZ^T) \\\\\ns.t. ZZ^T = I \n\\end{equation}\n$$\nå¯ä»¥é€šè¿‡ç‰¹å¾å€¼åˆ†è§£æ±‚è§£ï¼Œ$M$æœ€å°çš„$d'$çš„ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µå³ä¸º$Z^T$\n\nLLEç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š\n\n![](./é™ç»´æ–¹æ³•-æ€»ç»“/lle_ç®—æ³•æ­¥éª¤.png)\n\n##### 4.4 æ ¸PCA kernel PCA\n\nè¯´èµ·æœºå™¨å­¦ä¹ ä½ ä¸­æœ‰æˆ‘/æˆ‘ä¸­æœ‰ä½ /æ°´ä¹³ç›¸è...åœ¨è¿™é‡Œèƒ½å¤Ÿå¾—åˆ°å¾ˆå¥½çš„ä½“ç°ã€‚æ­£å¦‚SVMåœ¨å¤„ç†éçº¿æ€§å¯åˆ†æ—¶ï¼Œé€šè¿‡å¼•å…¥æ ¸å‡½æ•°å°†æ ·æœ¬æŠ•å½±åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œæ¥ç€åœ¨é«˜ç»´ç©ºé—´å†å¯¹æ ·æœ¬ç‚¹ä½¿ç”¨è¶…å¹³é¢åˆ’åˆ†ã€‚è¿™é‡Œä¹Ÿæ˜¯ç›¸åŒçš„é—®é¢˜ï¼šè‹¥æˆ‘ä»¬çš„æ ·æœ¬æ•°æ®ç‚¹æœ¬èº«å°±ä¸æ˜¯çº¿æ€§åˆ†å¸ƒï¼Œé‚£è¿˜å¦‚ä½•ä½¿ç”¨ä¸€ä¸ªè¶…å¹³é¢å»è¿‘ä¼¼è¡¨å‡ºå‘¢ï¼Ÿå› æ­¤ä¹Ÿå°±å¼•å…¥äº†æ ¸å‡½æ•°ï¼Œ**å³å…ˆå°†æ ·æœ¬æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œå†åœ¨é«˜ç»´ç©ºé—´ä¸­ä½¿ç”¨çº¿æ€§é™ç»´çš„æ–¹æ³•**ã€‚ä¸‹é¢ä¸»è¦ä»‹ç»**æ ¸åŒ–ä¸»æˆåˆ†åˆ†æï¼ˆKPCAï¼‰**çš„æ€æƒ³ã€‚\n\nè‹¥æ ¸å‡½æ•°çš„å½¢å¼å·²çŸ¥ï¼Œå³æˆ‘ä»¬çŸ¥é“å¦‚ä½•å°†ä½ç»´çš„åæ ‡å˜æ¢ä¸ºé«˜ç»´åæ ‡ï¼Œè¿™æ—¶æˆ‘ä»¬åªéœ€å…ˆå°†æ•°æ®æ˜ å°„åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œå†åœ¨é«˜ç»´ç©ºé—´ä¸­è¿ç”¨PCAå³å¯ã€‚ä½†æ˜¯ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“æ ¸å‡½æ•°å…·ä½“çš„æ˜ å°„è§„åˆ™ï¼Œä¾‹å¦‚ï¼šSigmoidã€é«˜æ–¯æ ¸ç­‰ï¼Œæˆ‘ä»¬åªçŸ¥é“å¦‚ä½•è®¡ç®—é«˜ç»´ç©ºé—´ä¸­çš„æ ·æœ¬å†…ç§¯ï¼Œè¿™æ—¶å°±å¼•å‡ºäº†KPCAçš„ä¸€ä¸ªé‡è¦åˆ›æ–°ä¹‹å¤„ï¼š**å³ç©ºé—´ä¸­çš„ä»»ä¸€å‘é‡ï¼Œéƒ½å¯ä»¥ç”±è¯¥ç©ºé—´ä¸­çš„æ‰€æœ‰æ ·æœ¬çº¿æ€§è¡¨ç¤º**ã€‚\n\nå‡å®šæˆ‘ä»¬å°†åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­çš„æ•°æ®æŠ•å½±åˆ°ç”±$W$ç¡®å®šçš„è¶…å¹³é¢ä¸Šï¼Œå³PCAæ¬²æ±‚è§£\n$$\n(\\sum_{i=1}^mz_iz_i^T)W = \\lambda W\n$$\nå…¶ä¸­$z_i$æ˜¯æ ·æœ¬ç‚¹$x_i$åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´çš„åƒï¼Œå¯çŸ¥ï¼š\n$$\nW = \\frac{1}{\\lambda}(\\sum_{i=1}^mz_iz^T_i)W = \\sum_{i=1}^m z_i\\frac{z_i^TW}{\\lambda} \\\\\n=\\sum_{i=1}^m z_i\\alpha _i\n$$\nå…¶ä¸­$\\alpha _i = \\frac{1}{\\lambda}z_i^TW$.å‡å®š$z_i$æ˜¯ç”±åŸå§‹å±æ€§ç©ºé—´ä¸­çš„æ ·æœ¬ç‚¹$x_i$é€šè¿‡æ˜ å°„$\\phi$äº§ç”Ÿï¼Œå³$z_i = \\phi (x_i),i= 1,2,...,m$,è‹¥$\\phi$èƒ½è¢«æ˜¾å¼çš„è¡¨è¾¾å‡ºæ¥ï¼Œåˆ™é€šè¿‡å®ƒå°†æ ·æœ¬æ˜ å°„è‡³é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œå†åœ¨ç‰¹å¾ç©ºé—´ä¸­å®æ–½PCAå³å¯ã€‚\n\nå³ï¼ˆ17ï¼‰å¯å˜ä¸º\n$$\n(\\sum_{i=1}^m \\phi (x_i) \\phi (x_i)^T)W = \\lambda W\n$$\nï¼ˆ18ï¼‰å¼å¯å˜ä¸ºï¼š\n$$\nW= \\sum_{i=1}^m \\phi(x_i)\\alpha_i\n$$\nä¸€èˆ¬æƒ…å½¢ä¸‹ï¼Œæˆ‘ä»¬ä¸æ¸…æ¥š$\\phi$çš„å…·ä½“å½¢å¼ï¼Œäºæ˜¯å¼•å…¥æ ¸å‡½æ•°:\n$$\n\\kappa(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)\n$$\nå°†ï¼ˆ21)(20)å¸¦å…¥åˆ°(19)å¼ä¸­å¯å¾—ï¼š\n$$\nKA= \\lambda A\n$$\nå…¶ä¸­$K$ä¸º$\\kappa$ å¯¹åº”çš„æ ¸çŸ©é˜µï¼Œ$(K)_{ij} = \\kappa(x_i,x_j),A = (\\alpha_1;\\alpha_2;...;\\alpha_m)$,æ˜¾ç„¶ä¸Šå¼æ˜¯ä¸ªç‰¹å¾å€¼åˆ†è§£é—®é¢˜ï¼Œå–$K$æœ€å¤§çš„$d'$ä¸ªç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å³å¯ã€‚\n\n##### 4.5 DiffusionMap\n\næ‰©æ•£æ˜ å°„æ˜¯ä¸€ç§é™ç»´æ–¹æ³•\n\n1. å…¶é€šè¿‡ æ•´åˆæ•°æ®çš„å±€éƒ¨å‡ ä½•å…³ç³» æ­ç¤º æ•°æ®é›†åœ¨ä¸åŒå°ºåº¦çš„å‡ ä½•ç»“æ„ã€‚\n2. ä¸PCA (principal component analysis)ã€MDS (Multidimensional Scaling) è¿™äº›é™ç»´æ–¹æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ˜ å°„ éçº¿æ€§ï¼Œèšç„¦äºå‘ç°æ•°æ®é›†æ½œåœ¨çš„æµå½¢ç»“æ„ã€‚\n3. ä¼˜ç‚¹ï¼šå¯¹å™ªå£°é²æ£’ï¼Œè®¡ç®—ä»£ä»·è¾ƒä½\n\nç®—æ³•æ­¥éª¤å¦‚ä¸‹:\n\n![](./é™ç»´æ–¹æ³•-æ€»ç»“/diffmap_ç®—æ³•æ­¥éª¤.png)"},{"title":"xgboost","url":"/2020/03/18/xgboost/","content":"#### 1. XGBoost ç®€ä»‹\n\n$XGBoost$çš„å…¨ç§°æ˜¯$e\\textcolor{red}Xtreme \\textcolor{red}Gradient \\textcolor{red}{Boosting}$ï¼Œå®ƒæ˜¯ç»è¿‡ä¼˜åŒ–çš„åˆ†å¸ƒå¼æ¢¯åº¦æå‡åº“ï¼Œæ—¨åœ¨é«˜æ•ˆã€çµæ´»ä¸”å¯ç§»æ¤ã€‚$XGBoost$æ˜¯å¤§è§„æ¨¡å¹¶è¡Œboosting treeçš„å·¥å…·ï¼Œå®ƒæ˜¯ç›®å‰æœ€å¿«æœ€å¥½çš„å¼€æº boosting treeå·¥å…·åŒ…ï¼Œæ¯”å¸¸è§çš„å·¥å…·åŒ…å¿«10å€ä»¥ä¸Šã€‚åœ¨æ•°æ®ç§‘å­¦æ–¹é¢ï¼Œæœ‰å¤§é‡çš„Kaggleé€‰æ‰‹é€‰ç”¨$XGBoost$è¿›è¡Œæ•°æ®æŒ–æ˜æ¯”èµ›ï¼Œæ˜¯å„å¤§æ•°æ®ç§‘å­¦æ¯”èµ›çš„å¿…æ€æ­¦å™¨ï¼›åœ¨å·¥ä¸šç•Œå¤§è§„æ¨¡æ•°æ®æ–¹é¢ï¼Œ$XGBoost$çš„åˆ†å¸ƒå¼ç‰ˆæœ¬æœ‰å¹¿æ³›çš„å¯ç§»æ¤æ€§ï¼Œæ”¯æŒåœ¨Kubernetesã€Hadoopã€SGEã€MPIã€ Daskç­‰å„ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒä¸Šè¿è¡Œï¼Œä½¿å¾—å®ƒå¯ä»¥å¾ˆå¥½åœ°è§£å†³å·¥ä¸šç•Œå¤§è§„æ¨¡æ•°æ®çš„é—®é¢˜ã€‚æœ¬æ–‡å°†ä»$XGBoost$çš„æ•°å­¦åŸç†å’Œå·¥ç¨‹å®ç°ä¸Šè¿›è¡Œä»‹ç»ï¼Œç„¶åä»‹ç»$XGBoost$çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶åœ¨æœ€åç»™å‡ºé¢è¯•ä¸­ç»å¸¸é‡åˆ°çš„å…³äº$XGBoost$çš„é—®é¢˜ã€‚\n\n#### 2.  $XGBoost \\ $ç®—æ³•\n\nâ€‹    $XGBoost$æ˜¯ç”± $k$ä¸ªåŸºæ¨¡å‹ç»„æˆçš„ä¸€ä¸ªåŠ æ³•æ¨¡å‹ï¼Œå‡è®¾æˆ‘ä»¬ç¬¬ $t$ æ¬¡è¿­ä»£è¦è®­ç»ƒçš„æ ‘æ¨¡å‹æ˜¯ $f_t(x)$ ï¼Œåˆ™æœ‰ï¼š\n\nâ€‹\t\n$$\n\\hat{y_i}^{(t)} = \\sum_{k=1}^{t}f_k(x_i) = \\hat{y_i}^{(t-1)} + f_t(x_i)\n$$\nå…¶ä¸­ï¼Œ$\\hat{y_i}^{(t)}$æ˜¯ç¬¬$t$æ¬¡è¿­ä»£åæ ·æœ¬çš„$i$çš„é¢„æµ‹ç»“æœï¼Œ$\\hat{y_i}^{(t-1)}$æ˜¯å‰$t-1$æ£µæ ‘çš„é¢„æµ‹ç»“æœï¼Œ$f_t(x_i)$æ˜¯ç¬¬$t$æ£µæ ‘çš„æ¨¡å‹ã€‚\n\nâ€‹\t$XGBoost  \\ $ç®—æ³•æ˜¯$\\ GBDT\\ $ç®—æ³•çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå…¶ç›®æ ‡å‡½æ•°ä¸ºï¼š\n$$\n\\begin{aligned}\nObj^{(k)}&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_k)+\\sum\\limits_{i=1}^T\\Omega(f_i)\\\\\n&=\\sum\\limits_{i=1}^ml(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}))+\\Omega(f_k)+C\n\\end{aligned}\n$$\nåŒç†ä¸ºäº†æ±‚æŸå¤±å‡½æ•°$\\ l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\ $åœ¨$\\ \\hat{y}^{(i)}_{k-1} \\ $å¤„çš„äºŒé˜¶å±•å¼€ï¼Œä¸å¦¨å…ˆå¯¹$\\ l(y^{(i)},x)\\ $åœ¨$\\ \\hat{y}^{(i)}_{k-1} \\ $å¤„è¿›è¡ŒäºŒé˜¶å±•å¼€å¯å¾—ï¼š\n$$\nl(y^{(i)},x)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+\\nabla_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})+\\dfrac{1}{2}\\nabla^2_{\\hat{y}^{(i)}_{k-1}}l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\cdot(x-\\hat{y}^{(i)}_{k-1})^2\n$$\nä»¤$\\ x=\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)}) \\ $ï¼Œä¸”è®°$\\ \\nabla_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $ä¸º$\\ g_i\\ $ã€$\\ \\nabla^2_{\\hat{y}^{(i)}_{k-1}}l\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}\\right) \\ $ä¸º$\\ h_i\\ $åˆ™æœ‰ï¼š\n$$\nl\\left(y^{(i)},\\hat{y}^{(i)}_{k-1}+f_k(x^{(i)})\\right)\\simeq l(y^{(i)},\\hat{y}^{(i)}_{k-1})+g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\n$$\nå…¶ä¸­ï¼Œ$g_i$ä¸ºæŸå¤±å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ï¼Œ$h_i$ä¸ºæŸå¤±å‡½æ•°çš„äºŒé˜¶å¯¼æ•°ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å¯¹$\\hat{y}^{(t-1)}_{i}$æ±‚å¯¼ã€‚\n\n**ä»¥å¹³æ–¹æŸå¤±ä¸ºä¾‹ï¼š**\n$$\nl(y^{(i)},\\hat{y}^{(t-1)}_i) = (y^{(i)},\\hat{y}^{(t-1)}_i)^2\n$$\nåˆ™ï¼š\n$$\n\\begin{aligned}\ng_i &= \\frac{\\partial{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{\\hat{y}^{(t-1)}_i}} = -2(y_i - \\hat{y}_i^{(t-1)}) \\\\\nh_i &= \\frac{\\partial ^2{l(y^{(i)},\\hat{y}^{(t-1)}_i)}}{\\partial{(\\hat{y}^{(t-1)}_i})^2} = 2\n\\end{aligned}\n$$\nåˆå› ä¸ºåœ¨ç¬¬$\\ k\\ $æ­¥$\\ \\hat{y}^{(i)}_{k-1} \\ $å…¶å®æ˜¯å·²çŸ¥çš„ï¼Œæ‰€ä»¥$\\ l(y^{(i)},\\hat{y}^{(i)}_{k-1})\\ $æ˜¯ä¸€ä¸ªå¸¸æ•°å‡½æ•°ï¼Œæ•…å¯¹ä¼˜åŒ–ç›®æ ‡å‡½æ•°ä¸ä¼šäº§ç”Ÿå½±å“ï¼Œå°†ä¸Šè¿°ç»“è®ºå¸¦å…¥ç›®æ ‡å‡½æ•°$\\ Obj^{(k)}\\ $å¯å¾—ï¼š\n$$\nObj^{(k)}\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\n$$\n\n##### 2.1  ä¼˜åŒ–ç›®æ ‡å‡½æ•°\n\nâ€‹\tä»¥$\\ XGBoost\\ $ç®—æ³•çš„ç›®æ ‡å‡½æ•°ä¸ºä¾‹ï¼Œå¯¹äºä»»æ„å†³ç­–æ ‘$\\ f_k \\ $ï¼Œ**å‡è®¾å…¶å¶å­ç»“ç‚¹ä¸ªæ•°$\\ T\\ $ï¼Œè¯¥å†³ç­–æ ‘æ˜¯ç”±æ‰€æœ‰ç»“ç‚¹å¯¹åº”çš„å€¼ç»„æˆçš„å‘é‡$\\ w\\in\\mathbb{R}^T\\ $ï¼Œä»¥åŠèƒ½å¤ŸæŠŠç‰¹å¾å‘é‡æ˜ å°„åˆ°å¶å­ç»“ç‚¹çš„å‡½æ•°$\\ q(*):\\mathbb{R}^d\\rightarrow \\{1,2,\\cdots,T \\} \\ $æ„é€ è€Œæˆçš„ï¼Œä¸”æ¯ä¸ªæ ·æœ¬æ•°æ®éƒ½å­˜åœ¨å”¯ä¸€çš„å¶å­ç»“ç‚¹ä¸Šã€‚å› æ­¤å†³ç­–æ ‘$\\ f_k\\ $å¯ä»¥å®šä¹‰ä¸º$\\ f_k(x)=w_{q(x)} \\ $ã€‚**å†³ç­–æ ‘çš„å¤æ‚åº¦å¯ä»¥ç”±æ­£åˆ™é¡¹$\\ \\Omega(f_k)=\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2 \\ $æ¥å®šä¹‰ï¼Œè¯¥æ­£åˆ™é¡¹è¡¨æ˜å†³ç­–æ ‘æ¨¡å‹çš„å¤æ‚åº¦å¯ä»¥ç”±å¶å­ç»“ç‚¹çš„æ•°é‡å’Œå¶å­ç»“ç‚¹å¯¹åº”å€¼å‘é‡$\\ w \\ $çš„$\\ L2\\ $èŒƒæ•°å†³å®šã€‚å®šä¹‰é›†åˆ$\\ I_j=\\{i|q(x^{(i)})=j \\}\\ $ä¸ºåˆ’åˆ†åˆ°å¶å­ç»“ç‚¹$\\ j \\ $çš„æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„é›†åˆï¼Œå³ä¹‹å‰è®­ç»ƒæ ·æœ¬çš„é›†åˆï¼Œç°åœ¨éƒ½æ”¹å†™æˆå¶å­ç»“ç‚¹çš„é›†åˆï¼Œå› æ­¤$\\ XGBoost\\ $ç®—æ³•çš„ç›®æ ‡å‡½æ•°å¯ä»¥æ”¹å†™ä¸ºï¼š\n$$\n\\begin{aligned}\nObj^{(k)}&\\simeq\\sum\\limits_{i=1}^m\\bigg[ g_if_k(x^{(i)})+\\dfrac{1}{2}h_if^2_k(x^{(i)})\\bigg]+\\Omega(f_k)\\\\\n&=\\sum\\limits_{i=1}^m\\bigg[g_iw_{q(x^{(i)})}+\\dfrac{1}{2}h_jw^2_{q(x^{(i)})} \\bigg]+\\gamma T+\\dfrac{1}{2}\\lambda\\sum\\limits_{j=1}^Tw_j^2\\\\\n&=\\sum\\limits_{j=1}^T\\bigg[(\\sum\\limits_{i\\in I_j}g_i)w_j+\\dfrac{1}{2}(\\sum\\limits_{i\\in I_j}h_i+\\lambda)w_j^2 \\bigg]+\\gamma T\n\\end{aligned}\n$$\nä»¤$\\ G_j=\\sum\\limits_{i\\in I_j}g_i ,\\ H_j=\\sum\\limits_{i\\in I_j}h_i \\ $åˆ™æœ‰ï¼š\n$$\nObj^{(k)}\\simeq\\sum\\limits_{j=1}^T\\bigg[G_jw_j+\\dfrac{1}{2}(H_j+\\lambda)w_j^2 \\bigg]\n$$\nåˆ†æå¯çŸ¥å½“æ›´æ–°åˆ°ç¬¬$\\ k\\ $æ­¥æ—¶ï¼Œæ­¤æ—¶**å†³ç­–æ ‘ç»“æ„å›ºå®šçš„æƒ…å†µä¸‹**ï¼Œæ¯ä¸ªå¶å­ç»“ç‚¹æœ‰å“ªäº›æ ·æœ¬æ˜¯å·²çŸ¥çš„ï¼Œé‚£ä¹ˆ$\\ q(*)\\ $å’Œ$\\ I_j\\ $ä¹Ÿæ˜¯å·²çŸ¥çš„ï¼›åˆå› ä¸º$\\ g_i\\ $å’Œ$\\ h_i\\ $æ˜¯ç¬¬$\\ k-1\\ $æ­¥çš„å¯¼æ•°ï¼Œé‚£ä¹ˆä¹Ÿæ˜¯å·²çŸ¥çš„ï¼Œå› æ­¤$\\ G_j\\ $å’Œ$\\ H_j\\ $éƒ½æ˜¯å·²çŸ¥çš„ã€‚ä»¤ç›®æ ‡å‡½æ•°$\\ Obj^{(k)}\\ $çš„ä¸€é˜¶å¯¼æ•°ä¸º$\\ 0\\ $ï¼Œå³å¯æ±‚å¾—å¶å­ç»“ç‚¹$\\ j\\ $å¯¹åº”çš„å€¼ä¸ºï¼š\n$$\nw^*_j=-\\dfrac{G_j}{H_j+\\lambda}\n$$\nå› æ­¤é’ˆå¯¹äºç»“æ„å›ºå®šçš„å†³ç­–æ ‘ï¼Œæœ€ä¼˜çš„ç›®æ ‡å‡½æ•°$\\ Obj\\ $ä¸ºï¼š\n$$\nObj=-\\dfrac{1}{2}\\sum\\limits_{j=1}^T\\dfrac{G_j^2}{H_j+\\lambda}+\\gamma T\n$$\nä¸Šé¢çš„æ¨å¯¼æ˜¯å»ºç«‹åœ¨å†³ç­–æ ‘ç»“æ„å›ºå®šçš„æƒ…å†µä¸‹ï¼Œç„¶è€Œå†³ç­–æ ‘ç»“æ„æ•°é‡æ˜¯æ— ç©·çš„ï¼Œæ‰€ä»¥å®é™…ä¸Šå¹¶ä¸èƒ½ç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„å†³ç­–æ ‘ç»“æ„ï¼Œä»€ä¹ˆæ ·çš„å†³ç­–æ ‘ç»“æ„æ˜¯æœ€ä¼˜çš„å‘¢ï¼Ÿé€šå¸¸ä½¿ç”¨è´ªå¿ƒç­–ç•¥æ¥ç”Ÿæˆå†³ç­–æ ‘çš„æ¯ä¸ªç»“ç‚¹ï¼Œ$\\ XGBoost \\ $ç®—æ³•çš„åœ¨å†³ç­–æ ‘çš„ç”Ÿæˆé˜¶æ®µå°±å¯¹è¿‡æ‹Ÿåˆçš„é—®é¢˜è¿›è¡Œäº†å¤„ç†ï¼Œå› æ­¤æ— éœ€ç‹¬ç«‹çš„å‰ªæé˜¶æ®µï¼Œå…·ä½“æ­¥éª¤å¯ä»¥å½’çº³ä¸ºï¼š\n\n1. ä»æ·±åº¦ä¸º$\\ 0\\ $çš„æ ‘å¼€å§‹å¯¹æ¯ä¸ªå¶å­ç»“ç‚¹ç©·ä¸¾æ‰€æœ‰çš„å¯ç”¨ç‰¹å¾ï¼›\n2. é’ˆå¯¹æ¯ä¸€ä¸ªç‰¹å¾ï¼ŒæŠŠå±äºè¯¥ç»“ç‚¹çš„è®­ç»ƒæ ·æœ¬çš„è¯¥ç‰¹å¾å‡åºæ’åˆ—ï¼Œé€šè¿‡çº¿æ€§æ‰«æçš„æ–¹å¼æ¥å†³å®šè¯¥ç‰¹å¾çš„**æœ€ä½³åˆ†è£‚ç‚¹**ï¼Œå¹¶é‡‡ç”¨æœ€ä½³åˆ†è£‚ç‚¹æ—¶çš„**æ”¶ç›Š**ï¼›\n3. é€‰æ‹©æ”¶ç›Šæœ€å¤§çš„ç‰¹å¾ä½œä¸ºåˆ†è£‚ç‰¹å¾ï¼Œç”¨è¯¥ç‰¹å¾çš„æœ€ä½³åˆ†è£‚ç‚¹ä½œä¸ºåˆ†è£‚ä½ç½®ï¼ŒæŠŠè¯¥ç»“ç‚¹ç”Ÿæˆå‡ºå·¦å³ä¸¤ä¸ªæ–°çš„å¶å­ç»“ç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªæ–°ç»“ç‚¹å…³è”æ–°çš„æ ·æœ¬é›†ï¼›\n4. é€€å›åˆ°ç¬¬ä¸€æ­¥ï¼Œç»§ç»­é€’å½’æ“ä½œç›´åˆ°æ»¡è¶³ç‰¹å®šæ¡ä»¶ã€‚\n\nå› ä¸ºå¯¹æŸä¸ªç»“ç‚¹é‡‡å–çš„æ˜¯äºŒåˆ†ç­–ç•¥ï¼Œåˆ†åˆ«å¯¹åº”å·¦å­ç»“ç‚¹å’Œå³å­ç»“ç‚¹ï¼Œé™¤äº†å½“å‰å¾…å¤„ç†çš„ç»“ç‚¹ï¼Œå…¶ä»–ç»“ç‚¹å¯¹åº”çš„$\\ Obj \\ $å€¼éƒ½ä¸å˜ï¼Œæ‰€ä»¥å¯¹äºæ”¶ç›Šçš„è®¡ç®—åªéœ€è¦è€ƒè™‘å½“å‰ç»“ç‚¹çš„$\\ Obj \\ $å€¼å³å¯ï¼Œåˆ†è£‚å‰é’ˆå¯¹è¯¥ç»“ç‚¹çš„æœ€ä¼˜ç›®æ ‡å‡½æ•°ä¸ºï¼š\n$$\nObj^{(before)}=-\\dfrac{1}{2}\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda}+\\gamma\n$$\nåˆ†è£‚åçš„æœ€ä¼˜ç›®æ ‡å‡½æ•°ä¸ºï¼š\n$$\nObj^{(later)}=-\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda} \\bigg]+2\\gamma\n$$\né‚£ä¹ˆå¯¹äºè¯¥ç›®æ ‡å‡½æ•°æ¥è¯´ï¼Œåˆ†è£‚åçš„æ”¶ç›Šä¸ºï¼š\n$$\n\\begin{aligned}\nGain&=Obj^{(before)}-Obj^{(later)}\\\\\n&=\\dfrac{1}{2}\\bigg[\\dfrac{G_L^2}{H_L+\\lambda}+\\dfrac{G_R^2}{H_R+\\lambda}-\\dfrac{(G_L+G_R)^2}{(H_L+H_R)+\\lambda} \\bigg]-\\gamma\n\\end{aligned}\n$$\næ•…å¯ä»¥ç”¨ä¸Šè¿°å…¬å¼æ¥å†³å®šæœ€æœ‰åˆ†è£‚ç‰¹å¾å’Œæœ€ä¼˜ç‰¹å¾åˆ†è£‚ç‚¹ã€‚\n\n##### 2.2  æ€»ç»“\n\nâ€‹\t$XGBoost  \\ $ç®—æ³•çš„è¿‡ç¨‹å¯ä»¥å½’çº³ä¸ºï¼š\n\n1. å‰å‘åˆ†å¸ƒç®—æ³•çš„æ¯ä¸€æ­¥éƒ½ç”Ÿæˆä¸€æ£µå†³ç­–æ ‘ï¼›\n2. æ‹Ÿåˆè¯¥å†³ç­–æ ‘ä¹‹å‰ï¼Œå…ˆè®¡ç®—æŸå¤±å‡½æ•°åœ¨æ¯ä¸ªæ ·æœ¬æ•°æ®ä¸Šçš„ä¸€é˜¶å¯¼$\\ g_i \\ $å’ŒäºŒé˜¶å¯¼$\\ h_i \\ $ï¼›\n3. é€šè¿‡è´ªå¿ƒç­–ç•¥ç”Ÿæˆä¸€æ£µå†³ç­–æ ‘ï¼Œè®¡ç®—æ¯ä¸ªå¶å­ç»“ç‚¹çš„$\\ G_j\\ $å’Œ$\\ H_j\\ $å¹¶è®¡ç®—é¢„æµ‹å€¼$\\ w\\ $ï¼›\n4. æŠŠæ–°ç”Ÿæˆçš„å†³ç­–æ ‘$\\ f_k(x)\\ $åŠ å…¥$\\ \\hat{y}^{(i)}_k=\\hat{y}^{(i)}_{k-1}+\\epsilon f_k(x^{(i)}) \\ $ï¼Œå…¶ä¸­$\\ \\epsilon\\ $æ˜¯å­¦ä¹ ç‡ä¸»è¦æ§åˆ¶æ¨¡å‹çš„è¿‡æ‹Ÿåˆã€‚\n\n#### 3 $XGBoost\\ $çš„ä¼˜ç¼ºç‚¹\n\nâ€‹\tç›¸æ¯”äºæ™®é€šçš„$\\ GBDT \\ $ç®—æ³•$\\ XGBoost\\ $ç®—æ³•çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºï¼š\n\n- ä¸ä»…æ”¯æŒå†³ç­–æ ‘ä½œä¸ºåŸºåˆ†ç±»å™¨ï¼Œè¿˜æ”¯æŒå…¶å®ƒçº¿æ€§åˆ†ç±»å™¨ï¼›\n- ä½¿ç”¨äº†æŸå¤±å‡½æ•°çš„äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå› æ­¤ä¸æŸå¤±å‡½æ•°æ›´æ¥è¿‘ï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼›\n-  åœ¨ç›®æ ‡å‡½æ•°ä¸­åŠ å…¥äº†æ­£åˆ™é¡¹ï¼Œç”¨äºæ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚æ­£åˆ™é¡¹é‡ŒåŒ…å«äº†æ ‘çš„å¶å­èŠ‚ç‚¹ä¸ªæ•°ã€å¶å­èŠ‚ç‚¹æƒé‡çš„  èŒƒå¼ã€‚æ­£åˆ™é¡¹é™ä½äº†æ¨¡å‹çš„æ–¹å·®ï¼Œä½¿å­¦ä¹ å‡ºæ¥çš„æ¨¡å‹æ›´åŠ ç®€å•ï¼Œæœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè¿™ä¹Ÿæ˜¯$XGBoost$ä¼˜äºä¼ ç»ŸGBDTçš„ä¸€ä¸ªç‰¹æ€§ã€‚ï¼›\n- $Shrinkage\\ $ä¹Ÿå°±æ˜¯ä¹‹å‰è¯´çš„$\\ \\epsilon\\ $ï¼Œä¸»è¦ç”¨äºå‰Šå¼±æ¯æ£µå†³ç­–æ ‘çš„å½±å“ï¼Œè®©åé¢æœ‰æ›´å¤§çš„å­¦ä¹ ç©ºé—´ï¼Œå®é™…åº”ç”¨ä¸­ä¸€èˆ¬æŠŠ$\\ \\epsilon\\ $è®¾ç½®çš„å°ç‚¹ï¼Œè¿­ä»£æ¬¡æ•°è®¾ç½®çš„å¤§ç‚¹ï¼›\n- åˆ—æŠ½æ ·ï¼Œ$\\ XGBoost\\ $ä»éšæœºæ£®æ—ç®—æ³•ä¸­å€Ÿé‰´è€Œæ¥ï¼Œæ”¯æŒåˆ—æŠ½æ ·å¯ä»¥é™ä½è¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”å‡å°‘è®¡ç®—ï¼›\n- æ”¯æŒå¯¹ç¼ºå¤±å€¼çš„å¤„ç†ï¼Œå¯¹äºç‰¹å¾å€¼ç¼ºå¤±çš„æ ·æœ¬ï¼Œ$\\ XGBoost\\ $å¯ä»¥å­¦ä¹ è¿™äº›ç¼ºå¤±å€¼çš„åˆ†è£‚æ–¹å‘ï¼›\n- æ”¯æŒå¹¶è¡Œï¼Œboostingä¸æ˜¯ä¸€ç§ä¸²è¡Œçš„ç»“æ„å—?æ€ä¹ˆå¹¶è¡Œçš„ï¼Ÿæ³¨æ„$XGBoost$çš„å¹¶è¡Œä¸æ˜¯treeç²’åº¦çš„å¹¶è¡Œï¼Œ$XGBoost$ä¹Ÿæ˜¯ä¸€æ¬¡è¿­ä»£å®Œæ‰èƒ½è¿›è¡Œä¸‹ä¸€æ¬¡è¿­ä»£çš„ï¼ˆç¬¬$t$æ¬¡è¿­ä»£çš„ä»£ä»·å‡½æ•°é‡ŒåŒ…å«äº†å‰é¢$t-1$æ¬¡è¿­ä»£çš„é¢„æµ‹å€¼ï¼‰ã€‚$XGBoost$çš„å¹¶è¡Œæ˜¯åœ¨ç‰¹å¾ç²’åº¦ä¸Šçš„ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œå†³ç­–æ ‘çš„å­¦ä¹ æœ€è€—æ—¶çš„ä¸€ä¸ªæ­¥éª¤å°±æ˜¯å¯¹ç‰¹å¾çš„å€¼è¿›è¡Œæ’åºï¼ˆå› ä¸ºè¦ç¡®å®šæœ€ä½³åˆ†å‰²ç‚¹ï¼‰ï¼Œ$XGBoost$åœ¨è®­ç»ƒä¹‹å‰ï¼Œé¢„å…ˆå¯¹æ•°æ®è¿›è¡Œäº†æ’åºï¼Œç„¶åä¿å­˜ä¸ºblockç»“æ„ï¼Œåé¢çš„è¿­ä»£ä¸­é‡å¤åœ°ä½¿ç”¨è¿™ä¸ªç»“æ„ï¼Œå¤§å¤§å‡å°è®¡ç®—é‡ã€‚è¿™ä¸ªblockç»“æ„ä¹Ÿä½¿å¾—å¹¶è¡Œæˆä¸ºäº†å¯èƒ½ï¼Œåœ¨è¿›è¡ŒèŠ‚ç‚¹çš„åˆ†è£‚æ—¶ï¼Œéœ€è¦è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¢ç›Šï¼Œæœ€ç»ˆé€‰å¢ç›Šæœ€å¤§çš„é‚£ä¸ªç‰¹å¾å»åšåˆ†è£‚ï¼Œé‚£ä¹ˆå„ä¸ªç‰¹å¾çš„å¢ç›Šè®¡ç®—å°±å¯ä»¥å¼€å¤šçº¿ç¨‹è¿›è¡Œã€‚ï¼›\n- è¿‘ä¼¼ç®—æ³•ï¼Œå†³ç­–æ ‘ç»“ç‚¹åœ¨åˆ†è£‚æ—¶éœ€è¦ç©·ä¸¾æ¯ä¸ªå¯èƒ½çš„åˆ†è£‚ç‚¹ï¼Œå½“æ•°æ®æ²¡æ³•å…¨éƒ¨åŠ è½½åˆ°å†…å­˜ä¸­æ—¶ï¼Œè¿™ç§æ–¹æ³•ä¼šæ¯”è¾ƒæ…¢ï¼Œ$\\ XGBoost\\ $æå‡ºäº†ä¸€ç§è¿‘ä¼¼çš„æ–¹æ³•å»é«˜æ•ˆçš„ç”Ÿæˆå€™é€‰åˆ†å‰²ç‚¹ã€‚\n\nç¼ºç‚¹\n\n- è™½ç„¶åˆ©ç”¨é¢„æ’åºå’Œè¿‘ä¼¼ç®—æ³•å¯ä»¥é™ä½å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹çš„è®¡ç®—é‡ï¼Œä½†åœ¨èŠ‚ç‚¹åˆ†è£‚è¿‡ç¨‹ä¸­ä»éœ€è¦éå†æ•°æ®é›†ï¼›\n- é¢„æ’åºè¿‡ç¨‹çš„ç©ºé—´å¤æ‚åº¦è¿‡é«˜ï¼Œä¸ä»…éœ€è¦å­˜å‚¨ç‰¹å¾å€¼ï¼Œè¿˜éœ€è¦å­˜å‚¨ç‰¹å¾å¯¹åº”æ ·æœ¬çš„æ¢¯åº¦ç»Ÿè®¡å€¼çš„ç´¢å¼•ï¼Œç›¸å½“äºæ¶ˆè€—äº†ä¸¤å€çš„å†…å­˜ã€‚\n\n#### 4. å…³äºXGBosstçš„è‹¥å¹²é—®é¢˜\n\n##### 4.1 XGBoostä¸GBDTçš„è”ç³»å’ŒåŒºåˆ«æœ‰å“ªäº›ï¼Ÿ\n\n1. GBDTæ˜¯æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒXGBoostæ˜¯è¯¥ç®—æ³•çš„å·¥ç¨‹å®ç°ã€‚\n2. **æ­£åˆ™é¡¹ï¼š** åœ¨ä½¿ç”¨CARTä½œä¸ºåŸºåˆ†ç±»å™¨æ—¶ï¼ŒXGBoostæ˜¾å¼åœ°åŠ å…¥äº†æ­£åˆ™é¡¹æ¥æ§åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæœ‰åˆ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚\n3. **å¯¼æ•°ä¿¡æ¯ï¼š** GBDTåœ¨æ¨¡å‹è®­ç»ƒæ—¶åªä½¿ç”¨äº†ä»£ä»·å‡½æ•°çš„ä¸€é˜¶å¯¼æ•°ä¿¡æ¯ï¼ŒXGBoostå¯¹ä»£ä»·å‡½æ•°è¿›è¡ŒäºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥åŒæ—¶ä½¿ç”¨ä¸€é˜¶å’ŒäºŒé˜¶å¯¼æ•°ã€‚\n4. **åŸºåˆ†ç±»å™¨ï¼š** ä¼ ç»Ÿçš„GBDTé‡‡ç”¨CARTä½œä¸ºåŸºåˆ†ç±»å™¨ï¼ŒXGBoostæ”¯æŒå¤šç§ç±»å‹çš„åŸºåˆ†ç±»å™¨ï¼Œæ¯”å¦‚çº¿æ€§åˆ†ç±»å™¨ã€‚\n5. **å­é‡‡æ ·ï¼š** ä¼ ç»Ÿçš„GBDTåœ¨æ¯è½®è¿­ä»£æ—¶ä½¿ç”¨å…¨éƒ¨çš„æ•°æ®ï¼ŒXGBooståˆ™é‡‡ç”¨äº†ä¸éšæœºæ£®æ—ç›¸ä¼¼çš„ç­–ç•¥ï¼Œæ”¯æŒå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ã€‚\n6. **ç¼ºå¤±å€¼å¤„ç†ï¼š** ä¼ ç»ŸGBDTæ²¡æœ‰è®¾è®¡å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¤„ç†ï¼ŒXGBoostèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ å‡ºç¼ºå¤±å€¼çš„å¤„ç†ç­–ç•¥ã€‚\n7. **å¹¶è¡ŒåŒ–ï¼š** ä¼ ç»ŸGBDTæ²¡æœ‰è¿›è¡Œå¹¶è¡ŒåŒ–è®¾è®¡ï¼Œæ³¨æ„ä¸æ˜¯treeç»´åº¦çš„å¹¶è¡Œï¼Œè€Œæ˜¯ç‰¹å¾ç»´åº¦çš„å¹¶è¡Œã€‚XGBoosté¢„å…ˆå°†æ¯ä¸ªç‰¹å¾æŒ‰ç‰¹å¾å€¼æ’å¥½åºï¼Œå­˜å‚¨ä¸ºå—ç»“æ„ï¼Œåˆ†è£‚ç»“ç‚¹æ—¶å¯ä»¥é‡‡ç”¨å¤šçº¿ç¨‹å¹¶è¡ŒæŸ¥æ‰¾æ¯ä¸ªç‰¹å¾çš„æœ€ä½³åˆ†å‰²ç‚¹ï¼Œæå¤§æå‡è®­ç»ƒé€Ÿåº¦ã€‚\n\n##### 4.2 ä¸ºä»€ä¹ˆXGBoostæ³°å‹’äºŒé˜¶å±•å¼€åæ•ˆæœå°±æ¯”è¾ƒå¥½å‘¢ï¼Ÿ\n\nï¼ˆ1ï¼‰**ä»ä¸ºä»€ä¹ˆä¼šæƒ³åˆ°å¼•å…¥æ³°å‹’äºŒé˜¶çš„è§’åº¦æ¥è¯´ï¼ˆå¯æ‰©å±•æ€§ï¼‰ï¼š** XGBoostå®˜ç½‘ä¸Šæœ‰è¯´ï¼Œå½“ç›®æ ‡å‡½æ•°æ˜¯`MSE`æ—¶ï¼Œå±•å¼€æ˜¯ä¸€é˜¶é¡¹ï¼ˆæ®‹å·®ï¼‰+äºŒé˜¶é¡¹çš„å½¢å¼ï¼Œè€Œå…¶å®ƒç›®æ ‡å‡½æ•°ï¼Œå¦‚`logistic loss`çš„å±•å¼€å¼å°±æ²¡æœ‰è¿™æ ·çš„å½¢å¼ã€‚ä¸ºäº†èƒ½æœ‰ä¸ªç»Ÿä¸€çš„å½¢å¼ï¼Œæ‰€ä»¥é‡‡ç”¨æ³°å‹’å±•å¼€æ¥å¾—åˆ°äºŒé˜¶é¡¹ï¼Œè¿™æ ·å°±èƒ½æŠŠ`MSE`æ¨å¯¼çš„é‚£å¥—ç›´æ¥å¤ç”¨åˆ°å…¶å®ƒè‡ªå®šä¹‰æŸå¤±å‡½æ•°ä¸Šã€‚ç®€çŸ­æ¥è¯´ï¼Œå°±æ˜¯ä¸ºäº†ç»Ÿä¸€æŸå¤±å‡½æ•°æ±‚å¯¼çš„å½¢å¼ä»¥æ”¯æŒè‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚è‡³äºä¸ºä»€ä¹ˆè¦åœ¨å½¢å¼ä¸Šä¸`MSE`ç»Ÿä¸€ï¼Ÿæ˜¯å› ä¸º`MSE`æ˜¯æœ€æ™®éä¸”å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œè€Œä¸”æ±‚å¯¼æœ€å®¹æ˜“ï¼Œæ±‚å¯¼åçš„å½¢å¼ä¹Ÿååˆ†ç®€å•ã€‚æ‰€ä»¥ç†è®ºä¸Šåªè¦æŸå¤±å‡½æ•°å½¢å¼ä¸`MSE`ç»Ÿä¸€äº†ï¼Œé‚£å°±åªç”¨æ¨å¯¼`MSE`å°±å¥½äº†ã€‚\n\nï¼ˆ2ï¼‰**ä»äºŒé˜¶å¯¼æœ¬èº«çš„æ€§è´¨ï¼Œä¹Ÿå°±æ˜¯ä»ä¸ºä»€ä¹ˆè¦ç”¨æ³°å‹’äºŒé˜¶å±•å¼€çš„è§’åº¦æ¥è¯´ï¼ˆç²¾å‡†æ€§ï¼‰ï¼š** äºŒé˜¶ä¿¡æ¯æœ¬èº«å°±èƒ½è®©æ¢¯åº¦æ”¶æ•›æ›´å¿«æ›´å‡†ç¡®ã€‚è¿™ä¸€ç‚¹åœ¨ä¼˜åŒ–ç®—æ³•é‡Œçš„ç‰›é¡¿æ³•ä¸­å·²ç»è¯å®ã€‚å¯ä»¥ç®€å•è®¤ä¸ºä¸€é˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘ï¼ŒäºŒé˜¶å¯¼æŒ‡å¼•æ¢¯åº¦æ–¹å‘å¦‚ä½•å˜åŒ–ã€‚ç®€å•æ¥è¯´ï¼Œç›¸å¯¹äºGBDTçš„ä¸€é˜¶æ³°å‹’å±•å¼€ï¼ŒXGBoosté‡‡ç”¨äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œå¯ä»¥æ›´ä¸ºç²¾å‡†çš„é€¼è¿‘çœŸå®çš„æŸå¤±å‡½æ•°ã€‚\n\n##### 4.3 XGBoostå¯¹ç¼ºå¤±å€¼æ˜¯æ€ä¹ˆå¤„ç†çš„ï¼Ÿ\n\nåœ¨æ™®é€šçš„GBDTç­–ç•¥ä¸­ï¼Œå¯¹äºç¼ºå¤±å€¼çš„æ–¹æ³•æ˜¯å…ˆæ‰‹åŠ¨å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ï¼Œç„¶åå½“åšæœ‰å€¼çš„ç‰¹å¾è¿›è¡Œå¤„ç†ï¼Œä½†æ˜¯è¿™æ ·äººå·¥å¡«å……ä¸ä¸€å®šå‡†ç¡®ï¼Œè€Œä¸”æ²¡æœ‰ä»€ä¹ˆç†è®ºä¾æ®ã€‚è€ŒXGBoosté‡‡å–çš„ç­–ç•¥æ˜¯å…ˆä¸å¤„ç†é‚£äº›å€¼ç¼ºå¤±çš„æ ·æœ¬ï¼Œé‡‡ç”¨é‚£äº›æœ‰å€¼çš„æ ·æœ¬æå‡ºåˆ†è£‚ç‚¹ï¼Œåœ¨éå†æ¯ä¸ªæœ‰å€¼ç‰¹å¾çš„æ—¶å€™ï¼Œå°è¯•å°†ç¼ºå¤±æ ·æœ¬åˆ’å…¥å·¦å­æ ‘å’Œå³å­æ ‘ï¼Œé€‰æ‹©ä½¿æŸå¤±æœ€ä¼˜çš„å€¼ä½œä¸ºåˆ†è£‚ç‚¹ã€‚\n\n#### 5 å‚è€ƒæ–‡çŒ® \n\n- é™ˆå¤©å¥‡è®ºæ–‡åŸæ–‡ XGBoost: A Scalable Tree Boosting System\n- [æ·±å…¥ç†è§£ XGBoostï¼šKaggle æœ€ä¸»æµçš„é›†æˆç®—æ³•ï¼](mp.weixin.qq.com/s/0dehrF5Zvn5ILkadfsqfUw)\n\n"},{"title":"æ¢¯åº¦æå‡æ ‘","url":"/2020/03/18/æ¢¯åº¦æå‡æ ‘/","content":"æå‡æ ‘æ˜¯åˆ†ç±»æ ‘æˆ–å›å½’æ ‘ä¸ºåŸºæœ¬åˆ†ç±»å™¨çš„æå‡æ–¹æ³•ã€‚æå‡æ ‘è¢«è®¤ä¸ºæ˜¯ç»Ÿè®¡å­¦ä¹ ä¸­æ€§èƒ½æœ€å¥½çš„æ–¹æ³•ä¹‹ä¸€ã€‚\n\n##### 1.1 æå‡æ ‘æ¨¡å‹\n\næå‡æ ‘çš„æ€æƒ³å¯ä»¥ç”¨ä¸€ä¸ªé€šä¿—çš„ä¾‹å­è§£é‡Šï¼Œå‡å¦‚æœ‰ä¸ªäºº30å²ï¼Œæˆ‘ä»¬é¦–å…ˆç”¨20å²å»æ‹Ÿåˆï¼Œå‘ç°æŸå¤±æœ‰10å²ï¼Œè¿™æ—¶æˆ‘ä»¬ç”¨6å²å»æ‹Ÿåˆå‰©ä¸‹çš„æŸå¤±ï¼Œå‘ç°å·®è·è¿˜æœ‰4å²ï¼Œç¬¬ä¸‰è½®æˆ‘ä»¬ç”¨3å²æ‹Ÿåˆå‰©ä¸‹çš„å·®è·ï¼Œå·®è·å°±åªæœ‰ä¸€å²äº†ã€‚å¦‚æœæˆ‘ä»¬çš„è¿­ä»£è½®æ•°è¿˜æ²¡æœ‰å®Œï¼Œå¯ä»¥ç»§ç»­è¿­ä»£ä¸‹é¢ï¼Œæ¯ä¸€è½®è¿­ä»£ï¼Œæ‹Ÿåˆçš„å²æ•°è¯¯å·®éƒ½ä¼šå‡å°ã€‚\n\næå‡æ–¹æ³•å®é™…é‡‡ç”¨åŠ æ³•æ¨¡å‹ï¼ˆå³åŸºå‡½æ•°çš„çº¿æ€§ç»„åˆï¼‰ä¸å‰å‘åˆ†å¸ƒç®—æ³•ã€‚ä»¥å†³ç­–æ ‘ä¸ºåŸºå‡½æ•°çš„æå‡æ–¹æ³•ç§°ä¸ºæå‡æ ‘ï¼ˆboosting treeï¼‰ã€‚å¯¹åˆ†ç±»é—®é¢˜å†³ç­–æ ‘æ˜¯äºŒå‰åˆ†ç±»æ ‘ï¼Œå¯¹å›å½’é—®é¢˜å†³ç­–æ ‘æ˜¯äºŒå‰å›å½’æ ‘ã€‚æå‡æ ‘æ¨¡å‹å¯ä»¥è¡¨ç¤ºæˆå†³ç­–æ ‘çš„åŠ æ³•æ¨¡å‹ã€‚\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\nâ€‹\tå…¶ä¸­ï¼Œ$T(x;\\Theta_m)$è¡¨ç¤ºå†³ç­–æ ‘ï¼›$\\Theta_m$è¡¨ç¤ºå†³ç­–æ ‘çš„å‚æ•°ï¼›$M$ä¸ºæ ‘çš„ä¸ªæ•°.\n\n##### 1.2 æå‡æ ‘ç®—æ³•\n\næå‡æ ‘ç®—æ³•é‡‡å–å‰å‘åˆ†æ­¥ç®—æ³•ã€‚é¦–å…ˆç¡®å®šåˆå§‹æå‡æ ‘$f_0(x) = 0$,ç¬¬$m$æ­¥çš„æ¨¡å‹æ˜¯\n$$\nf_m(x) = f_{m-1}(x)+T(x;\\Theta_m)\n$$\nå…¶ä¸­$f_{m-1}(x)$ä¸ºå½“å‰æ¨¡å‹ï¼Œé€šè¿‡ç»éªŒé£é™©æå°åŒ–ç¡®å®šä¸‹ä¸€æ£µå†³ç­–æ ‘çš„å‚æ•°$\\Theta_m$,\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\nç”±äºæ ‘çš„çº¿æ€§ç»„åˆå¯ä»¥å¾ˆå¥½çš„æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå³æ•°æ®ä¸­çš„è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„å…³ç³»å¾ˆå¤æ‚ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œæ‰€ä»¥æå‡æ ‘æ˜¯ä¸€ä¸ªé«˜åŠŸèƒ½çš„å­¦ä¹ ç®—æ³•ã€‚\n\nä¸‹é¢è®¨è®ºé’ˆå¯¹ä¸åŒé—®é¢˜çš„æå‡æ ‘å­¦ä¹ ç®—æ³•ï¼Œå…¶ä¸»è¦åŒºåˆ«åœ¨äºä½¿ç”¨çš„æŸå¤±å‡½æ•°ä¸åŒã€‚åŒ…æ‹¬ç”¨å¹³æ–¹æŸå¤±å‡½æ•°çš„å›å½’é—®é¢˜ï¼Œç”¨æŒ‡æ•°æŸå¤±å‡½æ•°çš„åˆ†ç±»é—®é¢˜ï¼Œä»¥åŠä¸€èˆ¬æŸå¤±å‡½æ•°çš„ä¸€èˆ¬å†³ç­–é—®é¢˜ã€‚\n\nå¯¹äºäºŒåˆ†ç±»åˆ†ç±»é—®é¢˜ï¼Œæå‡æ ‘ç®—æ³•åªéœ€è¦å°†$Adaboost$ç®—ä¸­åŸºæœ¬åˆ†ç±»å™¨é™åˆ¶ä¸ºäºŒç±»åˆ†ç±»æ ‘å³å¯ï¼Œå¯ä»¥è¯´è¿™æ—¶çš„æå‡æ ‘ç®—æ³•æ˜¯$Adaboost$ç®—æ³•çš„ç‰¹æ®Šæƒ…å†µï¼Œè¿™é‡Œä¸å†è¯¦ç»†å™è¿°ã€‚ä¸‹é¢é‡ç‚¹å™è¿°å›å½’é—®é¢˜çš„æå‡æ ‘ã€‚\n\nå·²çŸ¥ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$ä¸ºè¾“å…¥ç©ºé—´ï¼Œ$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$ä¸ºè¾“å‡ºç©ºé—´ã€‚å¦‚æœå°†è¾“å…¥ç©ºé—´$\\chi$åˆ’åˆ†æˆ$J$ä¸ªäº’ä¸ç›¸äº¤çš„åŒºåŸŸ$R_1,R_2,...,R_J$ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªåŒºåŸŸä¸Šç¡®å®šè¾“å‡ºçš„å¸¸é‡$c_j$,é‚£ä¹ˆæ ‘å¯ä»¥è¡¨ç¤ºä¸º\n$$\nT(x;\\Theta) = \\sum_{j=1}^{J}c_jI(x \\in R_j)\n$$\nå…¶ä¸­å‚æ•°$\\Theta={(R_1,c_1),(R_2,c_2),...,(R_J,c_J)}$è¡¨ç¤ºæ ‘çš„åŒºåŸŸåˆ’åˆ†å’Œå„åŒºåŸŸä¸Šå¸¸æ•°ï¼Œ$J$æ˜¯å›å½’æ ‘çš„å¤æ‚åº¦å³å¶å­èŠ‚ç‚¹çš„ä¸ªæ•°ã€‚\n\nå›å½’é—®é¢˜æå‡æ ‘ä½¿ç”¨ä»¥ä¸‹å‰å‘åˆ†å¸ƒç®—æ³•ï¼š\n$$\n\\begin{aligned}\nf_0(x) &= 0\\\\\nf_m(x) &= f_{m-1}(x) + T(x;\\Theta_m),m = 1,2,...,M \\\\\nf_M(x) &= \\sum_{m=1}^{M}T(x;\\Theta_m) \\\\\n\\end{aligned}\n$$\nåœ¨å‰å‘åˆ†å¸ƒç®—æ³•çš„ç¬¬$m$æ­¥ï¼Œç»™å®šå½“å‰æ¨¡å‹$f_{m-1}(x)$ï¼Œéœ€æ±‚è§£\n$$\n\\hat{\\Theta}_m = \\arg \\min_{\\Theta_m}\\sum_{i=1}^{M}L(y_i,f_{m-1}(x_i)+T(x_i;\\Theta_m))\n$$\nå¾—åˆ°$\\hat{\\Theta}_m$,å³ç¬¬$m$æ£µæ ‘çš„å‚æ•°ã€‚\n\nå½“é‡‡ç”¨å¹³æ–¹æŸå¤±å‡½æ•°æ—¶ï¼Œ\n$$\nL(y,f(x)) = (y - f(x))^{2}\n$$\nå…¶æŸå¤±å˜ä¸º\n$$\n\\begin{aligned}\nL(y,f_{m-1}(x) - T(x;\\Theta_m)) &=(y - f_{m-1}(x) - T(x;\\Theta_m))^{m}\\\\\n &= (r - T(x;\\Theta_m))^2\n\\end{aligned}\n$$\nè¿™é‡Œ,\n$$\nr = y - f_{m-1}(x)\n$$\næ˜¯å½“å‰æ¨¡å‹æ‹Ÿåˆæ•°æ®çš„**æ®‹å·®(residual)**,æ‰€ä»¥ï¼Œå¯¹å›å½’é—®é¢˜çš„æå‡æ ‘æ¥è¯´ï¼Œåªéœ€è¦ç®€å•åœ°æ‹Ÿåˆå½“å‰æ¨¡å‹çš„æ®‹å·®ã€‚è¿™æ ·ï¼Œç®—æ³•æ˜¯ç›¸å½“ç®€å•ã€‚ \n\n**ç®—æ³•1 å›å½’é—®é¢˜çš„æå‡æ ‘ç®—æ³•**\n\nè¾“å…¥ï¼šè®­ç»ƒæ•°æ®$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$ä¸ºè¾“å…¥ç©ºé—´ï¼Œ$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$ä¸ºè¾“å‡ºç©ºé—´\n\nè¾“å‡ºï¼šæå‡æ ‘$f_M(x)$\n\n(1) åˆå§‹åŒ–$f_0(x) = 0$\n\n(2) å¯¹$m=1,2,3,...,M$\n\nâ€‹     (a) æŒ‰ç…§å¼$r = y - f_{m-1}(x)$è®¡ç®—æ®‹å·®\n$$\nr_{mi} = y_i - f_{m-1}(x_i), i = 1,2,...,N\n$$\nâ€‹     (b) æ‹Ÿåˆæ®‹å·®$r_{mi}$å­¦ä¹ ä¸€æ£µå›å½’æ ‘ï¼Œå¾—åˆ°$T(x;\\Theta_m)$\n\nâ€‹\t (c) æ›´æ–°$f_m(x) = f_{m-1}(x) + T(x;\\Theta_m)$\n\n(3) å¾—åˆ°å›å½’é—®é¢˜æå‡æ ‘\n$$\nf_M(x) = \\sum_{m=1}^{M}T(x;\\Theta_m)\n$$\n\n##### 1.3 æ¢¯åº¦æå‡\n\næ¢¯åº¦æå‡(Gradient Boostingï¼‰æ˜¯Boostingä¸­çš„ä¸€å¤§ç±»ç®—æ³•ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯æ ¹æ®å½“å‰æ¨¡å‹æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ä¿¡æ¯æ¥è®­ç»ƒæ–°åŠ å…¥çš„å¼±åˆ†ç±»å™¨ï¼Œç„¶åå°†è®­ç»ƒå¥½çš„å¼±åˆ†ç±»å™¨ä»¥ç´¯åŠ çš„å½¢å¼ç»“åˆåˆ°ç°æœ‰æ¨¡å‹ä¸­ã€‚æå‡æ ‘åˆ©ç”¨åŠ æ³•æ¨¡å‹ä¸å‰å‘åˆ†æ­¥ç®—æ³•å®ç°å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚å½“æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹æŸå¤±å’ŒæŒ‡æ•°æŸå¤±æ—¶ï¼Œæ¯ä¸€æ­¥ä¼˜åŒ–æ˜¯å¾ˆç®€å•çš„ã€‚å¯¹äºä¸€èˆ¬çš„æŸå¤±å‡½æ•°è€Œè¨€ï¼Œå¾€å¾€æ¯ä¸€æ­¥ä¼˜åŒ–å¹¶ä¸æ˜¯é‚£ä¹ˆå®¹æ˜“ã€‚é’ˆå¯¹è¿™ä¸€é—®é¢˜ï¼ŒFreidmanæå‡ºäº†æ¢¯åº¦æå‡(gradient boosting)ç®—æ³•ã€‚è¿™æ˜¯åˆ©ç”¨æœ€é€Ÿä¸‹é™æ³•çš„è¿‘ä¼¼æ–¹æ³•ï¼Œ**å…¶å…³é”®æ˜¯åˆ©ç”¨æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦åœ¨å½“å‰æ¨¡å‹çš„å€¼**ï¼š\n$$\n-[\\frac{\\partial{L(y,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\n**ä½œä¸ºå›å½’é—®é¢˜æå‡æ ‘ç®—æ³•ä¸­æ®‹å·®çš„è¿‘ä¼¼å€¼**ï¼Œæ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ã€‚\n\n**ç®—æ³•2 æ¢¯åº¦æå‡æ ‘ç®—æ³•**\n\nè¾“å…¥ï¼šè®­ç»ƒæ•°æ®$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)},x_{i} \\in \\chi \\subseteq\\mathbb{R}^n$,$\\chi$ä¸ºè¾“å…¥ç©ºé—´ï¼Œ$y_i \\in \\nu \\subseteq \\mathbb{R}  $ ,$\\nu$ä¸ºè¾“å‡ºç©ºé—´\n\nè¾“å‡º: å›å½’æ ‘$\\hat{f}(x)$\n\n(1) åˆå§‹åŒ–$f_0(x) = \\arg \\min_{c}\\sum_{i=1}^{N}L(y_i,c)$\n\n(2) å¯¹$m=1,2,3,...,M$\n\nâ€‹     (a) å¯¹$i = 1,2, ...,N$è®¡ç®—æ®‹å·®\n$$\nr_{mi} = -[\\frac{\\partial{L(y_i,f(x_i))}}{\\partial{f(x_i)}}]_{f(x) = f_{m-1}(x)}\n$$\nâ€‹     (b) æ‹Ÿåˆ$r_{mi}$å­¦ä¹ ä¸€æ£µå›å½’æ ‘ï¼Œå¾—åˆ°ç¬¬$m$æ£µæ ‘çš„å¶èŠ‚ç‚¹åŒºåŸŸ$R_{mj},j  = 1,2,...,J$\n\nâ€‹\t (c) å¯¹äº$j  = 1,2,...,J$ï¼Œè®¡ç®—\n$$\nc_{mj} = \\arg \\min_{c}\\sum_{x_i \\in R_{mj}}L(y_i,f_{m-1}(x_i) +c)\n$$\nâ€‹    (dï¼‰æ›´æ–°$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^Jc_{mi}I(x \\in R_{mj})$\n\n(3) å¾—åˆ°å›å½’é—®é¢˜æå‡æ ‘\n$$\n\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x\\in R_{mj})\n$$\nç®—æ³•çš„ç¬¬ä¸€æ­¥åˆå§‹åŒ–ï¼Œä¼°è®¡ä½¿æŸå¤±å‡½æ•°æå°åŒ–çš„å¸¸æ•°å€¼ï¼Œå®ƒåªæœ‰ä¸€ä¸ªæ ¹èŠ‚ç‚¹çš„æ ‘ï¼Œç¬¬2(a)æ­¥è®¡ç®—æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦åœ¨å½“å‰æ¨¡å‹çš„å€¼ï¼Œå°†å®ƒä½œä¸ºæ®‹å·®çš„ä¼°è®¡ï¼Œå¯¹äºå¹³æ–¹æŸå¤±å‡½æ•°æ¥è¯´ï¼Œå®ƒçš„è´Ÿæ¢¯åº¦å…¶å®å°±æ˜¯å¸¸è¯´çš„æ®‹å·®ï¼Œå¯¹äºä¸€èˆ¬çš„æŸå¤±å‡½æ•°ï¼Œå®ƒå°±æ˜¯æ®‹å·®çš„è¿‘ä¼¼å€¼ã€‚ç¬¬2ï¼ˆb)æ­¥ï¼Œä¼°è®¡å›å½’æ ‘å¶å­èŠ‚ç‚¹åŒºåŸŸï¼Œä»¥æ‹Ÿåˆæ®‹å·®çš„è¿‘ä¼¼å€¼ã€‚ç¬¬2(c)æ­¥åˆ©ç”¨çº¿æ€§æœç´¢ä¼°è®¡å¶å­ç»ˆç‚¹åŒºåŸŸçš„å€¼ï¼Œä½¿æŸå¤±å‡½æ•°æå°åŒ–ã€‚ç¬¬2ï¼ˆdï¼‰æ­¥æ›´æ–°å›å½’æ ‘ã€‚ç¬¬3æ­¥ï¼Œå¾—åˆ°è¾“å‡ºçš„æœ€ç»ˆæ¨¡å‹ã€‚\n\n##### 1.4 æå‡æ ‘ä¸»è¦æŸå¤±å‡½æ•°\n\nä¸‹é¢æˆ‘ä»¬å¯¹æå‡æ ‘æ‰€ç”¨çš„æŸå¤±å‡½æ•°åšä¸€ä¸ªæ€»ç»“ï¼š\n\n1) å¯¹äºåˆ†ç±»ç®—æ³•æ¥è¯´ï¼šå…¶æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰å¯¹æ•°æŸå¤±å‡½æ•°å’ŒæŒ‡æ•°æŸå¤±å‡½æ•°ï¼š\n\naï¼‰**æŒ‡æ•°æŸå¤±å‡½æ•°**\n$$\nL(y_i,f(x_i)) = exp(-y_if(x_i))\n$$\nå…¶è´Ÿæ¢¯åº¦è¯¯å·®ä¸ºï¼š\n$$\n-y_i.exp(-f(x_i))\n$$\nbï¼‰**å¯¹æ•°æŸå¤±å‡½æ•°**\n$$\nL(y_i,f(x_i)) = ln(1+exp(-y_i.f(x_i)))\n$$\nå…¶è´Ÿæ¢¯åº¦ä¸ºï¼š\n$$\n\\frac{y_i.exp(-y_i.f(x_i))}{1+exp(-y_i.f(x_i))}\n$$\nåŒ–ç®€ä¸ºï¼š\n$$\n\\frac{y_i}{(1+exp(y_if(x_i)))}\n$$\n2) å›å½’ç®—æ³•ï¼šå¸¸è§çš„æœ‰ä»¥ä¸‹å››ç§\n\n1. **å‡æ–¹å·®æŸå¤±å‡½æ•°**\n   $$\n   L(y_i,f(x_i)) = (y_i - f(x_i))^2\n   $$\n   å…¶è´Ÿæ¢¯åº¦ä¸ºï¼š\n   $$\n   y_i - f(x_i)\n   $$\n   p.s. æŸå¤±å‡½æ•°ä¸º$L(y,f(x))=(y-f(x))^2$,æˆ‘ä»¬éœ€è¦æœ€å°åŒ–$J= \\sum_iL(y_i,f(x_i))$é€šè¿‡è°ƒæ•´$f(x_1),f(x_2),...,f(x_n)$.æˆ‘ä»¬æŠŠ$f(x_i)$å½“æˆå‚æ•°å¹¶æ±‚å¯¼\n   $$\n   \\frac{\\partial}{\\partial f(x_i)} = \\frac{\\partial \\sum_iL(y_i,f(x_i))}{\\partial f(x_i)} = \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} = f(x_i) - y_i\n   $$\n   æ‰€ä»¥ï¼Œ**æˆ‘ä»¬åœ¨å‡æ–¹å·®æŸå¤±å‡½æ•°ä¸‹ï¼Œå¯ä»¥æŠŠæ®‹å·®ç†è§£æˆè´Ÿæ¢¯åº¦ã€‚**\n   $$\n   y_i-f(x_i) = - \\frac{\\partial}{\\partial f(x_i)}\n   $$\n   \n2. ç»å¯¹æŸå¤±å‡½æ•°ï¼š\n   $$\n   L(y_i,f(x_i) = |y_i - f(x_i)|\n   $$\n   å…¶å¯¹åº”çš„è´Ÿæ¢¯åº¦ä¸ºï¼š\n   $$\n   sign (y_i - f(x_i))\n   $$\n\n3. HuberæŸå¤±å‡½æ•°ï¼šå®ƒæ˜¯å‡æ–¹å·®å’Œç»å¯¹æŸå¤±çš„æŠ˜è¡·äº§ç‰©ï¼Œå¯¹äºè¿œç¦»ä¸­å¿ƒçš„å¼‚å¸¸ç‚¹ï¼Œé‡‡ç”¨ç»å¯¹æŸå¤±ï¼Œè€Œä¸­å¿ƒé™„è¿‘çš„ç‚¹é‡‡ç”¨å‡æ–¹å·®ã€‚è¿™ä¸ªç•Œé™ä¸€èˆ¬ç”¨åˆ†ä½æ•°ç‚¹åº¦é‡ã€‚æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š\n   $$\n   \\begin{aligned}\n   L(y_i,f(x_i)) = \n   \\begin{cases} \n   \\frac{1}{2}(y_i-f(x_i))^2, |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta(|y_i -f(x_i)| - \\frac{\\delta}{2}), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n   å…¶å¯¹åº”çš„è´Ÿæ¢¯åº¦ä¸ºï¼š\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   y_i-f(x_i), |y_i - f(x_i)|\\leq \\delta\\\\\n   \\delta .sign(y_i -f(x_i)), |y_i - f(x_i)|\\geq \\delta\n   \\end{cases}\n   \\end{aligned}\n   $$\n\n4. åˆ†ä½æ•°æŸå¤±ã€‚å®ƒå¯¹åº”çš„æ˜¯åˆ†ä½æ•°å›å½’çš„æŸå¤±å‡½æ•°ï¼Œè¡¨è¾¾å¼ä¸º\n   $$\n   L(x_i,f(x_i)) = \\sum_{y_i \\geq f(x_i)}\\theta|y_i - f(x_i)| + \\sum_{y_i < f(x_i)}(1-\\theta)|y_i - f(x_i)| \n   $$\n   å…¶ä¸­ï¼Œ$\\theta$ä¸ºåˆ†ä½æ•°ï¼Œéœ€è¦åœ¨å›å½’é’±è®¾ç½®ï¼Œå…¶å¯¹åº”çš„è´Ÿæ¢¯åº¦ä¸ºï¼š\n   $$\n   \\begin{aligned}\n   r(y_i,f(x_i)) = \n   \\begin{cases} \n   \\theta,  y_i \\geq f(x_i)\\\\\n   \\theta - 1, y_i < f(x_i)\n   \\end{cases}\n   \\end{aligned}\n   $$\n   ![](.\\æ¢¯åº¦æå‡æ ‘\\æŸå¤±å‡½æ•°.png)\n\n#####    1.5 æ€»ç»“åŠä¼˜ç¼ºç‚¹\n\næœ¬æ–‡ä»‹ç»äº†boostingæ—çš„æå‡æ ‘ç®—æ³•å’Œæ¢¯åº¦æå‡æ ‘ï¼ˆGBDT)ç®—æ³•ï¼Œæå‡æ ‘ç®—æ³•çš„æ¯è½®å¼±å­¦ä¹ å™¨æ˜¯æ‹Ÿåˆä¸Šä¸€è½®çš„æ®‹å·®ç”Ÿæˆçš„ï¼ŒGBDTç®—æ³•çš„æ¯è½®å¼±å­¦ä¹ å™¨æ˜¯æ‹Ÿåˆä¸Šä¸€è½®æŸå¤±å‡½æ•°çš„è´Ÿæ¢¯åº¦ç”Ÿæˆçš„ã€‚æå‡æ ‘ç®—æ³•å’ŒGBDTç®—æ³•éƒ½æ˜¯ç”¨CARTå›å½’æ ‘ä½œä¸ºå¼±å­¦ä¹ å™¨ï¼Œåªè¦ç¡®å®šæ¨¡å‹çš„æŸå¤±å‡½æ•°ï¼Œæå‡æ ‘å’ŒGBDTå°±å¯ä»¥é€šè¿‡å‰å‘åˆ†å¸ƒç®—æ³•è¿›è¡Œæ„å»ºã€‚\n\n\n\næ¢¯åº¦æå‡æ ‘ä¸»è¦çš„ä¼˜ç‚¹æœ‰ï¼š\n\n1ï¼‰ å¯ä»¥çµæ´»å¤„ç†å„ç§ç±»å‹çš„æ•°æ®ï¼ŒåŒ…æ‹¬è¿ç»­å€¼å’Œç¦»æ•£å€¼ã€‚\n\n2ï¼‰åœ¨ç›¸å¯¹å°‘çš„è°ƒå‚æ—¶é—´æƒ…å†µä¸‹ï¼Œé¢„æµ‹çš„å‡†å¤‡ç‡ä¹Ÿå¯ä»¥æ¯”è¾ƒé«˜ã€‚è¿™ä¸ªæ˜¯ç›¸å¯¹SVMæ¥è¯´çš„ã€‚\n\n3ï¼‰ä½¿ç”¨ä¸€äº›å¥å£®çš„æŸå¤±å‡½æ•°ï¼Œå¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§éå¸¸å¼ºã€‚æ¯”å¦‚ HuberæŸå¤±å‡½æ•°å’ŒQuantileæŸå¤±å‡½æ•°ã€‚\n\næ¢¯åº¦æå‡æ ‘çš„ä¸»è¦ç¼ºç‚¹æœ‰ï¼š\n\n1ï¼‰ç”±äºå¼±å­¦ä¹ å™¨ä¹‹é—´å­˜åœ¨ä¾èµ–å…³ç³»ï¼Œéš¾ä»¥å¹¶è¡Œè®­ç»ƒæ•°æ®ã€‚ä¸è¿‡å¯ä»¥é€šè¿‡è‡ªé‡‡æ ·çš„SGBTæ¥è¾¾åˆ°éƒ¨åˆ†å¹¶è¡Œã€‚\n\n##### 1.6 å‚è€ƒæ–‡çŒ®\n\næèˆª ã€Šç»Ÿè®¡å­¦ä¹ ã€‹\n\n"},{"title":"é€»è¾‘å›å½’åˆ†ç±»å’Œsoftmaxåˆ†ç±»","url":"/2020/02/12/é€»è¾‘å›å½’åˆ†ç±»å’Œsoftmaxåˆ†ç±»/","content":"### é€»è¾‘å›å½’åˆ†ç±»å’Œsoftmaxåˆ†ç±»\n\n#### 1.é€»è¾‘å›å½’\n\n##### 1.1 ç®—æ³•åŸç†\n\nä¸€äº›å›å½’ç®—æ³•ä¹Ÿå¯ä»¥ç”¨äºåˆ†ç±»ï¼Œåä¹‹äº¦ç„¶ã€‚é€»è¾‘å›å½’å°±æ˜¯è¢«å¹¿æ³›ç”¨äºä¼°ç®—ä¸€ä¸ªå®ä¾‹å±äºæŸä¸ªç‰¹å®šç±»åˆ«çš„æ¦‚ç‡ã€‚å¦‚æœä¼°ç®—æ¦‚ç‡è¶…è¿‡50%å°±æ˜¯å±äºè¯¥ç±»ï¼Œåä¹‹åˆ™ä¸æ˜¯ã€‚\n\né€»è¾‘å›å½’æ¨¡å‹æ¦‚ç‡ä¼°ç®—:\n$$\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdot x)$$\n\né€»è¾‘å‡½æ•°ï¼š\n$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$\n\né¢„æµ‹æ¨¡å‹ï¼š\n$$\\hat{y}=\n\\begin{cases}\n0 & (\\hat{p}<0.5)\\\\\n1 & (\\hat{p}\\geq0.5)\n\\end{cases}$$\n\nå•ä¸ªè®­ç»ƒå®ä¾‹çš„æŸå¤±å‡½æ•°:\n$$c(\\theta)=\n\\begin{cases}\n-log(\\hat{p}) & (y=1)\\\\\n-log(1-\\hat{p}) & (y=0)\n\\end{cases}$$\n\næˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå½“$p$æ¥è¿‘äº$0$çš„æ—¶å€™ï¼Œ$-\\log(p)$ä¼šå˜å¾—éå¸¸å¤§ï¼Œæ‰€ä»¥å¦‚æœæ¨¡å‹ä¼°ç®—ä¸€ä¸ªæ­£å®ä¾‹çš„æ¦‚ç‡æ¥è¿‘äº$0$ï¼Œé‚£ä¹ˆæŸå¤±å‡½æ•°å°±ä¼šéå¸¸é«˜ï¼Œåè¿‡æ¥ï¼Œå½“$p$æ¥è¿‘äº$1$çš„æ—¶å€™ï¼Œ$-\\log(p)$æ¥è¿‘äº$0$ï¼Œæ‰€ä»¥å¯¹ä¸€ä¸ªè´Ÿç±»å®ä¾‹ä¼°ç®—å‡ºçš„æ¦‚ç‡æ¥è¿‘äº$0$ï¼ŒæŸå¤±å‡½æ•°ä¹Ÿä¼šå¾ˆä½ã€‚\n\né€»è¾‘å›å½’æˆæœ¬å‡½æ•°:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$$\n\nåæ¶ˆæ¯æ˜¯ï¼Œè¿™ä¸ªå‡½æ•°æ²¡æœ‰å·²çŸ¥çš„é—­å¼æ–¹ç¨‹(ä¹Ÿå°±æ˜¯ä¸å°Šåœ¨ä¸€ä¸ªæ ‡å‡†æ–¹å·®çš„ç­‰ä»·æ–¹ç¨‹)ã€‚å¥½æ¶ˆæ¯ï¼Œè¿™æ˜¯ä¸ªå‡¸å‡½æ•°ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ç®—æ³•ä¿è¯èƒ½å¤Ÿæ‰¾å‡ºå…¨å±€æœ€å°å€¼ã€‚\n\nLogisticæŸå¤±å‡½æ•°çš„åå¯¼æ•°:\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^T \\cdot x^{(i)})-y^{(i)})x_j^{(i)}$$\n\n$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^T\\cdot x^{i}-y^{i})x_j^{i}$$\n\n##### 1.2 scala ä»£ç å®ç°\n\n```scala\npackage ml.scrath.classification\n\nimport scala.collection.mutable.ArrayBuffer\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n\n\nobject LogitRegression{\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map{_.split(\",\").filter(_.length() > 0).map(_.toDouble)}\n      .toArray\n    val data = BDM(dataS:_*)\n\n    val features = data(0 to 98, 0 to 3)\n    val labels = data(0 to 98, 4)\n\n    val model = new LogitRegression\n    val w = model.fit(features,labels)\n    val predictions = model.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"æ­£ç¡®ç‡ä¸ºï¼š\" + rate)\n  }\n}\n\n\nclass LogitRegression (var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // æ³¨æ„æ˜¯:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = (x_train * weights).map(sigmoid(_))\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def sigmoid(inX: Double) = {\n    1.0 / (1 + scala.math.exp(-inX))\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = (x_test * weights).map(sigmoid(_)).map(x => if(x > 0.5) 1.0 else 0.0)\n    output\n  }\n\n}\n```\n\n\n\n##### 1.3 python ä»£ç å®ç°\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # ä¸ºXå¢åŠ ä¸€åˆ—ç‰¹å¾x1ï¼Œx1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # æ¢¯åº¦è®­ç»ƒn_iterationsè½®\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n\n\nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X = data.data[data.target != 0]\n    y = data.target[data.target != 0]\n    y[y == 1] = 0\n    y[y == 2] = 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    Plot().plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n```\n\nPythonç»“æœå±•ç¤ºå¦‚ä¸‹ã€åŸºäºPCAå°†é«˜ç»´æ•°æ®æŠ•å½±è€Œå¾—ã€‘ï¼š\n\n![](./é€»è¾‘å›å½’åˆ†ç±»å’Œsoftmaxåˆ†ç±»/logistic.png)\n\n#### 2.softmaxå›å½’\n\n##### 2.1 ç®—æ³•åŸç†å’Œæ­¥éª¤\n\nå¯¹é€»è¾‘å›å½’æ¨¡å‹åšæ¨å¹¿ï¼Œå¯ä»¥æ”¯æŒå¤šä¸ªç±»åˆ«äº†ã€‚\n\nåŸç†å¾ˆç®€å•ï¼Œå¯¹äºä¸€ä¸ªç»™å®šçš„å®ä¾‹$x$,Softmaxå›å½’æ¨¡å‹é¦–å…ˆè®¡ç®—å‡ºæ¯ä¸ªç±»åˆ«kçš„åˆ†ç±»$s_k(x)$ï¼Œç„¶åå¯¹è¿™äº›åˆ†æ•°åº”ç”¨softmaxå‡½æ•°(åˆå«åšå½’ä¸€åŒ–æŒ‡æ•°),ä¼°ç®—å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚\n\n1. ç”¨é›¶ï¼ˆæˆ–å°çš„éšæœºå€¼ï¼‰åˆå§‹åŒ–æƒé‡çŸ©é˜µå’Œåç½®å€¼.\n\n2. å¯¹äºæ¯ä¸ªç±» $k$ è®¡ç®—è¾“å…¥ç‰¹å¾å’Œç±» $k$ çš„æƒå‘é‡çš„çº¿æ€§ç»„åˆï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯¹äºæ¯ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œè®¡ç®—æ¯ä¸ªç±»çš„åˆ†æ•°ã€‚ å¯¹äºç±» $k$ å’Œè¾“å…¥å‘é‡ $x$ æœ‰:\n   $$s_k(x)= x\\cdot w_k $$\n\n   å‘é‡åŒ–è¡¨ç¤ºä¸Šå¼çš„è¯ï¼Œå¯ä»¥å†™ä¸º\n\n   $$ socres = X \\cdot W $$\n\n   $X$æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¾“å…¥æ ·æœ¬çš„å½¢çŠ¶ä¸º$(n_{samples},n_{features}  + 1)$çš„çŸ©é˜µ, $W$æ˜¯ä¸ªåŒ…å«æ¯ä¸€ä¸ªç±»çš„å½¢çŠ¶ä¸º$(n_{features}  + 1,n_{classes})$æƒé‡å‘é‡.\n\n3. åº”ç”¨softmaxæ¿€æ´»å‡½æ•°å°†åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡ã€‚ è¾“å…¥å‘é‡ $x$å±äºç±» $k$ çš„æ¦‚ç‡ç”±ä¸‹å¼ç»™å‡º:\n   $$\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(x))}$$\n\n4.  è®¡ç®—æ•´ä¸ªè®­ç»ƒé›†çš„æŸå¤±ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹ç›®æ ‡ç±»åˆ«çš„é«˜æ¦‚ç‡å’Œå…¶ä»–ç±»åˆ«çš„ä½æ¦‚ç‡ã€‚è¿™å¯ä»¥ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å®ç°:\n   $$J(W)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}log(\\hat{p}_k^{(i)})$$\n\n5. å¯¹äºç±»åˆ«kçš„äº¤å‰ç†µæ¢¯åº¦å‘é‡:\n   $$\\Delta_{w_k}J(W)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}$$\n\n6. æ›´æ–°æ¯ä¸ªç±»çš„æƒé‡$W$\n\n   $$w_k = w_k - \\eta \\Delta_{w_k}J$$\n\nâ€‹       äº¤å‰ç†µè¡¡é‡æ¯ä¸ªé¢„æµ‹æ¦‚ç‡åˆ†ç±»çš„å¹³å‡æ¯”ç‰¹æ•°ï¼Œå¦‚æœé¢„æµ‹å®Œç¾ï¼Œåˆ™ç»“æœç­‰äºæºæ•°æ®æœ¬èº«çš„ç†µ(ä¹Ÿå°±æ˜¯æœ¬èº«å›ºæœ‰çš„ä¸å¯é¢„æµ‹æ€§)ï¼Œä½†æ˜¯å¦‚æœé¢„æµ‹æœ‰è¯¯ï¼Œåˆ™äº¤å‰ç†µä¼šå˜å¤§ï¼Œå¢å¤§çš„éƒ¨åˆ†åˆç§°ä¸ºKLæ•£åº¦ã€‚ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒpå’Œqä¹‹é—´çš„äº¤å‰ç†µå¯ä»¥å®šä¹‰ä¸ºï¼š\n$$H(p,q)=-\\sum_xp(x)logq(x)$$\n\n\n\n##### 2.2 scala ä»£ç \n\n```scala\npackage ml.scrath.classification\n\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, _}\nimport breeze.numerics._\n\nobject softMax {\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map {\n        _.split(\",\").filter(_.length() > 0).map(_.toDouble)\n      }\n      .toArray\n    val data = BDM(dataS: _*)\n    val features = data(::, 0 to 3)\n    val labels = data(::, 4)\n\n    val soft = new SoftMaxRegression()\n    val w = soft.fit(features, labels)\n    println(w)\n    val predictions = soft.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"æ­£ç¡®ç‡ä¸ºï¼š\" + rate) // æ­£ç¡®ç‡ä¸º0.9664\n\n  }\n}\n\nclass SoftMaxRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y: BDV[Double]): BDM[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n\n    val ncol = x_train.cols\n    val nclasses = y.toArray.distinct.length\n    var weights = BDM.ones[Double](ncol, nclasses) :* 1.0 / nclasses\n    val n_samples = x_train.rows\n\n    for (iterations <- 0 to num_iters) {\n      val logits = x_train * weights\n      val probs = softmax(logits)\n      val y_one_hot = one_hot(y)\n//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble\n      val error: BDM[Double] = probs - y_one_hot\n      val gradients = (x_train.t * error) :/ n_samples.toDouble\n\n      weights -= gradients :* lr\n    }\n    weights\n  }\n\n  def softmax(logits: BDM[Double]): BDM[Double] = {\n    val scores = exp(logits)\n    val divisor = sum(scores(*, ::))\n    for (i <- 0 to scores.cols - 1) {\n      scores(::, i) := scores(::, i) :/ divisor\n    }\n    scores\n  }\n\n  def one_hot(y: BDV[Double]): BDM[Double] = {\n    val n_samples = y.length\n    val n_classes = y.toArray.toSet.size\n    val one_hot = Array.ofDim[Double](n_samples, n_classes)\n    for (i <- 0 to n_samples - 1) {\n      one_hot(i)(y(i).toInt) = 1.0\n    }\n    BDM(one_hot: _*)\n  }\n\n  def predict(weights: BDM[Double], x: BDM[Double]): BDV[Int] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_test = BDM.horzcat(ones, x)\n    val predictions = argmax(x_test * weights, Axis._1)\n    predictions\n  }\n\n}\n```\n\n\n\n##### 2.3 python ä»£ç \n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 12 11:58:06 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\nclass SoftmaxRegressorII:\n\n    def __init__(self,learning_rate = 0.1,n_iters = 1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n\n    def train(self, X, y_true, n_classes):\n\n        x_train = np.column_stack((np.ones(len(X)),X))\n        \n        self.n_samples, n_features = x_train.shape\n        self.n_classes = n_classes\n        \n        self.weights = np.random.rand(n_features,self.n_classes)\n        all_losses = []\n        \n        for i in range(self.n_iters):\n            logits = np.dot(x_train, self.weights)\n            probs = self.softmax(logits)\n            y_one_hot = self.one_hot(y_true)\n            loss = self.cross_entropy(y_one_hot, probs)\n            all_losses.append(loss)\n\n            gradients = (1 / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))\n\n            self.weights = self.weights - self.learning_rate * gradients\n\n#            if i % 100 == 0:\n#                print(f'Iteration number: {i}, loss: {np.round(loss, 4)}')\n\n        return self.weights, all_losses\n\n    def predict(self, X):\n\n        x_test = np.column_stack((np.ones(len(X)), X))\n        scores = np.dot(x_test, self.weights)\n        probs = self.softmax(scores)\n        return np.argmax(probs, axis=1)[:, np.newaxis]\n\n    def softmax(self, logits):\n        exp = np.exp(logits)\n        sum_exp = np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        return exp / sum_exp\n\n    def cross_entropy(self, y_true, scores):\n        loss = - (1 / self.n_samples) * np.sum(y_true * np.log(scores))\n        return loss\n\n    def one_hot(self, y):\n        one_hot = np.zeros((self.n_samples, self.n_classes))\n        one_hot[np.arange(self.n_samples), y.T] = 1\n        return one_hot\n    \nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X= data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = SoftmaxRegressorII()\n    ll = clf.train(X_train, y_train,3)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Reduce dimension to two using PCA and plot the results\n    Plot().plot_in_2d(X_test, y_pred, title=\"SoftMax Regression\", accuracy=accuracy)\n```\n\nç»“æœå¦‚å›¾æ‰€ç¤ºï¼š\n\n![](./é€»è¾‘å›å½’åˆ†ç±»å’Œsoftmaxåˆ†ç±»/softmax.png)"},{"title":"é™ç»´_çº¿æ€§åˆ¤åˆ«åˆ†æ","url":"/2020/01/16/é™ç»´_çº¿æ€§åˆ¤åˆ«åˆ†æ/","content":"#### 1. ç®—æ³•æ¦‚è¿°\n\nLDAçš„æ€æƒ³å¯ä»¥ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œå°±æ˜¯â€œæŠ•å½±åç±»å†…æ–¹å·®æœ€å°ï¼Œç±»é—´æ–¹å·®æœ€å¤§â€ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚æˆ‘ä»¬è¦å°†æ•°æ®åœ¨ä½ç»´åº¦ä¸Šè¿›è¡ŒæŠ•å½±ï¼ŒæŠ•å½±åå¸Œæœ›æ¯ä¸€ç§ç±»åˆ«æ•°æ®çš„æŠ•å½±ç‚¹å°½å¯èƒ½çš„æ¥è¿‘ï¼Œè€Œä¸åŒç±»åˆ«çš„æ•°æ®çš„ç±»åˆ«ä¸­å¿ƒä¹‹é—´çš„è·ç¦»å°½å¯èƒ½çš„å¤§ã€‚\n\n![](./é™ç»´_çº¿æ€§åˆ¤åˆ«åˆ†æ/lda_1.png)\n\n#### 2. ç®—æ³•æ¨å¯¼\n\nLDAå¤šåˆ†ç±»ï¼šå‡å®šå­˜åœ¨$N$ä¸ªç±»ï¼Œä¸”ç¬¬$i$ç±»ç¤ºä¾‹æ ‘ä¸º$m_i$,æˆ‘ä»¬å®šä¹‰å…¨å±€æ•£åº¦çŸ©é˜µï¼š\n$$\nS_t = S_b + S_w= \\sum_{i= 1}^{m}(x_i - \\mu)(x_i - \\mu)^T\n$$\nå…¶ä¸­$\\mu$æ˜¯æ‰€æœ‰ç¤ºä¾‹çš„å‡å€¼å‘é‡ã€‚å°†ç±»å†…æ•£åº¦çŸ©é˜µ$S_w$å®šä¹‰ä¸ºæ¯ä¸ªç±»åˆ«çš„æ•£åº¦çŸ©é˜µä¹‹å’Œï¼Œå³\n$$\nS_w = \\sum_{i=1}^{N}S_{w_i}\n$$\nå…¶ä¸­\n$$\nS_{w_i} = \\sum_{x \\in X_i}(x - \\mu_i)(x - \\mu_i)^T\n$$\nç”±å¼$(1)-(3)$å¯å¾—ï¼š\n$$\nS_b = S_t - S_w = \\sum_{i=1}^{N}(\\mu_i - \\mu)(\\mu_i - \\mu)^T\n$$\nå¤šåˆ†ç±»LDAæœ‰å¤šç§å®ç°æ–¹å¼ï¼šä½¿ç”¨$S_b,S_w,S_t$ä¸­çš„ä»»æ„ä¸¤å³å¯ã€‚å¸¸è§çš„ä¸€ç§å®ç°æ˜¯é‡‡ç”¨ä¼˜åŒ–ç›®æ ‡\n$$\n\\max_W \\dfrac{tr(W^TS_bW)}{tr(W^{T}S_wW)}\n$$\nå…¶ä¸­$W \\in R^{d\\times(N-1)}$,$tr(.)$è¡¨ç¤ºçŸ©é˜µçš„è¿¹ã€‚å¼$(5)$å¯ä»¥é€šè¿‡æ±‚è§£å¦‚ä¸‹å¼çš„å¹¿ä¹‰ç‰¹å¾é—®é¢˜\n$$\nS_bW = \\lambda S_wW\n$$\n$W$çš„é—­å¼è§£åˆ™æ˜¯$S_w^{-1}S_b$çš„N-1ä¸ªå¹¿ä¹‰ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µã€‚\n\nâ€‹\t\tè‹¥å°†$W$è§†ä¸ºä¸€ä¸ªæŠ•å½±çŸ©é˜µï¼Œåˆ™å¤šåˆ†ç±»LDAå°†æ ·æœ¬çŸ©é˜µæŠ•å½±åˆ°$N-1$ç»´ç©ºé—´ã€‚$N-1$é€šå¸¸åŸå°äºæ•°æ®åŸæ¥çš„ç»´æ•°ï¼Œä¸”æŠ•å½±è¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç±»åˆ«ä¿¡æ¯ï¼Œå› æ­¤LDAä¹Ÿè¢«è§†ä¸ºä¸€ç§æ•°æ®é™ç»´çš„æŠ€æœ¯ã€‚\n\n#### 3. å®ç°æ­¥éª¤\n\n1. å¯¹äºæ¯ä¸€ç±»åˆ«ï¼Œè®¡ç®—$d$ç»´æ•°æ®çš„å‡å€¼å‘é‡ï¼›\n2. æ„é€ ç±»é—´æ•£åº¦çŸ©é˜µ$S_b$å’Œç±»å†…æ•£åº¦çŸ©é˜µ$S_w$;\n3. è®¡ç®—çŸ©é˜µ$S_w^{-1}S_b$çš„ç‰¹å¾å€¼åŠå¯¹åº”çš„ç‰¹å¾å‘é‡ï¼›\n4. é€‰å–å‰$k$ç‰¹å¾å€¼æ‰€å¯¹åº”çš„ç‰¹å¾å‘é‡ï¼Œæ„é€ $d \\times k$ç»´çš„è½¬æ¢çŸ©é˜µ$W$,å…¶ä¸­ç‰¹å¾å€¼ä»¥åˆ—çš„å½¢å¼æ’åˆ—ï¼›\n5. ä½¿ç”¨è½¬æ¢çŸ©é˜µ$W$å°†æ ·æœ¬æ˜ å°„åˆ°æ–°çš„ç‰¹å¾å­ç©ºé—´ä¸Šã€‚\n\n#### 4. LDAä¸PCA\n\nLDAå’ŒPCAéƒ½å¯ä»¥ç”¨ä½œé™ç»´æŠ€æœ¯ï¼Œä¸‹é¢æ¯”è¾ƒä¸€ä¸‹ç›¸åŒç‚¹å’Œä¸åŒç‚¹ï¼š\n\n##### 4.1 ç›¸åŒç‚¹\n\nâ€‹\t1ï¼‰ä¸¤è€…å‡å¯ä»¥å¯¹æ•°æ®è¿›è¡Œé™ç»´;\n\nâ€‹\t2ï¼‰ä¸¤è€…åœ¨é™ç»´æ—¶å‡ä½¿ç”¨äº†çŸ©é˜µç‰¹å¾åˆ†è§£çš„æ€æƒ³;\n\nâ€‹\t3ï¼‰ä¸¤è€…éƒ½å‡è®¾æ•°æ®ç¬¦åˆé«˜æ–¯åˆ†å¸ƒ.\n\n##### 4.2 ä¸åŒç‚¹\n\nâ€‹\t1ï¼‰ LDAæ˜¯æœ‰ç›‘ç£çš„é™ç»´æŠ€æœ¯ï¼ŒPCAæ˜¯æ— ç›‘ç£çš„é™ç»´æŠ€æœ¯ï¼›\n\nâ€‹\t2ï¼‰ LDAè¿˜å¯ä»¥ç”¨äºåˆ†ç±»ï¼Œåç»­åœ¨åˆ†ç±»æ—¶ï¼Œè´´ä¸Šåˆ†ç±»çš„å¤„ç†æ–¹æ³•ï¼›\n\nâ€‹\t3ï¼‰ LDAæœ€å¤šå¯é™ä½åˆ°$k$ç»´ï¼ˆ$k$æ˜¯åˆ†ç±»çš„ä¸ªæ•°),è€ŒPCAæœ€å¤šå¯é™ä½åˆ°$n-1$ç»´ï¼ˆ$n$æ˜¯æ•°æ®çš„ç»´æ•°)ï¼›\n\nâ€‹\t4ï¼‰ LDAé€‰æ‹©åˆ†ç±»æ€§èƒ½æœ€å¥½çš„æŠ•å½±æ–¹å‘ï¼Œè€ŒPCAé€‰æ‹©æ ·æœ¬ç‚¹æŠ•å½±å…·æœ‰æœ€å¤§æ–¹å·®çš„æ–¹å‘ã€‚\n\n#### 5.  LDAçš„å®ç°ä»£ç \n\n##### 5.1 Scala ldaå®ç°\n\n```scala\npackage ml.scrath.lda\n\nimport breeze.linalg._\nimport breeze.stats._\nimport org.apache.spark.rdd.RDD\n\nclass LinearDiscriminantAnalysis extends Serializable {\n\n  def fit(data: RDD[DenseVector[Double]], labels: RDD[Int],k:Int) = {\n    val sample = labels.zip(data)\n    computeLDA(sample,k)\n  }\n\n  def computeLDA(dataAndLabels: RDD[(Int, DenseVector[Double])],k:Int)= {\n\n    val featuresByClass = dataAndLabels.groupBy(_._1).values.map(x => rowsToMatrix(x.map(_._2)))\n    val meanByClass = featuresByClass.map(f => mean(f(::, *))) // å¯¹è¡Œå‘é‡æ±‚å¹³å‡å€¼ each mean is a row vector, not col\n\n    //ç±»å†…æ•£åº¦çŸ©é˜µ\n    val Sw = featuresByClass.zip(meanByClass).map(f => {\n      val featuresMinusMean: DenseMatrix[Double] = f._1(*, ::) - f._2.t // row vector, not column\n      featuresMinusMean.t * featuresMinusMean: DenseMatrix[Double]\n    }).reduce(_+_)\n\n    val numByClass = featuresByClass.map(_.rows : Double)\n    val features = rddToMatrix(dataAndLabels.map(_._2))\n    val totalMean = mean(features(::, *)) // A row-vector, not a column-vector\n\n    val Sb = meanByClass.zip(numByClass).map {\n      case (classMean, classNum) => {\n        val m = classMean - totalMean\n        (m.t * m : DenseMatrix[Double]) :* classNum : DenseMatrix[Double]\n      }\n    }.reduce(_+_)\n\n    val eigen = eig((inv(Sw): DenseMatrix[Double]) * Sb)\n\n    val eigenvectors = (0 until eigen.eigenvectors.cols).map(eigen.eigenvectors(::, _).toDenseMatrix.t)\n\n    val topEigenvectors = eigenvectors.zip(eigen.eigenvalues.toArray).sortBy(x => -scala.math.abs(x._2)).map(_._1).take(k)\n    val W = DenseMatrix.horzcat(topEigenvectors:_*)\n    (W,Sb,Sw)\n  }\n\n  def rowsToMatrix(in: TraversableOnce[DenseVector[Double]]): DenseMatrix[Double] = {\n    rowsToMatrix(in.toArray)\n  }\n\n  def rowsToMatrix(inArr: Array[DenseVector[Double]]): DenseMatrix[Double] = {\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n\n\n  def rddToMatrix(inArr1: RDD[DenseVector[Double]]): DenseMatrix[Double] = {\n    val inArr = inArr1.collect()\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n}\n```\n\n##### 5.2 scala æµ‹è¯•ä»£ç \n\næµ‹è¯•ä»£ç ï¼ˆå…¶ä¸­æ•°æ®iris.csvæ˜¯ç”±ä¸‹é¢pythonä»£ç ç”Ÿæˆï¼‰\n\n```scala\npackage ml.scrath.lda\n\nimport org.apache.spark.sql.SparkSession\nimport breeze.linalg.DenseVector\n\nobject TestLDA extends App {\n\n  val spark =\n    SparkSession.builder()\n      .appName(\"DataFrame-Basic\")\n      .master(\"local[4]\")\n      .config(\"spark.sql.warehouse.dir\", \"file:///E:/spark-warehouse\")\n      .getOrCreate()\n  val sc = spark.sparkContext\n  val irisData = sc.textFile(\"D:\\\\data\\\\iris.csv\")\n\n  val trainData = irisData.map {\n    _.split(\",\").dropRight(1).map(_.toDouble)\n  }.map(new DenseVector(_))\n\n  val labels = irisData.map {\n    _.split(\",\").apply(4).map(_.toInt).apply(0)\n  }\n\n  val start = System.currentTimeMillis()\n  val model = new LinearDiscriminantAnalysis\n  val k = 2\n  val res = model.fit(trainData, labels, k)\n\n  println(\"=====W======\")\n  println(res._1)\n  println(\"=======Sb====\")\n  println(res._2)\n  println(\"=======Sw====\")\n  println(res._3)\n\n}\n```\n\n##### 5.3 scala ç»“æœ\n\nå±•ç¤ºè½¬æ¢çŸ©é˜µ$W$, ç±»é—´æ•£åº¦çŸ©é˜µ$S_b$å’Œç±»å†…æ•£åº¦çŸ©é˜µ$S_w$\n\n![](./é™ç»´_çº¿æ€§åˆ¤åˆ«åˆ†æ/scala_lda_res.png)\n\n##### 5.4 Python lda ä»£ç \n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef LDA(X, y, k):\n    '''\n    Xä¸ºæ•°æ®é›†ï¼Œyä¸ºlabelï¼Œkä¸ºç›®æ ‡ç»´æ•°\n    '''\n    label_ = np.unique(y)\n    mu = np.mean(X, axis=0)\n\n    Sw = np.zeros((len(mu), len(mu)))  # è®¡ç®—ç±»å†…æ•£åº¦çŸ©é˜µ\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sw += np.dot((_X - _mean).T,\n                     _X - _mean)\n\n    print(Sw)\n    \n    Sb = np.zeros((len(mu), len(mu)))  # è®¡ç®—ç±»å†…æ•£åº¦çŸ©é˜µ\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sb += len(_X) * np.dot(( _mean - mu).reshape(\n            (len(mu), 1)), (_mean - mu).reshape((1, len(mu))))\n        \n    print(Sb)\n\n    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # è®¡ç®—Sw-1*Sbçš„ç‰¹å¾å€¼å’Œç‰¹å¾çŸ©é˜µ\n\n    sorted_indices = np.argsort(eig_vals)\n    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]]  # æå–å‰kä¸ªç‰¹å¾å‘é‡\n    return topk_eig_vecs,Sb,Sw\n\n\nif '__main__' == __name__:\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n## å°†æ•°æ®å†™å‡ºåˆ°iris.csvæ–‡ä»¶ï¼Œä¾›scalaä½¿ç”¨ï¼Œä¿è¯æ•°æ®çš„ä¸€è‡´æ€§ã€‚\n    data_cat =  np.c_[X,y]\n    import pandas\n    iris_df = pandas.DataFrame(data_cat)\n    iris_df.to_csv(\"iris.csv\",index = 0,header = False)\n\n    W,Sb,Sw = LDA(X, y, 2)\n    X_new = np.dot((X), W)\n    plt.figure(1)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    \n    print(\"========W=========\")\n    print(W)\n    print(\"========Sb========\")\n    print(Sb)\n    print(\"========Sw========\")\n    print(Sw)\n    \n    \n    # ä¸sklearnä¸­çš„LDAå‡½æ•°å¯¹æ¯”\n    lda = LinearDiscriminantAnalysis(n_components=2)\n    lda.fit(X, y)\n    X_new = lda.transform(X)\n#    print(X_new)\n    plt.figure(2)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    plt.show()\n\n```\n\n##### 5.5 Python ç»“æœ\n\nå±•ç¤ºè½¬æ¢çŸ©é˜µ$W$, ç±»é—´æ•£åº¦çŸ©é˜µ$S_b$å’Œç±»å†…æ•£åº¦çŸ©é˜µ$S_w$,å¯è§scalaå’Œpythonå¾—åˆ°ç»“æœæ˜¯ä¸€è‡´çš„ã€‚\n\n![](./é™ç»´_çº¿æ€§åˆ¤åˆ«åˆ†æ/python_lda_res.png)\n\n"},{"title":"çº¿æ€§å›å½’","url":"/2020/01/09/çº¿æ€§å›å½’/","content":"### çº¿æ€§å›å½’çš„ä»£ç å®ç°\n\n#### 1. åŸç†\n\nå¤šå…ƒçº¿æ€§å›å½’çš„æŸå¤±å‡½æ•°ä¸ºï¼š\n$$\nJ=\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2\n$$\nå…¶ä¸­ï¼š$\\hat{y}^{(i)} = \\theta_{0} + \\theta_{1}X_{1}^{(i)} + \\theta_{2}X_{2}^{(i)} + ... + \\theta_{n}X_{n}^{(i)}$ ã€‚\n\nå¯¹ $J$ æ±‚å¯¼ä¸ºï¼š\n$$\n\\nabla J=(\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})\n$$\nå…¶ä¸­ï¼š$\\frac{\\partial J}{\\partial \\theta_i}$ ä¸ºåå¯¼æ•°ï¼Œä¸å¯¼æ•°çš„æ±‚æ³•ä¸€æ ·ã€‚\n\n\n\nå¯¹ $\\nabla J$ è¿›ä¸€æ­¥è®¡ç®—ï¼š\n$$\n\\nabla J(\\theta) =  \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} =   \\begin{pmatrix} \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)Â·(-1) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)Â·(-X_1^{(i)}) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)Â·(-X_2^{(i)}) \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)Â·(-X_n^{(i)}) \\end{pmatrix} = 2Â·\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_n^{(i)} \\end{pmatrix}\n$$\n\nå…¶ä¸­ï¼š$X_b = \\begin{pmatrix}\n1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n1 & X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)}\n\\end{pmatrix}$\n\n\n\nâ€‹        ç›¸åº”çš„å¯¹$J$ä¸Šå¯¹$Î¸$è¿™ä¸ªå‘é‡å»æ±‚æ¢¯åº¦å€¼ï¼Œä¹Ÿå°±æ˜¯æŸå¤±å‡½æ•°$J$å¯¹$Î¸$æ¯ä¸€ä¸ªç»´åº¦çš„æœªçŸ¥é‡å»æ±‚å¯¼ã€‚æ­¤æ—¶éœ€è¦æ³¨æ„ï¼Œæ±‚å¯¼è¿‡ç¨‹ä¸­ï¼Œ$Î¸$æ˜¯æœªçŸ¥æ•°ï¼Œç›¸åº”çš„$X$å’Œ$y$éƒ½æ˜¯å·²çŸ¥çš„ï¼Œéƒ½æ˜¯åœ¨ç›‘ç£å­¦ä¹ ä¸­è·å¾—çš„æ ·æœ¬ä¿¡æ¯ã€‚å¯¹äºæœ€å³è¾¹å¼å­çš„æ¯ä¸€é¡¹éƒ½æ˜¯mé¡¹çš„æ±‚å’Œï¼Œæ˜¾ç„¶æ¢¯åº¦çš„å¤§å°å’Œæ ·æœ¬æ•°é‡æœ‰å…³ï¼Œæ ·æœ¬æ•°é‡è¶Šå¤§ï¼Œæ±‚å‡ºæ¥çš„æ¢¯åº¦ä¸­ï¼Œæ¯ä¸€ä¸ªå…ƒç´ ç›¸åº”çš„ä¹Ÿå°±è¶Šå¤§ï¼Œè¿™ä¸ªå…¶å®æ˜¯ä¸åˆç†çš„ï¼Œæ±‚å‡ºæ¥çš„æ¢¯åº¦ä¸­æ¯ä¸€ä¸ªå…ƒç´ çš„å€¼åº”è¯¥å’Œ$m$æ ·æœ¬æ•°é‡æ˜¯æ— å…³çš„ï¼Œä¸ºæ­¤å°†æ•´ä¸ªæ¢¯åº¦å€¼å†é™¤ä¸Šä¸€ä¸ªmï¼Œç›¸åº”çš„ç›®æ ‡å‡½æ•°çš„å¼å­å˜æˆäº†ä¸‹é¢çš„å¼å­å³ï¼š\n$$\n\\nabla J(\\theta)  = \\frac{2}{m}\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})Â·X_n^{(i)} \\end{pmatrix}\n$$\nâ€‹        æ­¤æ—¶ï¼Œç›®æ ‡å‡½æ•°å°±æˆäº†ä½¿ $\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ å°½å¯èƒ½å°ï¼Œå³å‡æ–¹è¯¯å·®å°½å¯èƒ½å°ï¼š\n$$\nJ(\\theta) = MSE(y, \\hat{y})\n$$\nâ€‹\t\t\n\næ³¨1. æœ‰æ—¶å€™ç›®æ ‡å‡½æ•°ä¹Ÿå» $\\frac{1}{\\boldsymbol{2}m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ ,å…¶ä¸­çš„**2**æ˜¯ä¸ºäº†æŠµæ¶ˆæ¢¯åº¦ä¸­çš„2ï¼Œå®é™…çš„æ•ˆæœæœ‰é™ã€‚\n\næ³¨2. å°†æ¢¯åº¦é™¤ä»¥mç›¸å½“äºç›®æ ‡å‡½æ•°æœ¬èº«å˜æˆäº†MSEï¼Œä¹Ÿå°±æ˜¯å¯¹åŸæ¥çš„ç›®æ ‡å‡½æ•°é™¤ä¸Šmã€‚å¦‚æœæ²¡æœ‰1/mçš„è¯ï¼Œæ¢¯åº¦ä¸­çš„å…ƒç´ å°±ä¼šç‰¹åˆ«çš„å¤§ï¼Œåœ¨å…·ä½“ç¼–ç¨‹å®è·µä¸­å°±ä¼šå‡ºç°ä¸€äº›é—®é¢˜ã€‚å½“æˆ‘ä»¬åœ¨ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥æ±‚å‡½æ•°çš„æœ€å°å€¼çš„æ—¶å€™ï¼Œæœ‰æ—¶å€™éœ€è¦å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œä¸€äº›ç‰¹æ®Šçš„è®¾è®¡ï¼Œä¸è§å¾—æ‰€æœ‰çš„ç›®æ ‡å‡½æ•°éƒ½éå¸¸çš„åˆé€‚ï¼Œè™½ç„¶ç†è®ºä¸Šæ¢¯åº¦ä¸­æ¯ä¸€ä¸ªå…ƒç´ éƒ½éå¸¸å¤§çš„è¯ï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥é€šè¿‡è°ƒèŠ‚learning_rate(å­¦ä¹ ç‡)æ¥å¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼Œä½†æ˜¯é‚£æ ·çš„è¯å¯èƒ½ä¼šå½±å“æ•ˆç‡ã€‚\n\n\n\n#### 2.å®ç°ä»£ç \n\n##### 2.1 pythonç‰ˆæœ¬å®ç°\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  8 17:25:39 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\n\nclass LinearRegression():\n    def __init__(self,lr=.01, num_iters=10000,tolerance=1e-8):\n        self.lr = lr\n        self.num_iters = num_iters\n        self.w = None\n        self.toterance = tolerance\n\n    def fit(self,x_train, y_train):\n        n_samples = len(x_train)\n        x_train = np.column_stack((np.ones(n_samples), x_train)) #ä¹Ÿå¯ä»¥å†™æˆnp.c_\n        \n        self.w = .01 * np.ones(x_train.shape[1])\n        self.loss_ = [0]\n        \n        # w_{i} = w_{i} - lr * (h(w_{i}) - y)*x_{i} è¿­ä»£å…¬å¼\n        self.count = 0\n        for iteration in range(self.num_iters):\n            self.count += 1\n            raw_output = np.matmul(x_train, self.w)\n            errors =  raw_output - y_train\n            loss = 1/(2 * n_samples) * errors.dot(errors)\n            delta_loss = loss - self.loss_[-1]\n\n            self.loss_.append(loss)\n            if np.abs(delta_loss) < self.toterance:\n                break\n            else:\n                grad = (1.0 /n_samples) *np.matmul(x_train.T, np.array(errors))\n                self.w -= self.lr * grad\n\n    def predict(self,x_test):\n        x_test = np.column_stack((np.ones(len(x_test)), x_test))\n        \n        output = np.matmul(x_test, self.w)\n        return output\n    \nif __name__ == '__main__':\n    \n    num_inputs = 2\n    num_examples = 10000\n    true_w = [6.4, -3.2]\n    true_b = 2.3\n    features = np.random.random((num_examples, num_inputs))\n    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n    labels += np.random.normal(0, 0.1,size = len(labels))\n    \n    import sklearn.model_selection\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    print(\"å›å½’ç³»æ•°:\",lr.w)\n    print(\"è¿­ä»£æ¬¡æ•°:\",lr.count)\n    \n    y_pred = lr.predict(x_test)\n    from sklearn import metrics\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    print(\"MSE: %.4f\" % mse)\n\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    print(\"MAE: %.4f\" % mae)\n\n    R2 = metrics.r2_score(y_test,y_pred)\n    print(\"R2: %.4f\" % R2)\n```\n\n##### 2.2 Scalaç‰ˆæœ¬å®ç°\n\n```scala\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n * Scala ç‰ˆæœ¬çš„å®ç°\n */\n\nobject LinearRegression {\n  def main(args: Array[String]): Unit = {\n    val num_inputs = 2\n    val num_examples = 1000\n    val x_train: BDM[Double] = BDM.rand(num_examples, num_inputs)\n    val ones = BDM.ones[Double](num_examples, 1)\n    val x_cat = BDM.horzcat(ones, x_train)\n    val y_train = x_cat * BDV(2.3, 6.4, -3.2)\n\n    val model = new LinearRegression(num_iters = 10000)\n    val weights = model.fit(x_train, y_train)\n    val predictions = model.predict(weights, x_train)\n    println(\"æ¢¯åº¦ä¸‹é™æ±‚è§£çš„æƒé‡ä¸ºï¼š\" + weights)\n    println(predictions)\n  }\n}\n\nclass LinearRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // æ³¨æ„æ˜¯:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = x_train * weights\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = x_test * weights\n    output\n  }\n}\n```\n\nå‚è€ƒæ–‡çŒ®ï¼š1. [æœºå™¨å­¦ä¹ ä¹‹æ¢¯åº¦ä¸‹é™æ³•ä¸çº¿æ€§å›å½’](https://segmentfault.com/a/1190000017048213)\n\n\n\n"},{"title":"ç¥ç»ç½‘ç»œ","url":"/2019/11/27/ç¥ç»ç½‘ç»œ-1/","content":"\n### ç¥ç»ç½‘ç»œçš„numpyå®ç°å’Œå…¬å¼æ¨å¯¼\n\n\n\n\n\nâ€‹       è¿‡å»10å¤šå¹´æ˜¯ç¥ç»ç½‘ç»œå‘å±•çš„é»„é‡‘æ—¶æœŸï¼Œç¥ç»ç½‘ç»œ(æ·±åº¦å­¦ä¹ )æˆä¸ºäº†æ–°æ—¶ä»£çš„ä¸€ç§æµªæ½®ï¼Œæ‰€ä»¥ä»Šå¤©å€Ÿç”¨å›½å¤–ä¸€ä¸ªå°å“¥å®ç°çº¯numpyçš„ç¥ç»ç½‘ç»œï¼Œæ¥è®°å½•ç¥ç»ç½‘ç»œçš„å®ç°è¿‡ç¨‹ã€‚\n\n![](./ç¥ç»ç½‘ç»œ-1/nn_architecture.png)\n\n\n\n#### æ¦‚è§ˆ\n\nâ€‹        åœ¨å¼€å§‹ç¼–ç¨‹ä¹‹å‰ï¼Œå…ˆè®©æˆ‘ä»¬å‡†å¤‡ä¸€ä»½åŸºæœ¬çš„è·¯çº¿å›¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªç‰¹å®šæ¶æ„ï¼ˆå±‚æ•°ã€å±‚å¤§å°ã€æ¿€æ´»å‡½æ•°ï¼‰çš„å¯†é›†è¿æ¥ç¥ç»ç½‘ç»œã€‚ç„¶åè®­ç»ƒè¿™ä¸€ç¥ç»ç½‘ç»œå¹¶åšå‡ºé¢„æµ‹ã€‚\n\n![](./ç¥ç»ç½‘ç»œ-1/blueprint.gif)\n\nä¸Šé¢çš„ç¤ºæ„å›¾å±•ç¤ºäº†è®­ç»ƒç½‘ç»œã€ç‰¹åˆ«æ˜¯æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„æ“ä½œã€‘æ—¶è¿›è¡Œçš„æ“ä½œï¼Œä»¥åŠå•æ¬¡è¿­ä»£ä¸åŒé˜¶æ®µéœ€è¦æ›´æ–°å’Œè¯»å–çš„å‚æ•°ã€‚\n\n#### åˆå§‹åŒ–ç¥ç»ç½‘ç»œå±‚\n\n  è®©æˆ‘ä»¬ä»åˆå§‹åŒ–æ¯ä¸€å±‚çš„æƒé‡çŸ©é˜µ$W$å’Œåç½®å‘é‡$b$å¼€å§‹ã€‚ä¸‹å›¾å±•ç¤ºäº†ç½‘ç»œå±‚lçš„æƒé‡çŸ©é˜µå’Œåç½®å‘é‡ï¼Œå…¶ä¸­ï¼Œä¸Šæ ‡$[l]$è¡¨ç¤ºå½“å‰å±‚çš„ç´¢å¼•ï¼Œ$n$è¡¨ç¤ºç»™å®šå±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡ã€‚\n\n![](./ç¥ç»ç½‘ç»œ-1/params_sizes.png)\n\næˆ‘ä»¬çš„ç¨‹åºä¹Ÿå°†ä»¥ç±»ä¼¼çš„åˆ—è¡¨å½¢å¼æè¿°ç¥ç»ç½‘ç»œæ¶æ„ã€‚åˆ—è¡¨çš„æ¯ä¸€é¡¹æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæè¿°å•ä¸ªç½‘ç»œå±‚çš„åŸºæœ¬å‚æ•°ï¼š`input_dim`æ˜¯ç½‘ç»œå±‚è¾“å…¥çš„ä¿¡å·å‘é‡çš„å¤§å°ï¼Œ`output_dim`æ˜¯ç½‘ç»œå±‚è¾“å‡ºçš„æ¿€æ´»å‘é‡çš„å¤§å°ï¼Œ`activation`æ˜¯ç½‘ç»œå±‚æ‰€ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚\n\n```python\nnn_architecture = [\n    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n]\n```\n\nå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸€ä¸ªç½‘ç»œå±‚çš„è¾“å‡ºå‘é‡åŒæ—¶ä¹Ÿæ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ã€‚\n\n```python\ndef init_layers(nn_architecture, seed = 99):\n    np.random.seed(seed)\n    number_of_layers = len(nn_architecture)\n    params_values = {}\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        layer_input_size = layer[\"input_dim\"]\n        layer_output_size = layer[\"output_dim\"]\n\n        params_values['W' + str(layer_idx)] = np.random.randn(\n            layer_output_size, layer_input_size) * 0.1\n        params_values['b' + str(layer_idx)] = np.random.randn(\n            layer_output_size, 1) * 0.1\n\n    return params_values\n```\n\nâ€‹        ä¸Šé¢çš„ä»£ç åˆå§‹åŒ–äº†ç½‘ç»œå±‚çš„å‚æ•°ã€‚æ³¨æ„æˆ‘ä»¬ç”¨éšæœºçš„å°æ•°å­—å¡«å……çŸ©é˜µ**W**å’Œå‘é‡**b**ã€‚è¿™å¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚æƒé‡å€¼æ— æ³•ä½¿ç”¨ç›¸åŒçš„æ•°å­—åˆå§‹åŒ–ï¼Œå¦åˆ™ä¼šé€ æˆ**ç ´åæ€§çš„å¯¹ç§°é—®é¢˜**ã€‚**åŸºæœ¬ä¸Šï¼Œå¦‚æœæƒé‡éƒ½ä¸€æ ·ï¼Œä¸ç®¡è¾“å…¥Xæ˜¯ä»€ä¹ˆï¼Œéšè—å±‚çš„æ‰€æœ‰å•å…ƒä¹Ÿéƒ½ä¸€æ ·**ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±ä¼šé™·å…¥åˆå§‹çŠ¶æ€ï¼Œä¸ç®¡è®­ç»ƒå¤šä¹…ï¼Œç½‘ç»œå¤šæ·±ï¼Œéƒ½æ— æœ›æ‘†è„±ã€‚çº¿æ€§ä»£æ•°ä¸ä¼šåŸè°…æˆ‘ä»¬ã€‚\n\n[^åˆå§‹åŒ–æ–¹æ³•åŒ…å«å¾ˆå¤šç§ï¼Œæˆ‘ä»¬è¿™é‡Œç®€ä¾¿èµ·è§ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ–¹å¼ç”Ÿæˆæƒé‡]: \n\nâ€‹       å°æ•°å€¼å¢åŠ äº†ç®—æ³•çš„æ•ˆç‡ã€‚æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ä¸‹é¢çš„sigmoidå‡½æ•°å›¾åƒï¼Œå¤§æ•°å€¼å¤„çš„å‡½æ•°å›¾åƒå‡ ä¹æ˜¯æ‰å¹³çš„ï¼Œè¿™ä¼šå¯¹ç¥ç»ç½‘ç»œçš„å­¦ä¹ é€Ÿåº¦é€ æˆæ˜¾è‘—å½±å“ã€‚æ‰€æœ‰å‚æ•°ä½¿ç”¨å°éšæœºæ•°æ˜¯ä¸€ä¸ªç®€å•çš„æ–¹æ³•ï¼Œä½†å®ƒä¿è¯äº†ç®—æ³•æœ‰ä¸€ä¸ª**è¶³å¤Ÿå¥½**çš„å¼€å§‹ã€‚\n\n![](./ç¥ç»ç½‘ç»œ-1/activations.gif)\n\n#### æ¿€æ´»å‡½æ•°\n\nâ€‹        æ¿€æ´»å‡½æ•°åªéœ€ä¸€è¡Œä»£ç å°±å¯ä»¥å®šä¹‰ï¼Œä½†å®ƒä»¬ç»™ç¥ç»ç½‘ç»œå¸¦æ¥äº†éçº¿æ€§å’Œæ‰€éœ€çš„è¡¨è¾¾åŠ›ã€‚**â€œæ²¡æœ‰å®ƒä»¬ï¼Œç¥ç»ç½‘ç»œå°†å˜æˆçº¿æ€§å‡½æ•°çš„ç»„åˆï¼Œä¹Ÿå°±æ˜¯å•ä¸ªçº¿æ€§å‡½æ•°ã€‚â€**æ¿€æ´»å‡½æ•°æœ‰å¾ˆå¤šç§ï¼Œä½†åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å†³å®šä½¿ç”¨å…¶ä¸­ä¸¤ç§â€”â€”sigmoidå’ŒReLUã€‚ä¸ºäº†åŒæ—¶æ”¯æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å‡†å¤‡å¥½å®ƒä»¬çš„å¯¼æ•°ã€‚\n\n```python\ndef sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef relu(Z):\n    return np.maximum(0,Z)\n\ndef sigmoid_backward(dA, Z):\n    sig = sigmoid(Z)\n    return dA * sig * (1 - sig)\n\ndef relu_backward(dA, Z):\n    dZ = np.array(dA, copy = True)\n    dZ[Z <= 0] = 0;\n    return dZ;\n```\n\n#### å‰å‘ä¼ æ’­\n\nâ€‹    æˆ‘ä»¬è®¾è®¡çš„ç¥ç»ç½‘ç»œæœ‰ä¸€ä¸ªç®€å•çš„æ¶æ„ã€‚è¾“å…¥çŸ©é˜µ**X**ä¼ å…¥ç½‘ç»œï¼Œæ²¿ç€éšè—å•å…ƒä¼ æ’­ï¼Œæœ€ç»ˆå¾—åˆ°é¢„æµ‹å‘é‡**Y_hat**ã€‚ä¸ºäº†è®©ä»£ç æ›´æ˜“è¯»ï¼Œæˆ‘å°†å‰å‘ä¼ æ’­æ‹†åˆ†æˆä¸¤ä¸ªå‡½æ•°â€”â€”å•å±‚å‰å‘ä¼ æ’­ï¼Œå’Œæ•´ä¸ªç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­ã€‚\n\nâ€‹\t    **å‰å‘ä¼ æ’­çš„è¿‡ç¨‹æ˜¯ï¼šè¾“å…¥$a^{[l-1]}$, è¾“å‡º$a^{[l]}$, ç¼“å­˜ä¸º$z^{[l]}$,  ä»æ–¹ä¾¿å®ç°çš„è§’åº¦ä¸Šçœ‹ï¼Œ$z^{[l]}$æ˜¯$w^{[l]}$ï¼Œ$b^{[l]}$çš„å‡½æ•°ã€‚**\n\n```python\ndef single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n    Z_curr = np.dot(W_curr, A_prev) + b_curr\n    if activation is \"relu\":\n        activation_func = relu\n    elif activation is \"sigmoid\":\n        activation_func = sigmoid\n    else:\n        raise Exception('Non-supported activation function')\n\n    return activation_func(Z_curr), Z_curr\n```\nè¿™éƒ¨åˆ†ä»£ç å¤§æ¦‚æ˜¯æœ€ç›´æ¥ï¼Œæœ€å®¹æ˜“ç†è§£çš„ã€‚ç»™å®šæ¥è‡ªä¸Šä¸€å±‚çš„è¾“å…¥ä¿¡å·ï¼Œæˆ‘ä»¬è®¡ç®—ä»¿å°„å˜æ¢**Z**ï¼Œæ¥ç€åº”ç”¨é€‰ä¸­çš„æ¿€æ´»å‡½æ•°ã€‚åŸºäºNumPyï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ•´ä¸ªç½‘ç»œå±‚å’Œæ•´æ‰¹æ ·æœ¬ä¸€ä¸‹å­è¿›è¡ŒçŸ©é˜µæ“ä½œï¼Œæ— éœ€è¿­ä»£ï¼Œè¿™å¤§å¤§åŠ é€Ÿäº†è®¡ç®—ã€‚é™¤äº†è®¡ç®—ç»“æœå¤–ï¼Œå‡½æ•°è¿˜è¿”å›äº†ä¸€ä¸ªåå‘ä¼ æ’­æ—¶éœ€è¦ç”¨åˆ°çš„ä¸­é—´å€¼**Z**ã€‚\n\n![](./ç¥ç»ç½‘ç»œ-1/matrix_sizes_2.png)\n\nåŸºäºå•å±‚å‰å‘ä¼ æ’­å‡½æ•°ï¼Œç¼–å†™æ•´ä¸ªå‰å‘ä¼ æ’­æ­¥éª¤å¾ˆå®¹æ˜“ã€‚è¿™æ˜¯ä¸€ä¸ªç•¥å¾®å¤æ‚ä¸€ç‚¹çš„å‡½æ•°ï¼Œå®ƒçš„è§’è‰²ä¸ä»…æ˜¯è¿›è¡Œé¢„æµ‹ï¼Œè¿˜åŒ…æ‹¬ç»„ç»‡ä¸­é—´å€¼ã€‚\n$$\n\\begin{aligned}\nz^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}  \\\\\n\na^{[l]} = g^{[l]}(z^{[l]})\n\\end{aligned}\n$$\n\n\n```python\ndef full_forward_propagation(X, params_values, nn_architecture):\n    memory = {}\n    A_curr = X\n\n    for idx, layer in enumerate(nn_architecture):\n        layer_idx = idx + 1\n        A_prev = A_curr\n\n        activ_function_curr = layer[\"activation\"]\n        W_curr = params_values[\"W\" + str(layer_idx)]\n        b_curr = params_values[\"b\" + str(layer_idx)]\n        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\n        memory[\"A\" + str(idx)] = A_prev\n        memory[\"Z\" + str(layer_idx)] = Z_curr\n\n    return A_curr, memory\n```\n\n#### æŸå¤±å‡½æ•°\n\nâ€‹          æŸå¤±å‡½æ•°å¯ä»¥ç›‘æµ‹è¿›å±•ï¼Œç¡®ä¿æˆ‘ä»¬å‘ç€æ­£ç¡®çš„æ–¹å‘ç§»åŠ¨ã€‚**â€œä¸€èˆ¬æ¥è¯´ï¼ŒæŸå¤±å‡½æ•°æ˜¯ä¸ºäº†æ˜¾ç¤ºæˆ‘ä»¬ç¦»â€˜ç†æƒ³â€™è§£ç­”è¿˜æœ‰å¤šè¿œã€‚â€**æŸå¤±å‡½æ•°æ ¹æ®æˆ‘ä»¬è®¡åˆ’è§£å†³çš„é—®é¢˜è€Œé€‰ç”¨ï¼ŒKerasä¹‹ç±»çš„æ¡†æ¶æä¾›äº†å¾ˆå¤šé€‰é¡¹ã€‚å› ä¸ºæˆ‘è®¡åˆ’å°†ç¥ç»ç½‘ç»œç”¨äºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘å†³å®šä½¿ç”¨äº¤å‰ç†µï¼š\n$$\nJ(W,b) = \\dfrac{1}{m}{\\sum}_{i=1}^{m}L(\\hat{y}^{i} - y^{i})  \\\\\nL(\\hat{y} - y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))\n$$\nä¸ºäº†å–å¾—æ›´å¤šå…³äºå­¦ä¹ è¿‡ç¨‹çš„ä¿¡æ¯ï¼Œæˆ‘å†³å®šå¦å¤–å®ç°ä¸€ä¸ªè®¡ç®—ç²¾ç¡®åº¦çš„å‡½æ•°ã€‚\n\n```python\n'''\nJ(W,b) = 1/m sum_{i}^{m}L(y^{hat}_{i} - y_{i})\nL(y^{hat}_{i} - y_{i}) = -(ylogy^{hat} + (1-y)log(1-y^{hat}))\n'''\n\ndef get_cost_value(Y_hat, Y):\n    m = Y_hat.shape[1]\n    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n    return np.squeeze(cost)\n\n# an auxiliary function that converts probability into class\ndef convert_prob_into_class(probs):\n    probs_ = np.copy(probs)\n    probs_[probs_ > 0.5] = 1\n    probs_[probs_ <= 0.5] = 0\n    return probs_\n\ndef get_accuracy_value(Y_hat, Y):\n    Y_hat_ = convert_prob_into_class(Y_hat)\n    return (Y_hat_ == Y).all(axis=0).mean()\n```\n\n#### åå‘ä¼ æ’­\n\nä¸å¹¸çš„æ˜¯ï¼Œå¾ˆå¤šç¼ºä¹ç»éªŒçš„æ·±åº¦å­¦ä¹ çˆ±å¥½è€…éƒ½è§‰å¾—åå‘ä¼ æ’­å¾ˆå“äººï¼Œéš¾ä»¥ç†è§£ã€‚å¾®ç§¯åˆ†å’Œçº¿æ€§ä»£æ•°çš„ç»„åˆç»å¸¸ä¼šå“é€€é‚£äº›æ²¡æœ‰ç»è¿‡æ‰å®çš„æ•°å­¦è®­ç»ƒçš„äººã€‚æ‰€ä»¥ä¸è¦è¿‡äºæ‹…å¿ƒä½ ç°åœ¨è¿˜ä¸èƒ½ç†è§£è¿™ä¸€åˆ‡ã€‚ç›¸ä¿¡æˆ‘ï¼Œæˆ‘ä»¬éƒ½ç»å†è¿‡è¿™ä¸ªè¿‡ç¨‹ã€‚\n\n```python\ndef single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n    # number of examples\n    m = A_prev.shape[1]\n    \n    # selection of activation function\n    if activation is \"relu\":\n        backward_activation_func = relu_backward\n    elif activation is \"sigmoid\":\n        backward_activation_func = sigmoid_backward\n    else:\n        raise Exception('Non-supported activation function')\n    \n    # calculation of the activation function derivative\n    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n    \n    # derivative of the matrix W\n    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n    # derivative of the vector b\n    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n    # derivative of the matrix A_prev\n    dA_prev = np.dot(W_curr.T, dZ_curr)\n\n    return dA_prev, dW_curr, db_curr\n\n```\n\nâ€‹        äººä»¬ç»å¸¸ææ··åå‘ä¼ æ’­å’Œæ¢¯åº¦ä¸‹é™ï¼Œä½†äº‹å®ä¸Šå®ƒä»¬ä¸ä¸€æ ·ã€‚å‰è€…æ˜¯ä¸ºäº†é«˜æ•ˆåœ°è®¡ç®—æ¢¯åº¦ï¼Œåè€…åˆ™æ˜¯ä¸ºäº†åŸºäºè®¡ç®—å‡ºçš„æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ã€‚åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æŸå¤±å‡½æ•°åœ¨å‚æ•°ä¸Šçš„æ¢¯åº¦ï¼Œä½†åå‘ä¼ æ’­å¯ä»¥ç”¨æ¥è®¡ç®—ä»»ä½•å‡½æ•°çš„å¯¼æ•°ã€‚**åå‘ä¼ æ’­ç®—æ³•çš„ç²¾é«“åœ¨äºé€’å½’åœ°ä½¿ç”¨æ±‚å¯¼çš„é“¾å¼æ³•åˆ™ï¼Œé€šè¿‡ç»„åˆå¯¼æ•°å·²çŸ¥çš„å‡½æ•°ï¼Œè®¡ç®—å‡½æ•°çš„å¯¼æ•°**ã€‚ä¸‹é¢çš„å…¬å¼æè¿°äº†å•ä¸ªç½‘ç»œå±‚ä¸Šçš„åå‘ä¼ æ’­è¿‡ç¨‹ã€‚ç”±äºæœ¬æ–‡çš„é‡ç‚¹åœ¨å®é™…å®ç°ï¼Œæ‰€ä»¥æˆ‘å°†çœç•¥æ±‚å¯¼è¿‡ç¨‹ã€‚ä»å…¬å¼ä¸Šæˆ‘ä»¬å¯ä»¥å¾ˆæ˜æ˜¾åœ°çœ‹åˆ°ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨å‰å‘ä¼ æ’­æ—¶è®°ä½ä¸­é—´å±‚çš„**A**ã€**Z**çŸ©é˜µçš„å€¼ã€‚\n\n$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} =\\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}} \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{W}^{[l]}} =  \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n\n$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}}  = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}   \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{b}^{[l]}}= \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n\n$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} =  \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}  \\frac{\\partial \\boldsymbol{Z}^{[l]} }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n\n$$\\boldsymbol{dZ}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{Z}^{[l]}}= \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l]}} \\frac{\\partial {A}^{[l]} }{\\partial \\boldsymbol{Z}^{[l]}}=\\boldsymbol{dA}^{[l]} * g^{[l]}{'}(\\boldsymbol{Z}^{[l]})$$\n\n![](./ç¥ç»ç½‘ç»œ-1/640.webp)\n\nå’Œå‰å‘ä¼ æ’­ä¸€æ ·ï¼Œæˆ‘å†³å®šå°†è®¡ç®—æ‹†åˆ†æˆä¸¤ä¸ªå‡½æ•°ã€‚ä¹‹å‰ç»™å‡ºçš„æ˜¯å•ä¸ªç½‘ç»œå±‚çš„åå‘ä¼ æ’­å‡½æ•°ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯ä»¥NumPyæ–¹å¼é‡å†™ä¸Šé¢çš„æ•°å­¦å…¬å¼ã€‚è€Œå®šä¹‰å®Œæ•´åå‘ä¼ æ’­è¿‡ç¨‹çš„å‡½æ•°ï¼Œä¸»è¦æ˜¯è¯»å–ã€æ›´æ–°ä¸‰ä¸ªå­—å…¸ä¸­çš„å€¼ã€‚\n\n```python\ndef full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n    grads_values = {}\n    \n    # number of examples\n    m = Y.shape[1]\n    # a hack ensuring the same shape of the prediction vector and labels vector\n    Y = Y.reshape(Y_hat.shape)\n    \n    # initiation of gradient descent algorithm\n    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n    \n    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n        # we number network layers from 1\n        layer_idx_curr = layer_idx_prev + 1\n        # extraction of the activation function for the current layer\n        activ_function_curr = layer[\"activation\"]\n        \n        dA_curr = dA_prev\n        \n        A_prev = memory[\"A\" + str(layer_idx_prev)]\n        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n        \n        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n        \n        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n        \n        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n    \n    return grads_values\n```\n\nåŸºäºå•ä¸ªç½‘ç»œå±‚çš„åå‘ä¼ æ’­å‡½æ•°ï¼Œæˆ‘ä»¬ä»æœ€åä¸€å±‚å¼€å§‹è¿­ä»£è®¡ç®—æ‰€æœ‰å‚æ•°ä¸Šçš„å¯¼æ•°ï¼Œå¹¶æœ€ç»ˆè¿”å›åŒ…å«æ‰€éœ€æ¢¯åº¦çš„pythonå­—å…¸ã€‚\n\n\n\n#### æ›´æ–°å‚æ•°å€¼\n\nåå‘ä¼ æ’­æ˜¯ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼Œä»¥æ ¹æ®æ¢¯åº¦è¿›è¡Œä¼˜åŒ–ï¼Œæ›´æ–°ç½‘ç»œçš„å‚æ•°å€¼ã€‚ä¸ºäº†å®Œæˆè¿™ä¸€ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªå­—å…¸ä½œä¸ºå‡½æ•°å‚æ•°ï¼š`params_values`ï¼Œå…¶ä¸­ä¿å­˜äº†å½“å‰å‚æ•°å€¼ï¼›`grads_values`ï¼Œå…¶ä¸­ä¿å­˜äº†ç”¨äºæ›´æ–°å‚æ•°å€¼æ‰€éœ€çš„æ¢¯åº¦ä¿¡æ¯ã€‚ç°åœ¨æˆ‘ä»¬åªéœ€åœ¨æ¯ä¸ªç½‘ç»œå±‚ä¸Šåº”ç”¨ä»¥ä¸‹ç­‰å¼å³å¯ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ä¼˜åŒ–ç®—æ³•ï¼Œä½†æˆ‘å†³å®šä½¿ç”¨å®ƒä½œä¸ºæ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³•çš„èµ·ç‚¹ï¼ˆå¤§æ¦‚ä¼šæ˜¯æˆ‘ä¸‹ä¸€ç¯‡æ–‡ç« çš„ä¸»é¢˜ï¼‰ã€‚\n\n$$\\boldsymbol{W}^{[l]} = \\boldsymbol{W}^{[l]} - \\alpha \\boldsymbol{dW}^{[l]} $$\n\n$$\\boldsymbol{b}^{[l]} = \\boldsymbol{b}^{[l]} - \\alpha \\boldsymbol{b}^{[l]} $$\n\n```\ndef update(params_values, grads_values, nn_architecture, learning_rate):\n\n    # iteration over network layers\n    for layer_idx, layer in enumerate(nn_architecture, 1):\n        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n\n    return params_values;\n```\n\n#### æ•´åˆä¸€åˆ‡\n\nä¸‡äº‹ä¿±å¤‡åªæ¬ ä¸œé£ã€‚æœ€å›°éš¾çš„éƒ¨åˆ†å·²ç»å®Œæˆäº†â€”â€”æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ‰€éœ€çš„å‡½æ•°ï¼Œç°åœ¨åªéœ€ä»¥æ­£ç¡®çš„é¡ºåºæŠŠå®ƒä»¬æ”¾åˆ°ä¸€èµ·ã€‚\n\n```python\ndef train(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None):\n    # initiation of neural net parameters\n    params_values = init_layers(nn_architecture, 2)\n    # initiation of lists storing the history \n    # of metrics calculated during the learning process \n    cost_history = []\n    accuracy_history = []\n    \n    # performing calculations for subsequent iterations\n    for i in range(epochs):\n        # step forward\n        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n        \n        # calculating metrics and saving them in history\n        cost = get_cost_value(Y_hat, Y)\n        cost_history.append(cost)\n        accuracy = get_accuracy_value(Y_hat, Y)\n        accuracy_history.append(accuracy)\n        \n        # step backward - calculating gradient\n        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n        # updating model state\n        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n        \n        if(i % 50 == 0):\n            if(verbose):\n                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n            if(callback is not None):\n                callback(i, params_values)\n            \n    return params_values\n```\n\n\n"}]