[{"title":"逻辑回归分类和softmax分类","url":"/2020/02/12/逻辑回归分类和softmax分类/","content":"### 逻辑回归分类和softmax分类\n\n#### 1.逻辑回归\n\n##### 1.1 算法原理\n\n一些回归算法也可以用于分类，反之亦然。逻辑回归就是被广泛用于估算一个实例属于某个特定类别的概率。如果估算概率超过50%就是属于该类，反之则不是。\n\n逻辑回归模型概率估算:\n$$\\hat{p}=h_\\theta(x)=\\sigma(\\theta^T\\cdot x)$$\n\n逻辑函数：\n$$\\sigma(t)=\\frac{1}{1+exp(-t)}$$\n\n预测模型：\n$$\\hat{y}=\n\\begin{cases}\n0 & (\\hat{p}<0.5)\\\\\n1 & (\\hat{p}\\geq0.5)\n\\end{cases}$$\n\n单个训练实例的损失函数:\n$$c(\\theta)=\n\\begin{cases}\n-log(\\hat{p}) & (y=1)\\\\\n-log(1-\\hat{p}) & (y=0)\n\\end{cases}$$\n\n我们可以看到，当$p$接近于$0$的时候，$-\\log(p)$会变得非常大，所以如果模型估算一个正实例的概率接近于$0$，那么损失函数就会非常高，反过来，当$p$接近于$1$的时候，$-\\log(p)$接近于$0$，所以对一个负类实例估算出的概率接近于$0$，损失函数也会很低。\n\n逻辑回归成本函数:\n$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}log(\\hat{p}^{(i)})+(1-y^{(i)})log(1-\\hat{p}^{(i)})]$$\n\n坏消息是，这个函数没有已知的闭式方程(也就是不尊在一个标准方差的等价方程)。好消息，这是个凸函数，通过梯度下降算法保证能够找出全局最小值。\n\nLogistic损失函数的偏导数:\n$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}(\\sigma(\\theta^T \\cdot x^{(i)})-y^{(i)})x_j^{(i)}$$\n\n$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}(\\theta^T\\cdot x^{i}-y^{i})x_j^{i}$$\n\n##### 1.2 scala 代码实现\n\n```scala\npackage ml.scrath.classification\n\nimport scala.collection.mutable.ArrayBuffer\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n\n\nobject LogitRegression{\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map{_.split(\",\").filter(_.length() > 0).map(_.toDouble)}\n      .toArray\n    val data = BDM(dataS:_*)\n\n    val features = data(0 to 98, 0 to 3)\n    val labels = data(0 to 98, 4)\n\n    val model = new LogitRegression\n    val w = model.fit(features,labels)\n    val predictions = model.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate)\n  }\n}\n\n\nclass LogitRegression (var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = (x_train * weights).map(sigmoid(_))\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def sigmoid(inX: Double) = {\n    1.0 / (1 + scala.math.exp(-inX))\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = (x_test * weights).map(sigmoid(_)).map(x => if(x > 0.5) 1.0 else 0.0)\n    output\n  }\n\n}\n```\n\n\n\n##### 1.3 python 代码实现\n\n```python\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\nclass LogisticRegression():\n    def __init__(self, learning_rate=.1, n_iterations=4000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n\n    def initialize_weights(self, n_features):\n        limit = np.sqrt(1 / n_features)\n        w = np.random.uniform(-limit, limit, (n_features, 1))\n        b = 0\n        self.w = np.insert(w, 0, b, axis=0)\n\n    def fit(self, X, y):\n        m_samples, n_features = X.shape\n        self.initialize_weights(n_features)\n        # 为X增加一列特征x1，x1 = 0\n        X = np.insert(X, 0, 1, axis=1)\n        y = np.reshape(y, (m_samples, 1))\n\n        # 梯度训练n_iterations轮\n        for i in range(self.n_iterations):\n            h_x = X.dot(self.w)\n            y_pred = sigmoid(h_x)\n            w_grad = X.T.dot(y_pred - y)\n            self.w = self.w - self.learning_rate * w_grad\n\n    def predict(self, X):\n        X = np.insert(X, 0, 1, axis=1)\n        h_x = X.dot(self.w)\n        y_pred = np.round(sigmoid(h_x))\n        return y_pred.astype(int)\n\n\nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X = data.data[data.target != 0]\n    y = data.target[data.target != 0]\n    y[y == 1] = 0\n    y[y == 2] = 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = LogisticRegression()\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    Plot().plot_in_2d(X_test, y_pred, title=\"Logistic Regression\", accuracy=accuracy)\n```\n\nPython结果展示如下【基于PCA将高维数据投影而得】：\n\n![](./逻辑回归分类和softmax分类/logistic.png)\n\n#### 2.softmax回归\n\n##### 2.1 算法原理和步骤\n\n对逻辑回归模型做推广，可以支持多个类别了。\n\n原理很简单，对于一个给定的实例$x$,Softmax回归模型首先计算出每个类别k的分类$s_k(x)$，然后对这些分数应用softmax函数(又叫做归一化指数),估算出每个类别的概率。\n\n1. 用零（或小的随机值）初始化权重矩阵和偏置值.\n\n2. 对于每个类 $k$ 计算输入特征和类 $k$ 的权向量的线性组合，也就是说，对于每个训练样本，计算每个类的分数。 对于类 $k$ 和输入向量 $x$ 有:\n   $$s_k(x)= x\\cdot w_k $$\n\n   向量化表示上式的话，可以写为\n\n   $$ socres = X \\cdot W $$\n\n   $X$是一个包含所有输入样本的形状为$(n_{samples},n_{features}  + 1)$的矩阵, $W$是个包含每一个类的形状为$(n_{features}  + 1,n_{classes})$权重向量.\n\n3. 应用softmax激活函数将分数转换为概率。 输入向量 $x$属于类 $k$ 的概率由下式给出:\n   $$\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\sum_{j=1}^{K}exp(s_j(x))}$$\n\n4.  计算整个训练集的损失。我们希望我们的模型能够预测目标类别的高概率和其他类别的低概率。这可以使用交叉熵损失函数来实现:\n   $$J(W)=-\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_k^{(i)}log(\\hat{p}_k^{(i)})$$\n\n5. 对于类别k的交叉熵梯度向量:\n   $$\\Delta_{w_k}J(W)=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{p}_k^{(i)}-y_k^{(i)})x^{(i)}$$\n\n6. 更新每个类的权重$W$\n\n   $$w_k = w_k - \\eta \\Delta_{w_k}J$$\n\n​       交叉熵衡量每个预测概率分类的平均比特数，如果预测完美，则结果等于源数据本身的熵(也就是本身固有的不可预测性)，但是如果预测有误，则交叉熵会变大，增大的部分又称为KL散度。两个概率分布p和q之间的交叉熵可以定义为：\n$$H(p,q)=-\\sum_xp(x)logq(x)$$\n\n\n\n##### 2.2 scala 代码\n\n```scala\npackage ml.scrath.classification\n\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, _}\nimport breeze.numerics._\n\nobject softMax {\n  def main(args: Array[String]): Unit = {\n    val dataS = scala.io.Source.fromFile(\"D:/data/iris.csv\").getLines().toSeq.tail\n      .map {\n        _.split(\",\").filter(_.length() > 0).map(_.toDouble)\n      }\n      .toArray\n    val data = BDM(dataS: _*)\n    val features = data(::, 0 to 3)\n    val labels = data(::, 4)\n\n    val soft = new SoftMaxRegression()\n    val w = soft.fit(features, labels)\n    println(w)\n    val predictions = soft.predict(w, features)\n    val predictionsNlabels = predictions.toArray.zip(labels.toArray)\n    val rate = predictionsNlabels.filter(f => f._1==f._2).length.toDouble/predictionsNlabels.length.toDouble\n    println(\"正确率为：\" + rate) // 正确率为0.9664\n\n  }\n}\n\nclass SoftMaxRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y: BDV[Double]): BDM[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n\n    val ncol = x_train.cols\n    val nclasses = y.toArray.distinct.length\n    var weights = BDM.ones[Double](ncol, nclasses) :* 1.0 / nclasses\n    val n_samples = x_train.rows\n\n    for (iterations <- 0 to num_iters) {\n      val logits = x_train * weights\n      val probs = softmax(logits)\n      val y_one_hot = one_hot(y)\n//      val loss = sum(y_one_hot :* log(probs)) /n_samples.toDouble\n      val error: BDM[Double] = probs - y_one_hot\n      val gradients = (x_train.t * error) :/ n_samples.toDouble\n\n      weights -= gradients :* lr\n    }\n    weights\n  }\n\n  def softmax(logits: BDM[Double]): BDM[Double] = {\n    val scores = exp(logits)\n    val divisor = sum(scores(*, ::))\n    for (i <- 0 to scores.cols - 1) {\n      scores(::, i) := scores(::, i) :/ divisor\n    }\n    scores\n  }\n\n  def one_hot(y: BDV[Double]): BDM[Double] = {\n    val n_samples = y.length\n    val n_classes = y.toArray.toSet.size\n    val one_hot = Array.ofDim[Double](n_samples, n_classes)\n    for (i <- 0 to n_samples - 1) {\n      one_hot(i)(y(i).toInt) = 1.0\n    }\n    BDM(one_hot: _*)\n  }\n\n  def predict(weights: BDM[Double], x: BDM[Double]): BDV[Int] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_test = BDM.horzcat(ones, x)\n    val predictions = argmax(x_test * weights, Axis._1)\n    predictions\n  }\n\n}\n```\n\n\n\n##### 2.3 python 代码\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 12 11:58:06 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\nfrom sklearn import datasets\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom utils import train_test_split, accuracy_score\nfrom utils import Plot\n\nclass SoftmaxRegressorII:\n\n    def __init__(self,learning_rate = 0.1,n_iters = 1000):\n        self.learning_rate = learning_rate\n        self.n_iters = n_iters\n\n    def train(self, X, y_true, n_classes):\n\n        x_train = np.column_stack((np.ones(len(X)),X))\n        \n        self.n_samples, n_features = x_train.shape\n        self.n_classes = n_classes\n        \n        self.weights = np.random.rand(n_features,self.n_classes)\n        all_losses = []\n        \n        for i in range(self.n_iters):\n            logits = np.dot(x_train, self.weights)\n            probs = self.softmax(logits)\n            y_one_hot = self.one_hot(y_true)\n            loss = self.cross_entropy(y_one_hot, probs)\n            all_losses.append(loss)\n\n            gradients = (1 / self.n_samples) * np.dot(x_train.T, (probs - y_one_hot))\n\n            self.weights = self.weights - self.learning_rate * gradients\n\n#            if i % 100 == 0:\n#                print(f'Iteration number: {i}, loss: {np.round(loss, 4)}')\n\n        return self.weights, all_losses\n\n    def predict(self, X):\n\n        x_test = np.column_stack((np.ones(len(X)), X))\n        scores = np.dot(x_test, self.weights)\n        probs = self.softmax(scores)\n        return np.argmax(probs, axis=1)[:, np.newaxis]\n\n    def softmax(self, logits):\n        exp = np.exp(logits)\n        sum_exp = np.sum(np.exp(logits), axis=1, keepdims=True)\n        \n        return exp / sum_exp\n\n    def cross_entropy(self, y_true, scores):\n        loss = - (1 / self.n_samples) * np.sum(y_true * np.log(scores))\n        return loss\n\n    def one_hot(self, y):\n        one_hot = np.zeros((self.n_samples, self.n_classes))\n        one_hot[np.arange(self.n_samples), y.T] = 1\n        return one_hot\n    \nif __name__ == \"__main__\":\n    data = datasets.load_iris()\n    X= data.data\n    y = data.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, seed=1)\n\n    clf = SoftmaxRegressorII()\n    ll = clf.train(X_train, y_train,3)\n    y_pred = clf.predict(X_test)\n    y_pred = np.reshape(y_pred, y_test.shape)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n\n    # Reduce dimension to two using PCA and plot the results\n    Plot().plot_in_2d(X_test, y_pred, title=\"SoftMax Regression\", accuracy=accuracy)\n```\n\n结果如图所示：\n\n![](./逻辑回归分类和softmax分类/softmax.png)"},{"title":"降维_线性判别分析","url":"/2020/01/16/降维_线性判别分析/","content":"#### 1. 算法概述\n\nLDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”，如下图所示。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。\n\n![](./降维_线性判别分析/lda_1.png)\n\n#### 2. 算法推导\n\nLDA多分类：假定存在$N$个类，且第$i$类示例树为$m_i$,我们定义全局散度矩阵：\n$$\nS_t = S_b + S_w= \\sum_{i= 1}^{m}(x_i - \\mu)(x_i - \\mu)^T\n$$\n其中$\\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即\n$$\nS_w = \\sum_{i=1}^{N}S_{w_i}\n$$\n其中\n$$\nS_{w_i} = \\sum_{x \\in X_i}(x - \\mu_i)(x - \\mu_i)^T\n$$\n由式$(1)-(3)$可得：\n$$\nS_b = S_t - S_w = \\sum_{i=1}^{N}(\\mu_i - \\mu)(\\mu_i - \\mu)^T\n$$\n多分类LDA有多种实现方式：使用$S_b,S_w,S_t$中的任意两即可。常见的一种实现是采用优化目标\n$$\n\\max_W \\dfrac{tr(W^TS_bW)}{tr(W^{T}S_wW)}\n$$\n其中$W \\in R^{d\\times(N-1)}$,$tr(.)$表示矩阵的迹。式$(5)$可以通过求解如下式的广义特征问题\n$$\nS_bW = \\lambda S_wW\n$$\n$W$的闭式解则是$S_w^{-1}S_b$的N-1个广义特征值所对应的特征向量组成的矩阵。\n\n​\t\t若将$W$视为一个投影矩阵，则多分类LDA将样本矩阵投影到$N-1$维空间。$N-1$通常原小于数据原来的维数，且投影过程中使用了类别信息，因此LDA也被视为一种数据降维的技术。\n\n#### 3. 实现步骤\n\n1. 对于每一类别，计算$d$维数据的均值向量；\n2. 构造类间散度矩阵$S_b$和类内散度矩阵$S_w$;\n3. 计算矩阵$S_w^{-1}S_b$的特征值及对应的特征向量；\n4. 选取前$k$特征值所对应的特征向量，构造$d \\times k$维的转换矩阵$W$,其中特征值以列的形式排列；\n5. 使用转换矩阵$W$将样本映射到新的特征子空间上。\n\n#### 4. LDA与PCA\n\nLDA和PCA都可以用作降维技术，下面比较一下相同点和不同点：\n\n##### 4.1 相同点\n\n​\t1）两者均可以对数据进行降维;\n\n​\t2）两者在降维时均使用了矩阵特征分解的思想;\n\n​\t3）两者都假设数据符合高斯分布.\n\n##### 4.2 不同点\n\n​\t1） LDA是有监督的降维技术，PCA是无监督的降维技术；\n\n​\t2） LDA还可以用于分类，后续在分类时，贴上分类的处理方法；\n\n​\t3） LDA最多可降低到$k$维（$k$是分类的个数),而PCA最多可降低到$n-1$维（$n$是数据的维数)；\n\n​\t4） LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。\n\n#### 5.  LDA的实现代码\n\n##### 5.1 Scala lda实现\n\n```scala\npackage ml.scrath.lda\n\nimport breeze.linalg._\nimport breeze.stats._\nimport org.apache.spark.rdd.RDD\n\nclass LinearDiscriminantAnalysis extends Serializable {\n\n  def fit(data: RDD[DenseVector[Double]], labels: RDD[Int],k:Int) = {\n    val sample = labels.zip(data)\n    computeLDA(sample,k)\n  }\n\n  def computeLDA(dataAndLabels: RDD[(Int, DenseVector[Double])],k:Int)= {\n\n    val featuresByClass = dataAndLabels.groupBy(_._1).values.map(x => rowsToMatrix(x.map(_._2)))\n    val meanByClass = featuresByClass.map(f => mean(f(::, *))) // 对行向量求平均值 each mean is a row vector, not col\n\n    //类内散度矩阵\n    val Sw = featuresByClass.zip(meanByClass).map(f => {\n      val featuresMinusMean: DenseMatrix[Double] = f._1(*, ::) - f._2.t // row vector, not column\n      featuresMinusMean.t * featuresMinusMean: DenseMatrix[Double]\n    }).reduce(_+_)\n\n    val numByClass = featuresByClass.map(_.rows : Double)\n    val features = rddToMatrix(dataAndLabels.map(_._2))\n    val totalMean = mean(features(::, *)) // A row-vector, not a column-vector\n\n    val Sb = meanByClass.zip(numByClass).map {\n      case (classMean, classNum) => {\n        val m = classMean - totalMean\n        (m.t * m : DenseMatrix[Double]) :* classNum : DenseMatrix[Double]\n      }\n    }.reduce(_+_)\n\n    val eigen = eig((inv(Sw): DenseMatrix[Double]) * Sb)\n\n    val eigenvectors = (0 until eigen.eigenvectors.cols).map(eigen.eigenvectors(::, _).toDenseMatrix.t)\n\n    val topEigenvectors = eigenvectors.zip(eigen.eigenvalues.toArray).sortBy(x => -scala.math.abs(x._2)).map(_._1).take(k)\n    val W = DenseMatrix.horzcat(topEigenvectors:_*)\n    (W,Sb,Sw)\n  }\n\n  def rowsToMatrix(in: TraversableOnce[DenseVector[Double]]): DenseMatrix[Double] = {\n    rowsToMatrix(in.toArray)\n  }\n\n  def rowsToMatrix(inArr: Array[DenseVector[Double]]): DenseMatrix[Double] = {\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n\n\n  def rddToMatrix(inArr1: RDD[DenseVector[Double]]): DenseMatrix[Double] = {\n    val inArr = inArr1.collect()\n    val nRows = inArr.length\n    val nCols = inArr(0).length\n    val outArr = new Array[Double](nRows * nCols)\n    var i = 0\n    while (i < nRows) {\n      var j = 0\n      val row = inArr(i)\n      while (j < nCols) {\n        outArr(i + nRows * j) = row(j)\n        j = j + 1\n      }\n      i = i + 1\n    }\n    val outMat = new DenseMatrix[Double](nRows, nCols, outArr)\n    outMat\n  }\n}\n```\n\n##### 5.2 scala 测试代码\n\n测试代码（其中数据iris.csv是由下面python代码生成）\n\n```scala\npackage ml.scrath.lda\n\nimport org.apache.spark.sql.SparkSession\nimport breeze.linalg.DenseVector\n\nobject TestLDA extends App {\n\n  val spark =\n    SparkSession.builder()\n      .appName(\"DataFrame-Basic\")\n      .master(\"local[4]\")\n      .config(\"spark.sql.warehouse.dir\", \"file:///E:/spark-warehouse\")\n      .getOrCreate()\n  val sc = spark.sparkContext\n  val irisData = sc.textFile(\"D:\\\\data\\\\iris.csv\")\n\n  val trainData = irisData.map {\n    _.split(\",\").dropRight(1).map(_.toDouble)\n  }.map(new DenseVector(_))\n\n  val labels = irisData.map {\n    _.split(\",\").apply(4).map(_.toInt).apply(0)\n  }\n\n  val start = System.currentTimeMillis()\n  val model = new LinearDiscriminantAnalysis\n  val k = 2\n  val res = model.fit(trainData, labels, k)\n\n  println(\"=====W======\")\n  println(res._1)\n  println(\"=======Sb====\")\n  println(res._2)\n  println(\"=======Sw====\")\n  println(res._3)\n\n}\n```\n\n##### 5.3 scala 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$\n\n![](./降维_线性判别分析/scala_lda_res.png)\n\n##### 5.4 Python lda 代码\n\n```python\nimport numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\ndef LDA(X, y, k):\n    '''\n    X为数据集，y为label，k为目标维数\n    '''\n    label_ = np.unique(y)\n    mu = np.mean(X, axis=0)\n\n    Sw = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sw += np.dot((_X - _mean).T,\n                     _X - _mean)\n\n    print(Sw)\n    \n    Sb = np.zeros((len(mu), len(mu)))  # 计算类内散度矩阵\n    for i in label_:\n        _X = X[y == i]\n        _mean = np.mean(_X, axis=0)\n        Sb += len(_X) * np.dot(( _mean - mu).reshape(\n            (len(mu), 1)), (_mean - mu).reshape((1, len(mu))))\n        \n    print(Sb)\n\n    eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))  # 计算Sw-1*Sb的特征值和特征矩阵\n\n    sorted_indices = np.argsort(eig_vals)\n    topk_eig_vecs = eig_vecs[:, sorted_indices[:-k - 1:-1]]  # 提取前k个特征向量\n    return topk_eig_vecs,Sb,Sw\n\n\nif '__main__' == __name__:\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n## 将数据写出到iris.csv文件，供scala使用，保证数据的一致性。\n    data_cat =  np.c_[X,y]\n    import pandas\n    iris_df = pandas.DataFrame(data_cat)\n    iris_df.to_csv(\"iris.csv\",index = 0,header = False)\n\n    W,Sb,Sw = LDA(X, y, 2)\n    X_new = np.dot((X), W)\n    plt.figure(1)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    \n    print(\"========W=========\")\n    print(W)\n    print(\"========Sb========\")\n    print(Sb)\n    print(\"========Sw========\")\n    print(Sw)\n    \n    \n    # 与sklearn中的LDA函数对比\n    lda = LinearDiscriminantAnalysis(n_components=2)\n    lda.fit(X, y)\n    X_new = lda.transform(X)\n#    print(X_new)\n    plt.figure(2)\n    plt.scatter(X_new[:, 0], X_new[:, 1], marker='o', c=y)\n    plt.show()\n\n```\n\n##### 5.5 Python 结果\n\n展示转换矩阵$W$, 类间散度矩阵$S_b$和类内散度矩阵$S_w$,可见scala和python得到结果是一致的。\n\n![](./降维_线性判别分析/python_lda_res.png)\n\n"},{"title":"线性回归","url":"/2020/01/09/线性回归/","content":"### 线性回归的代码实现\n\n#### 1. 原理\n\n多元线性回归的损失函数为：\n$$\nJ=\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2\n$$\n其中：$\\hat{y}^{(i)} = \\theta_{0} + \\theta_{1}X_{1}^{(i)} + \\theta_{2}X_{2}^{(i)} + ... + \\theta_{n}X_{n}^{(i)}$ 。\n\n对 $J$ 求导为：\n$$\n\\nabla J=(\\frac{\\partial J}{\\partial \\theta_0},\\frac{\\partial J}{\\partial \\theta_1},...,\\frac{\\partial J}{\\partial \\theta_n})\n$$\n其中：$\\frac{\\partial J}{\\partial \\theta_i}$ 为偏导数，与导数的求法一样。\n\n\n\n对 $\\nabla J$ 进一步计算：\n$$\n\\nabla J(\\theta) =  \\begin{pmatrix} \\frac{\\partial J}{\\partial \\theta_0} \\\\\\ \\frac{\\partial J}{\\partial \\theta_1} \\\\\\ \\frac{\\partial J}{\\partial \\theta_2} \\\\\\ \\cdots \\\\\\ \\frac{\\partial J}{\\partial \\theta_n} \\end{pmatrix} =   \\begin{pmatrix} \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-1) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_1^{(i)}) \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_2^{(i)}) \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}2(y^{(i)} - X_b^{(i)}\\theta)·(-X_n^{(i)}) \\end{pmatrix} = 2·\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n\n其中：$X_b = \\begin{pmatrix}\n1 & X_1^{(1)} & X_2^{(1)} & \\cdots & X_n^{(1)} \\\\\n1 & X_1^{(2)} & X_2^{(2)} & \\cdots & X_n^{(2)} \\\\\n\\vdots & \\vdots & \\vdots & \\cdots & \\vdots \\\\\n1 & X_1^{(m)} & X_2^{(m)} & \\cdots & X_n^{(m)}\n\\end{pmatrix}$\n\n\n\n​        相应的对$J$上对$θ$这个向量去求梯度值，也就是损失函数$J$对$θ$每一个维度的未知量去求导。此时需要注意，求导过程中，$θ$是未知数，相应的$X$和$y$都是已知的，都是在监督学习中获得的样本信息。对于最右边式子的每一项都是m项的求和，显然梯度的大小和样本数量有关，样本数量越大，求出来的梯度中，每一个元素相应的也就越大，这个其实是不合理的，求出来的梯度中每一个元素的值应该和$m$样本数量是无关的，为此将整个梯度值再除上一个m，相应的目标函数的式子变成了下面的式子即：\n$$\n\\nabla J(\\theta)  = \\frac{2}{m}\\begin{pmatrix} \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)}) \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_1^{(i)} \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_2^{(i)} \\\\\\ \\cdots \\\\\\ \\sum_{i=1}^{m}(X_b^{(i)}\\theta - y^{(i)})·X_n^{(i)} \\end{pmatrix}\n$$\n​        此时，目标函数就成了使 $\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ 尽可能小，即均方误差尽可能小：\n$$\nJ(\\theta) = MSE(y, \\hat{y})\n$$\n​\t\t\n\n注1. 有时候目标函数也去 $\\frac{1}{\\boldsymbol{2}m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2$ ,其中的**2**是为了抵消梯度中的2，实际的效果有限。\n\n注2. 将梯度除以m相当于目标函数本身变成了MSE，也就是对原来的目标函数除上m。如果没有1/m的话，梯度中的元素就会特别的大，在具体编程实践中就会出现一些问题。当我们在使用梯度下降法来求函数的最小值的时候，有时候需要对目标函数进行一些特殊的设计，不见得所有的目标函数都非常的合适，虽然理论上梯度中每一个元素都非常大的话，我们依然可以通过调节learning_rate(学习率)来得到我们想要的结果，但是那样的话可能会影响效率。\n\n\n\n#### 2.实现代码\n\n##### 2.1 python版本实现\n\n```python\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jan  8 17:25:39 2020\n\n@author: lixin\n\"\"\"\n\nimport numpy as np\n\nclass LinearRegression():\n    def __init__(self,lr=.01, num_iters=10000,tolerance=1e-8):\n        self.lr = lr\n        self.num_iters = num_iters\n        self.w = None\n        self.toterance = tolerance\n\n    def fit(self,x_train, y_train):\n        n_samples = len(x_train)\n        x_train = np.column_stack((np.ones(n_samples), x_train)) #也可以写成np.c_\n        \n        self.w = .01 * np.ones(x_train.shape[1])\n        self.loss_ = [0]\n        \n        # w_{i} = w_{i} - lr * (h(w_{i}) - y)*x_{i} 迭代公式\n        self.count = 0\n        for iteration in range(self.num_iters):\n            self.count += 1\n            raw_output = np.matmul(x_train, self.w)\n            errors =  raw_output - y_train\n            loss = 1/(2 * n_samples) * errors.dot(errors)\n            delta_loss = loss - self.loss_[-1]\n\n            self.loss_.append(loss)\n            if np.abs(delta_loss) < self.toterance:\n                break\n            else:\n                grad = (1.0 /n_samples) *np.matmul(x_train.T, np.array(errors))\n                self.w -= self.lr * grad\n\n    def predict(self,x_test):\n        x_test = np.column_stack((np.ones(len(x_test)), x_test))\n        \n        output = np.matmul(x_test, self.w)\n        return output\n    \nif __name__ == '__main__':\n    \n    num_inputs = 2\n    num_examples = 10000\n    true_w = [6.4, -3.2]\n    true_b = 2.3\n    features = np.random.random((num_examples, num_inputs))\n    labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n    labels += np.random.normal(0, 0.1,size = len(labels))\n    \n    import sklearn.model_selection\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(features, labels, test_size = .20, random_state=42)\n\n    lr = LinearRegression()\n    lr.fit(x_train,y_train)\n    \n    print(\"回归系数:\",lr.w)\n    print(\"迭代次数:\",lr.count)\n    \n    y_pred = lr.predict(x_test)\n    from sklearn import metrics\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    print(\"MSE: %.4f\" % mse)\n\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    print(\"MAE: %.4f\" % mae)\n\n    R2 = metrics.r2_score(y_test,y_pred)\n    print(\"R2: %.4f\" % R2)\n```\n\n##### 2.2 Scala版本实现\n\n```scala\nimport breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\nimport scala.collection.mutable.ArrayBuffer\n\n/**\n * Scala 版本的实现\n */\n\nobject LinearRegression {\n  def main(args: Array[String]): Unit = {\n    val num_inputs = 2\n    val num_examples = 1000\n    val x_train: BDM[Double] = BDM.rand(num_examples, num_inputs)\n    val ones = BDM.ones[Double](num_examples, 1)\n    val x_cat = BDM.horzcat(ones, x_train)\n    val y_train = x_cat * BDV(2.3, 6.4, -3.2)\n\n    val model = new LinearRegression(num_iters = 10000)\n    val weights = model.fit(x_train, y_train)\n    val predictions = model.predict(weights, x_train)\n    println(\"梯度下降求解的权重为：\" + weights)\n    println(predictions)\n  }\n}\n\nclass LinearRegression(var lr: Double = 0.01, var tolerance: Double = 1e-6, var num_iters: Int = 1000) {\n\n  def fit(x: BDM[Double], y_train: BDV[Double]): BDV[Double] = {\n    val ones = BDM.ones[Double](x.rows, 1)\n    val x_train = BDM.horzcat(ones, x)\n    val n_samples = x_train.rows\n    val n_features = x_train.cols\n    var weights = BDV.ones[Double](n_features) :* .01 // 注意是:*\n\n    val loss_lst: ArrayBuffer[Double] = new ArrayBuffer[Double]()\n    loss_lst.append(0.0)\n\n    var flag = true\n    for (i <- 0 to num_iters if flag) {\n      val raw_output = x_train * weights\n      val error = raw_output - y_train\n      val loss: Double = error.t * error\n      val delta_loss = loss - loss_lst.apply(loss_lst.size - 1)\n      loss_lst.append(loss)\n      if (scala.math.abs(delta_loss) < tolerance) {\n        flag = false\n      } else {\n        val gradient = (error.t * x_train) :/ n_samples.toDouble\n        weights = weights - (gradient :* lr).t\n      }\n    }\n    weights\n  }\n\n  def predict(weights: BDV[Double], x: BDM[Double]): BDV[Double] = {\n    val x_test = BDM.horzcat(BDM.ones[Double](x.rows, 1), x)\n    val output = x_test * weights\n    output\n  }\n}\n```\n\n参考文献：1. [机器学习之梯度下降法与线性回归](https://segmentfault.com/a/1190000017048213)\n\n\n\n"}]