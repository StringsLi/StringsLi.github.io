<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        神经网络 - undefined
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i>  </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/" />
        </div>
        <div class="name">
            <i>Lixin</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>HOME</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>TAGS</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>ARCHIVES</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>ABOUT</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#神经网络的numpy实现和公式推导"><span class="toc-text">神经网络的numpy实现和公式推导</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#概览"><span class="toc-text">概览</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#初始化神经网络层"><span class="toc-text">初始化神经网络层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#激活函数"><span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#前向传播"><span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#损失函数"><span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#反向传播"><span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#更新参数值"><span class="toc-text">更新参数值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#整合一切"><span class="toc-text">整合一切</span></a></li></ol></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">search</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i>  </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        神经网络
    </div>

    <div class="post-meta">
        <span class="attr">Post：<span>2019-11-27 15:24:46</span></span>
        
        <span class="attr">Tags：/
        
        <a class="tag" href="/tags/#机器学习" title="机器学习">机器学习</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">Visit：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h3 id="神经网络的numpy实现和公式推导"><a href="#神经网络的numpy实现和公式推导" class="headerlink" title="神经网络的numpy实现和公式推导"></a>神经网络的numpy实现和公式推导</h3><p>​       过多10多年是神经网络发展的黄金时期，神经网络(深度学习)成为了新时代的一种浪潮，所以今天借用国外一个小哥实现纯numpy的神经网络，来记录神经网络的实现过程。</p>
<p><img src="./pic/nn_architecture.png" alt=""></p>
<h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p>​        在开始编程之前，先让我们准备一份基本的路线图。我们的目标是创建一个特定架构（层数、层大小、激活函数）的密集连接神经网络。然后训练这一神经网络并做出预测。</p>
<p><img src="./pic/blueprint.gif" alt=""></p>
<p>上面的示意图展示了训练网络【特别是正向传播和反向传播的操作】时进行的操作，以及单次迭代不同阶段需要更新和读取的参数。</p>
<h4 id="初始化神经网络层"><a href="#初始化神经网络层" class="headerlink" title="初始化神经网络层"></a>初始化神经网络层</h4><p>  让我们从初始化每一层的权重矩阵$W$和偏置向量$b$开始。下图展示了网络层l的权重矩阵和偏置向量，其中，上标$[l]$表示当前层的索引，$n$表示给定层中的神经元数量。</p>
<p><img src="./pic/params_sizes.png" alt=""></p>
<p>我们的程序也将以类似的列表形式描述神经网络架构。列表的每一项是一个字典，描述单个网络层的基本参数：<code>input_dim</code>是网络层输入的信号向量的大小，<code>output_dim</code>是网络层输出的激活向量的大小，<code>activation</code>是网络层所用的激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nn_architecture = [</span><br><span class="line">    &#123;<span class="string">"input_dim"</span>: <span class="number">2</span>, <span class="string">"output_dim"</span>: <span class="number">4</span>, <span class="string">"activation"</span>: <span class="string">"relu"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"input_dim"</span>: <span class="number">4</span>, <span class="string">"output_dim"</span>: <span class="number">6</span>, <span class="string">"activation"</span>: <span class="string">"relu"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"input_dim"</span>: <span class="number">6</span>, <span class="string">"output_dim"</span>: <span class="number">6</span>, <span class="string">"activation"</span>: <span class="string">"relu"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"input_dim"</span>: <span class="number">6</span>, <span class="string">"output_dim"</span>: <span class="number">4</span>, <span class="string">"activation"</span>: <span class="string">"relu"</span>&#125;,</span><br><span class="line">    &#123;<span class="string">"input_dim"</span>: <span class="number">4</span>, <span class="string">"output_dim"</span>: <span class="number">1</span>, <span class="string">"activation"</span>: <span class="string">"sigmoid"</span>&#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>值得注意的是，一个网络层的输出向量同时也是下一层的输入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_layers</span><span class="params">(nn_architecture, seed = <span class="number">99</span>)</span>:</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    number_of_layers = len(nn_architecture)</span><br><span class="line">    params_values = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, layer <span class="keyword">in</span> enumerate(nn_architecture):</span><br><span class="line">        layer_idx = idx + <span class="number">1</span></span><br><span class="line">        layer_input_size = layer[<span class="string">"input_dim"</span>]</span><br><span class="line">        layer_output_size = layer[<span class="string">"output_dim"</span>]</span><br><span class="line"></span><br><span class="line">        params_values[<span class="string">'W'</span> + str(layer_idx)] = np.random.randn(</span><br><span class="line">            layer_output_size, layer_input_size) * <span class="number">0.1</span></span><br><span class="line">        params_values[<span class="string">'b'</span> + str(layer_idx)] = np.random.randn(</span><br><span class="line">            layer_output_size, <span class="number">1</span>) * <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params_values</span><br></pre></td></tr></table></figure>

<p>​        上面的代码初始化了网络层的参数。注意我们用随机的小数字填充矩阵<strong>W</strong>和向量<strong>b</strong>。这并不是偶然的。权重值无法使用相同的数字初始化，否则会造成<strong>破坏性的对称问题</strong>。<strong>基本上，如果权重都一样，不管输入X是什么，隐藏层的所有单元也都一样</strong>。这样，我们就会陷入初始状态，不管训练多久，网络多深，都无望摆脱。线性代数不会原谅我们。</p>
<p>[^初始化方法包含很多种，我们这里简便起见，使用随机初始化的方式生成权重]: </p>
<p>​       小数值增加了算法的效率。我们可以看看下面的sigmoid函数图像，大数值处的函数图像几乎是扁平的，这会对神经网络的学习速度造成显著影响。所有参数使用小随机数是一个简单的方法，但它保证了算法有一个<strong>足够好</strong>的开始。</p>
<p><img src="./pic/activations.gif" alt=""></p>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><p>​        激活函数只需一行代码就可以定义，但它们给神经网络带来了非线性和所需的表达力。<strong>“没有它们，神经网络将变成线性函数的组合，也就是单个线性函数。”</strong>激活函数有很多种，但在这个项目中，我决定使用其中两种——sigmoid和ReLU。为了同时支持前向传播和反向传播，我们还需要准备好它们的导数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>,Z)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    sig = sigmoid(Z)</span><br><span class="line">    <span class="keyword">return</span> dA * sig * (<span class="number">1</span> - sig)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, Z)</span>:</span></span><br><span class="line">    dZ = np.array(dA, copy = <span class="literal">True</span>)</span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">return</span> dZ;</span><br></pre></td></tr></table></figure>

<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>​    我们设计的神经网络有一个简单的架构。输入矩阵<strong>X</strong>传入网络，沿着隐藏单元传播，最终得到预测向量<strong>Y_hat</strong>。为了让代码更易读，我将前向传播拆分成两个函数——单层前向传播，和整个神经网络前向传播。</p>
<p>​        <strong>前向传播的过程是：输入$a^{[l-1]}$, 输出$a^{[l]}$, 缓存为$z^{[l]}$,  从方便实现的角度上看，$z^{[l]}$是$w^{[l]}$，$b^{[l]}$的函数。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_layer_forward_propagation</span><span class="params">(A_prev, W_curr, b_curr, activation=<span class="string">"relu"</span>)</span>:</span></span><br><span class="line">    Z_curr = np.dot(W_curr, A_prev) + b_curr</span><br><span class="line">    <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="string">"relu"</span>:</span><br><span class="line">        activation_func = relu</span><br><span class="line">    <span class="keyword">elif</span> activation <span class="keyword">is</span> <span class="string">"sigmoid"</span>:</span><br><span class="line">        activation_func = sigmoid</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Non-supported activation function'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> activation_func(Z_curr), Z_curr</span><br></pre></td></tr></table></figure>
<p>这部分代码大概是最直接，最容易理解的。给定来自上一层的输入信号，我们计算仿射变换<strong>Z</strong>，接着应用选中的激活函数。基于NumPy，我们可以对整个网络层和整批样本一下子进行矩阵操作，无需迭代，这大大加速了计算。除了计算结果外，函数还返回了一个反向传播时需要用到的中间值<strong>Z</strong>。</p>
<p><img src="./pic/matrix_sizes_2.png" alt=""></p>
<p>基于单层前向传播函数，编写整个前向传播步骤很容易。这是一个略微复杂一点的函数，它的角色不仅是进行预测，还包括组织中间值。<br>$$<br>\begin{aligned}<br>z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}  \</p>
<p>a^{[l]} = g^{[l]}(z^{[l]})<br>\end{aligned}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">full_forward_propagation</span><span class="params">(X, params_values, nn_architecture)</span>:</span></span><br><span class="line">    memory = &#123;&#125;</span><br><span class="line">    A_curr = X</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, layer <span class="keyword">in</span> enumerate(nn_architecture):</span><br><span class="line">        layer_idx = idx + <span class="number">1</span></span><br><span class="line">        A_prev = A_curr</span><br><span class="line"></span><br><span class="line">        activ_function_curr = layer[<span class="string">"activation"</span>]</span><br><span class="line">        W_curr = params_values[<span class="string">"W"</span> + str(layer_idx)]</span><br><span class="line">        b_curr = params_values[<span class="string">"b"</span> + str(layer_idx)]</span><br><span class="line">        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)</span><br><span class="line"></span><br><span class="line">        memory[<span class="string">"A"</span> + str(idx)] = A_prev</span><br><span class="line">        memory[<span class="string">"Z"</span> + str(layer_idx)] = Z_curr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A_curr, memory</span><br></pre></td></tr></table></figure>

<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>​          损失函数可以监测进展，确保我们向着正确的方向移动。<strong>“一般来说，损失函数是为了显示我们离‘理想’解答还有多远。”</strong>损失函数根据我们计划解决的问题而选用，Keras之类的框架提供了很多选项。因为我计划将神经网络用于二元分类问题，我决定使用交叉熵：<br>$$<br>J(W,b) = \dfrac{1}{m}{\sum}_{i=1}^{m}L(\hat{y}^{i} - y^{i})  \<br>L(\hat{y} - y) = -(ylog\hat{y} + (1-y)log(1-\hat{y}))<br>$$<br>为了取得更多关于学习过程的信息，我决定另外实现一个计算精确度的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">J(W,b) = 1/m sum_&#123;i&#125;^&#123;m&#125;L(y^&#123;hat&#125;_&#123;i&#125; - y_&#123;i&#125;)</span></span><br><span class="line"><span class="string">L(y^&#123;hat&#125;_&#123;i&#125; - y_&#123;i&#125;) = -(ylogy^&#123;hat&#125; + (1-y)log(1-y^&#123;hat&#125;))</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cost_value</span><span class="params">(Y_hat, Y)</span>:</span></span><br><span class="line">    m = Y_hat.shape[<span class="number">1</span>]</span><br><span class="line">    cost = <span class="number">-1</span> / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - Y_hat).T))</span><br><span class="line">    <span class="keyword">return</span> np.squeeze(cost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># an auxiliary function that converts probability into class</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_prob_into_class</span><span class="params">(probs)</span>:</span></span><br><span class="line">    probs_ = np.copy(probs)</span><br><span class="line">    probs_[probs_ &gt; <span class="number">0.5</span>] = <span class="number">1</span></span><br><span class="line">    probs_[probs_ &lt;= <span class="number">0.5</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> probs_</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy_value</span><span class="params">(Y_hat, Y)</span>:</span></span><br><span class="line">    Y_hat_ = convert_prob_into_class(Y_hat)</span><br><span class="line">    <span class="keyword">return</span> (Y_hat_ == Y).all(axis=<span class="number">0</span>).mean()</span><br></pre></td></tr></table></figure>

<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>不幸的是，很多缺乏经验的深度学习爱好者都觉得反向传播很吓人，难以理解。微积分和线性代数的组合经常会吓退那些没有经过扎实的数学训练的人。所以不要过于担心你现在还不能理解这一切。相信我，我们都经历过这个过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single_layer_backward_propagation</span><span class="params">(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=<span class="string">"relu"</span>)</span>:</span></span><br><span class="line">    <span class="comment"># number of examples</span></span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># selection of activation function</span></span><br><span class="line">    <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="string">"relu"</span>:</span><br><span class="line">        backward_activation_func = relu_backward</span><br><span class="line">    <span class="keyword">elif</span> activation <span class="keyword">is</span> <span class="string">"sigmoid"</span>:</span><br><span class="line">        backward_activation_func = sigmoid_backward</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">'Non-supported activation function'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calculation of the activation function derivative</span></span><br><span class="line">    dZ_curr = backward_activation_func(dA_curr, Z_curr)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># derivative of the matrix W</span></span><br><span class="line">    dW_curr = np.dot(dZ_curr, A_prev.T) / m</span><br><span class="line">    <span class="comment"># derivative of the vector b</span></span><br><span class="line">    db_curr = np.sum(dZ_curr, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) / m</span><br><span class="line">    <span class="comment"># derivative of the matrix A_prev</span></span><br><span class="line">    dA_prev = np.dot(W_curr.T, dZ_curr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW_curr, db_curr</span><br></pre></td></tr></table></figure>

<p>​        人们经常搞混反向传播和梯度下降，但事实上它们不一样。前者是为了高效地计算梯度，后者则是为了基于计算出的梯度进行优化。在神经网络中，我们计算损失函数在参数上的梯度，但反向传播可以用来计算任何函数的导数。<strong>反向传播算法的精髓在于递归地使用求导的链式法则，通过组合导数已知的函数，计算函数的导数</strong>。下面的公式描述了单个网络层上的反向传播过程。由于本文的重点在实际实现，所以我将省略求导过程。从公式上我们可以很明显地看到，为什么我们需要在前向传播时记住中间层的<strong>A</strong>、<strong>Z</strong>矩阵的值。</p>
<p>$$\boldsymbol{dW}^{[l]} = \frac{\partial L }{\partial \boldsymbol{W}^{[l]}} =\frac{\partial L }{\partial \boldsymbol{Z}^{[l]}} \frac{\partial \boldsymbol{Z}^{[l]} }{\partial \boldsymbol{W}^{[l]}} =  \frac{1}{m} \boldsymbol{dZ}^{[l]} \boldsymbol{A}^{[l-1] T}$$</p>
<p>$$\boldsymbol{db}^{[l]} = \frac{\partial L }{\partial \boldsymbol{b}^{[l]}}  = \frac{\partial L }{\partial \boldsymbol{Z}^{[l]}}   \frac{\partial \boldsymbol{Z}^{[l]} }{\partial \boldsymbol{b}^{[l]}}= \frac{1}{m} \sum_{i = 1}^{m} \boldsymbol{dZ}^{<a href="i">l</a>}$$</p>
<p>$$\boldsymbol{dA}^{[l-1]} = \frac{\partial L }{\partial \boldsymbol{A}^{[l-1]}} =  \frac{\partial L }{\partial \boldsymbol{Z}^{[l]}}  \frac{\partial \boldsymbol{Z}^{[l]} }{\partial \boldsymbol{A}^{[l-1]}} = \boldsymbol{W}^{[l] T} \boldsymbol{dZ}^{[l]}$$</p>
<p>$$\boldsymbol{dZ}^{[l]} = \frac{\partial L }{\partial \boldsymbol{Z}^{[l]}}= \frac{\partial L }{\partial \boldsymbol{A}^{[l]}} \frac{\partial {A}^{[l]} }{\partial \boldsymbol{Z}^{[l]}}=\boldsymbol{dA}^{[l]} * g^{[l]}{‘}(\boldsymbol{Z}^{[l]})$$</p>
<p><img src="./pic/640.webp" alt=""></p>
<p>和前向传播一样，我决定将计算拆分成两个函数。之前给出的是单个网络层的反向传播函数，基本上就是以NumPy方式重写上面的数学公式。而定义完整反向传播过程的函数，主要是读取、更新三个字典中的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">full_backward_propagation</span><span class="params">(Y_hat, Y, memory, params_values, nn_architecture)</span>:</span></span><br><span class="line">    grads_values = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># number of examples</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># a hack ensuring the same shape of the prediction vector and labels vector</span></span><br><span class="line">    Y = Y.reshape(Y_hat.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initiation of gradient descent algorithm</span></span><br><span class="line">    dA_prev = - (np.divide(Y, Y_hat) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - Y_hat));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer_idx_prev, layer <span class="keyword">in</span> reversed(list(enumerate(nn_architecture))):</span><br><span class="line">        <span class="comment"># we number network layers from 1</span></span><br><span class="line">        layer_idx_curr = layer_idx_prev + <span class="number">1</span></span><br><span class="line">        <span class="comment"># extraction of the activation function for the current layer</span></span><br><span class="line">        activ_function_curr = layer[<span class="string">"activation"</span>]</span><br><span class="line">        </span><br><span class="line">        dA_curr = dA_prev</span><br><span class="line">        </span><br><span class="line">        A_prev = memory[<span class="string">"A"</span> + str(layer_idx_prev)]</span><br><span class="line">        Z_curr = memory[<span class="string">"Z"</span> + str(layer_idx_curr)]</span><br><span class="line">        </span><br><span class="line">        W_curr = params_values[<span class="string">"W"</span> + str(layer_idx_curr)]</span><br><span class="line">        b_curr = params_values[<span class="string">"b"</span> + str(layer_idx_curr)]</span><br><span class="line">        </span><br><span class="line">        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(</span><br><span class="line">            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)</span><br><span class="line">        </span><br><span class="line">        grads_values[<span class="string">"dW"</span> + str(layer_idx_curr)] = dW_curr</span><br><span class="line">        grads_values[<span class="string">"db"</span> + str(layer_idx_curr)] = db_curr</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads_values</span><br></pre></td></tr></table></figure>

<p>基于单个网络层的反向传播函数，我们从最后一层开始迭代计算所有参数上的导数，并最终返回包含所需梯度的python字典。</p>
<h4 id="更新参数值"><a href="#更新参数值" class="headerlink" title="更新参数值"></a>更新参数值</h4><p>反向传播是为了计算梯度，以根据梯度进行优化，更新网络的参数值。为了完成这一任务，我们将使用两个字典作为函数参数：<code>params_values</code>，其中保存了当前参数值；<code>grads_values</code>，其中保存了用于更新参数值所需的梯度信息。现在我们只需在每个网络层上应用以下等式即可。这是一个非常简单的优化算法，但我决定使用它作为更高级的优化算法的起点（大概会是我下一篇文章的主题）。</p>
<p>$$\boldsymbol{W}^{[l]} = \boldsymbol{W}^{[l]} - \alpha \boldsymbol{dW}^{[l]} $$</p>
<p>$$\boldsymbol{b}^{[l]} = \boldsymbol{b}^{[l]} - \alpha \boldsymbol{b}^{[l]} $$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def update(params_values, grads_values, nn_architecture, learning_rate):</span><br><span class="line"></span><br><span class="line">    # iteration over network layers</span><br><span class="line">    for layer_idx, layer in enumerate(nn_architecture, 1):</span><br><span class="line">        params_values[&quot;W&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;dW&quot; + str(layer_idx)]        </span><br><span class="line">        params_values[&quot;b&quot; + str(layer_idx)] -= learning_rate * grads_values[&quot;db&quot; + str(layer_idx)]</span><br><span class="line"></span><br><span class="line">    return params_values;</span><br></pre></td></tr></table></figure>

<h4 id="整合一切"><a href="#整合一切" class="headerlink" title="整合一切"></a>整合一切</h4><p>万事俱备只欠东风。最困难的部分已经完成了——我们已经准备好了所需的函数，现在只需以正确的顺序把它们放到一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(X, Y, nn_architecture, epochs, learning_rate, verbose=False, callback=None)</span>:</span></span><br><span class="line">    <span class="comment"># initiation of neural net parameters</span></span><br><span class="line">    params_values = init_layers(nn_architecture, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># initiation of lists storing the history </span></span><br><span class="line">    <span class="comment"># of metrics calculated during the learning process </span></span><br><span class="line">    cost_history = []</span><br><span class="line">    accuracy_history = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># performing calculations for subsequent iterations</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">        <span class="comment"># step forward</span></span><br><span class="line">        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># calculating metrics and saving them in history</span></span><br><span class="line">        cost = get_cost_value(Y_hat, Y)</span><br><span class="line">        cost_history.append(cost)</span><br><span class="line">        accuracy = get_accuracy_value(Y_hat, Y)</span><br><span class="line">        accuracy_history.append(accuracy)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># step backward - calculating gradient</span></span><br><span class="line">        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)</span><br><span class="line">        <span class="comment"># updating model state</span></span><br><span class="line">        params_values = update(params_values, grads_values, nn_architecture, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(i % <span class="number">50</span> == <span class="number">0</span>):</span><br><span class="line">            <span class="keyword">if</span>(verbose):</span><br><span class="line">                print(<span class="string">"Iteration: &#123;:05&#125; - cost: &#123;:.5f&#125; - accuracy: &#123;:.5f&#125;"</span>.format(i, cost, accuracy))</span><br><span class="line">            <span class="keyword">if</span>(callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">                callback(i, params_values)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> params_values</span><br></pre></td></tr></table></figure>



        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        

        

        

        

        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud" target="_blank" rel="noopener">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = ""
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
